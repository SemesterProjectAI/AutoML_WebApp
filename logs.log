2024-05-03 10:46:46,331:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-03 10:46:46,331:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-03 10:46:46,331:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-03 10:46:46,331:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-03 10:55:12,436:INFO:PyCaret ClassificationExperiment
2024-05-03 10:55:12,436:INFO:Logging name: clf-default-name
2024-05-03 10:55:12,437:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-03 10:55:12,437:INFO:version 3.3.2
2024-05-03 10:55:12,437:INFO:Initializing setup()
2024-05-03 10:55:12,437:INFO:self.USI: 8794
2024-05-03 10:55:12,437:INFO:self._variable_keys: {'n_jobs_param', '_available_plots', 'exp_name_log', 'X_train', 'pipeline', 'USI', '_ml_usecase', 'html_param', 'exp_id', 'y_test', 'y_train', 'target_param', 'fold_groups_param', 'X', 'X_test', 'gpu_n_jobs_param', 'idx', 'logging_param', 'fold_shuffle_param', 'fix_imbalance', 'seed', 'data', 'gpu_param', 'memory', 'log_plots_param', 'y', 'is_multiclass', 'fold_generator'}
2024-05-03 10:55:12,437:INFO:Checking environment
2024-05-03 10:55:12,437:INFO:python_version: 3.11.7
2024-05-03 10:55:12,437:INFO:python_build: ('tags/v3.11.7:fa7a6f2', 'Dec  4 2023 19:24:49')
2024-05-03 10:55:12,437:INFO:machine: AMD64
2024-05-03 10:55:12,450:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-03 10:55:12,454:INFO:Memory: svmem(total=16939401216, available=1817731072, percent=89.3, used=15121670144, free=1817731072)
2024-05-03 10:55:12,455:INFO:Physical Core: 4
2024-05-03 10:55:12,455:INFO:Logical Core: 8
2024-05-03 10:55:12,455:INFO:Checking libraries
2024-05-03 10:55:12,455:INFO:System:
2024-05-03 10:55:12,455:INFO:    python: 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]
2024-05-03 10:55:12,455:INFO:executable: Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Scripts\python.exe
2024-05-03 10:55:12,455:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-03 10:55:12,455:INFO:PyCaret required dependencies:
2024-05-03 10:55:12,496:INFO:                 pip: 23.2.1
2024-05-03 10:55:12,496:INFO:          setuptools: 68.2.0
2024-05-03 10:55:12,496:INFO:             pycaret: 3.3.2
2024-05-03 10:55:12,496:INFO:             IPython: 8.24.0
2024-05-03 10:55:12,496:INFO:          ipywidgets: 8.1.2
2024-05-03 10:55:12,496:INFO:                tqdm: 4.66.4
2024-05-03 10:55:12,496:INFO:               numpy: 1.26.4
2024-05-03 10:55:12,496:INFO:              pandas: 2.1.4
2024-05-03 10:55:12,496:INFO:              jinja2: 3.1.3
2024-05-03 10:55:12,496:INFO:               scipy: 1.11.4
2024-05-03 10:55:12,496:INFO:              joblib: 1.3.2
2024-05-03 10:55:12,496:INFO:             sklearn: 1.4.2
2024-05-03 10:55:12,496:INFO:                pyod: 1.1.3
2024-05-03 10:55:12,496:INFO:            imblearn: 0.12.2
2024-05-03 10:55:12,496:INFO:   category_encoders: 2.6.3
2024-05-03 10:55:12,496:INFO:            lightgbm: 4.3.0
2024-05-03 10:55:12,496:INFO:               numba: 0.58.1
2024-05-03 10:55:12,496:INFO:            requests: 2.31.0
2024-05-03 10:55:12,496:INFO:          matplotlib: 3.7.5
2024-05-03 10:55:12,496:INFO:          scikitplot: 0.3.7
2024-05-03 10:55:12,496:INFO:         yellowbrick: 1.5
2024-05-03 10:55:12,497:INFO:              plotly: 5.22.0
2024-05-03 10:55:12,497:INFO:    plotly-resampler: Not installed
2024-05-03 10:55:12,497:INFO:             kaleido: 0.2.1
2024-05-03 10:55:12,497:INFO:           schemdraw: 0.15
2024-05-03 10:55:12,497:INFO:         statsmodels: 0.14.2
2024-05-03 10:55:12,497:INFO:              sktime: 0.26.0
2024-05-03 10:55:12,497:INFO:               tbats: 1.1.3
2024-05-03 10:55:12,497:INFO:            pmdarima: 2.0.4
2024-05-03 10:55:12,497:INFO:              psutil: 5.9.8
2024-05-03 10:55:12,497:INFO:          markupsafe: 2.1.5
2024-05-03 10:55:12,497:INFO:             pickle5: Not installed
2024-05-03 10:55:12,497:INFO:         cloudpickle: 3.0.0
2024-05-03 10:55:12,497:INFO:         deprecation: 2.1.0
2024-05-03 10:55:12,497:INFO:              xxhash: 3.4.1
2024-05-03 10:55:12,497:INFO:           wurlitzer: Not installed
2024-05-03 10:55:12,497:INFO:PyCaret optional dependencies:
2024-05-03 10:55:12,512:INFO:                shap: Not installed
2024-05-03 10:55:12,512:INFO:           interpret: Not installed
2024-05-03 10:55:12,513:INFO:                umap: Not installed
2024-05-03 10:55:12,513:INFO:     ydata_profiling: 4.7.0
2024-05-03 10:55:12,513:INFO:  explainerdashboard: Not installed
2024-05-03 10:55:12,513:INFO:             autoviz: Not installed
2024-05-03 10:55:12,513:INFO:           fairlearn: Not installed
2024-05-03 10:55:12,513:INFO:          deepchecks: Not installed
2024-05-03 10:55:12,513:INFO:             xgboost: Not installed
2024-05-03 10:55:12,513:INFO:            catboost: Not installed
2024-05-03 10:55:12,513:INFO:              kmodes: Not installed
2024-05-03 10:55:12,513:INFO:             mlxtend: Not installed
2024-05-03 10:55:12,513:INFO:       statsforecast: Not installed
2024-05-03 10:55:12,513:INFO:        tune_sklearn: Not installed
2024-05-03 10:55:12,513:INFO:                 ray: Not installed
2024-05-03 10:55:12,513:INFO:            hyperopt: Not installed
2024-05-03 10:55:12,513:INFO:              optuna: Not installed
2024-05-03 10:55:12,513:INFO:               skopt: Not installed
2024-05-03 10:55:12,513:INFO:              mlflow: Not installed
2024-05-03 10:55:12,513:INFO:              gradio: Not installed
2024-05-03 10:55:12,513:INFO:             fastapi: Not installed
2024-05-03 10:55:12,513:INFO:             uvicorn: Not installed
2024-05-03 10:55:12,513:INFO:              m2cgen: Not installed
2024-05-03 10:55:12,513:INFO:           evidently: Not installed
2024-05-03 10:55:12,513:INFO:               fugue: Not installed
2024-05-03 10:55:12,513:INFO:           streamlit: 1.34.0
2024-05-03 10:55:12,513:INFO:             prophet: Not installed
2024-05-03 10:55:12,513:INFO:None
2024-05-03 10:55:12,513:INFO:Set up data.
2024-05-03 10:55:12,516:INFO:Set up folding strategy.
2024-05-03 10:55:12,516:INFO:Set up train/test split.
2024-05-03 10:55:15,194:INFO:PyCaret ClassificationExperiment
2024-05-03 10:55:15,194:INFO:Logging name: clf-default-name
2024-05-03 10:55:15,194:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-03 10:55:15,194:INFO:version 3.3.2
2024-05-03 10:55:15,196:INFO:Initializing setup()
2024-05-03 10:55:15,196:INFO:self.USI: ef88
2024-05-03 10:55:15,196:INFO:self._variable_keys: {'n_jobs_param', '_available_plots', 'exp_name_log', 'X_train', 'pipeline', 'USI', '_ml_usecase', 'html_param', 'exp_id', 'y_test', 'y_train', 'target_param', 'fold_groups_param', 'X', 'X_test', 'gpu_n_jobs_param', 'idx', 'logging_param', 'fold_shuffle_param', 'fix_imbalance', 'seed', 'data', 'gpu_param', 'memory', 'log_plots_param', 'y', 'is_multiclass', 'fold_generator'}
2024-05-03 10:55:15,196:INFO:Checking environment
2024-05-03 10:55:15,196:INFO:python_version: 3.11.7
2024-05-03 10:55:15,196:INFO:python_build: ('tags/v3.11.7:fa7a6f2', 'Dec  4 2023 19:24:49')
2024-05-03 10:55:15,196:INFO:machine: AMD64
2024-05-03 10:55:15,196:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-03 10:55:15,200:INFO:Memory: svmem(total=16939401216, available=1812877312, percent=89.3, used=15126523904, free=1812877312)
2024-05-03 10:55:15,200:INFO:Physical Core: 4
2024-05-03 10:55:15,200:INFO:Logical Core: 8
2024-05-03 10:55:15,200:INFO:Checking libraries
2024-05-03 10:55:15,200:INFO:System:
2024-05-03 10:55:15,200:INFO:    python: 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]
2024-05-03 10:55:15,200:INFO:executable: Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Scripts\python.exe
2024-05-03 10:55:15,200:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-03 10:55:15,200:INFO:PyCaret required dependencies:
2024-05-03 10:55:15,200:INFO:                 pip: 23.2.1
2024-05-03 10:55:15,200:INFO:          setuptools: 68.2.0
2024-05-03 10:55:15,200:INFO:             pycaret: 3.3.2
2024-05-03 10:55:15,200:INFO:             IPython: 8.24.0
2024-05-03 10:55:15,200:INFO:          ipywidgets: 8.1.2
2024-05-03 10:55:15,200:INFO:                tqdm: 4.66.4
2024-05-03 10:55:15,201:INFO:               numpy: 1.26.4
2024-05-03 10:55:15,201:INFO:              pandas: 2.1.4
2024-05-03 10:55:15,201:INFO:              jinja2: 3.1.3
2024-05-03 10:55:15,201:INFO:               scipy: 1.11.4
2024-05-03 10:55:15,201:INFO:              joblib: 1.3.2
2024-05-03 10:55:15,201:INFO:             sklearn: 1.4.2
2024-05-03 10:55:15,201:INFO:                pyod: 1.1.3
2024-05-03 10:55:15,201:INFO:            imblearn: 0.12.2
2024-05-03 10:55:15,201:INFO:   category_encoders: 2.6.3
2024-05-03 10:55:15,201:INFO:            lightgbm: 4.3.0
2024-05-03 10:55:15,201:INFO:               numba: 0.58.1
2024-05-03 10:55:15,201:INFO:            requests: 2.31.0
2024-05-03 10:55:15,201:INFO:          matplotlib: 3.7.5
2024-05-03 10:55:15,201:INFO:          scikitplot: 0.3.7
2024-05-03 10:55:15,201:INFO:         yellowbrick: 1.5
2024-05-03 10:55:15,201:INFO:              plotly: 5.22.0
2024-05-03 10:55:15,201:INFO:    plotly-resampler: Not installed
2024-05-03 10:55:15,201:INFO:             kaleido: 0.2.1
2024-05-03 10:55:15,201:INFO:           schemdraw: 0.15
2024-05-03 10:55:15,201:INFO:         statsmodels: 0.14.2
2024-05-03 10:55:15,201:INFO:              sktime: 0.26.0
2024-05-03 10:55:15,201:INFO:               tbats: 1.1.3
2024-05-03 10:55:15,201:INFO:            pmdarima: 2.0.4
2024-05-03 10:55:15,201:INFO:              psutil: 5.9.8
2024-05-03 10:55:15,201:INFO:          markupsafe: 2.1.5
2024-05-03 10:55:15,201:INFO:             pickle5: Not installed
2024-05-03 10:55:15,201:INFO:         cloudpickle: 3.0.0
2024-05-03 10:55:15,201:INFO:         deprecation: 2.1.0
2024-05-03 10:55:15,201:INFO:              xxhash: 3.4.1
2024-05-03 10:55:15,202:INFO:           wurlitzer: Not installed
2024-05-03 10:55:15,202:INFO:PyCaret optional dependencies:
2024-05-03 10:55:15,202:INFO:                shap: Not installed
2024-05-03 10:55:15,202:INFO:           interpret: Not installed
2024-05-03 10:55:15,202:INFO:                umap: Not installed
2024-05-03 10:55:15,202:INFO:     ydata_profiling: 4.7.0
2024-05-03 10:55:15,202:INFO:  explainerdashboard: Not installed
2024-05-03 10:55:15,202:INFO:             autoviz: Not installed
2024-05-03 10:55:15,202:INFO:           fairlearn: Not installed
2024-05-03 10:55:15,202:INFO:          deepchecks: Not installed
2024-05-03 10:55:15,202:INFO:             xgboost: Not installed
2024-05-03 10:55:15,202:INFO:            catboost: Not installed
2024-05-03 10:55:15,202:INFO:              kmodes: Not installed
2024-05-03 10:55:15,202:INFO:             mlxtend: Not installed
2024-05-03 10:55:15,202:INFO:       statsforecast: Not installed
2024-05-03 10:55:15,202:INFO:        tune_sklearn: Not installed
2024-05-03 10:55:15,202:INFO:                 ray: Not installed
2024-05-03 10:55:15,202:INFO:            hyperopt: Not installed
2024-05-03 10:55:15,202:INFO:              optuna: Not installed
2024-05-03 10:55:15,202:INFO:               skopt: Not installed
2024-05-03 10:55:15,202:INFO:              mlflow: Not installed
2024-05-03 10:55:15,202:INFO:              gradio: Not installed
2024-05-03 10:55:15,202:INFO:             fastapi: Not installed
2024-05-03 10:55:15,202:INFO:             uvicorn: Not installed
2024-05-03 10:55:15,202:INFO:              m2cgen: Not installed
2024-05-03 10:55:15,202:INFO:           evidently: Not installed
2024-05-03 10:55:15,202:INFO:               fugue: Not installed
2024-05-03 10:55:15,202:INFO:           streamlit: 1.34.0
2024-05-03 10:55:15,202:INFO:             prophet: Not installed
2024-05-03 10:55:15,202:INFO:None
2024-05-03 10:55:15,203:INFO:Set up data.
2024-05-03 10:55:15,205:INFO:Set up folding strategy.
2024-05-03 10:55:15,205:INFO:Set up train/test split.
2024-05-03 10:55:15,215:INFO:Set up index.
2024-05-03 10:55:15,216:INFO:Assigning column types.
2024-05-03 10:55:15,220:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-05-03 10:55:15,272:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 10:55:15,276:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 10:55:15,314:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,314:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,360:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 10:55:15,360:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 10:55:15,388:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,388:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,388:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-05-03 10:55:15,431:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 10:55:15,457:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,457:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,498:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 10:55:15,521:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,521:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,521:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-05-03 10:55:15,584:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,584:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,649:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,650:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,653:INFO:Preparing preprocessing pipeline...
2024-05-03 10:55:15,656:INFO:Set up simple imputation.
2024-05-03 10:55:15,674:INFO:Finished creating preprocessing pipeline.
2024-05-03 10:55:15,678:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-05-03 10:55:15,678:INFO:Creating final display dataframe.
2024-05-03 10:55:15,728:INFO:Setup _display_container:                     Description             Value
0                    Session id              1615
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (210, 8)
4        Transformed data shape          (210, 8)
5   Transformed train set shape          (147, 8)
6    Transformed test set shape           (63, 8)
7              Numeric features                 7
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              ef88
2024-05-03 10:55:15,798:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,799:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,868:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,869:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 10:55:15,870:INFO:setup() successfully completed in 0.68s...............
2024-05-03 10:55:15,878:INFO:Initializing compare_models()
2024-05-03 10:55:15,878:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-05-03 10:55:15,878:INFO:Checking exceptions
2024-05-03 10:55:15,884:INFO:Preparing display monitor
2024-05-03 10:55:15,889:INFO:Initializing Logistic Regression
2024-05-03 10:55:15,889:INFO:Total runtime is 0.0 minutes
2024-05-03 10:55:15,889:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:15,889:INFO:Initializing create_model()
2024-05-03 10:55:15,889:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:15,889:INFO:Checking exceptions
2024-05-03 10:55:15,889:INFO:Importing libraries
2024-05-03 10:55:15,890:INFO:Copying training dataset
2024-05-03 10:55:15,895:INFO:Defining folds
2024-05-03 10:55:15,895:INFO:Declaring metric variables
2024-05-03 10:55:15,895:INFO:Importing untrained model
2024-05-03 10:55:15,896:INFO:Logistic Regression Imported successfully
2024-05-03 10:55:15,897:INFO:Starting cross validation
2024-05-03 10:55:15,897:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:19,771:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:19,771:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:19,827:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:19,848:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:19,900:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:19,919:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:19,922:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:19,926:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:19,941:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:19,955:INFO:Calculating mean and std
2024-05-03 10:55:19,956:INFO:Creating metrics dataframe
2024-05-03 10:55:19,958:INFO:Uploading results into container
2024-05-03 10:55:19,958:INFO:Uploading model into container now
2024-05-03 10:55:19,959:INFO:_master_model_container: 1
2024-05-03 10:55:19,959:INFO:_display_container: 2
2024-05-03 10:55:19,959:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1615, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-05-03 10:55:19,959:INFO:create_model() successfully completed......................................
2024-05-03 10:55:20,076:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:20,076:INFO:Creating metrics dataframe
2024-05-03 10:55:20,085:INFO:Initializing K Neighbors Classifier
2024-05-03 10:55:20,086:INFO:Total runtime is 0.06995004415512085 minutes
2024-05-03 10:55:20,086:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:20,086:INFO:Initializing create_model()
2024-05-03 10:55:20,086:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:20,086:INFO:Checking exceptions
2024-05-03 10:55:20,086:INFO:Importing libraries
2024-05-03 10:55:20,086:INFO:Copying training dataset
2024-05-03 10:55:20,089:INFO:Defining folds
2024-05-03 10:55:20,089:INFO:Declaring metric variables
2024-05-03 10:55:20,089:INFO:Importing untrained model
2024-05-03 10:55:20,090:INFO:K Neighbors Classifier Imported successfully
2024-05-03 10:55:20,090:INFO:Starting cross validation
2024-05-03 10:55:20,090:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:20,234:INFO:Calculating mean and std
2024-05-03 10:55:20,234:INFO:Creating metrics dataframe
2024-05-03 10:55:20,235:INFO:Uploading results into container
2024-05-03 10:55:20,236:INFO:Uploading model into container now
2024-05-03 10:55:20,236:INFO:_master_model_container: 2
2024-05-03 10:55:20,236:INFO:_display_container: 2
2024-05-03 10:55:20,236:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-05-03 10:55:20,236:INFO:create_model() successfully completed......................................
2024-05-03 10:55:20,347:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:20,347:INFO:Creating metrics dataframe
2024-05-03 10:55:20,355:INFO:Initializing Naive Bayes
2024-05-03 10:55:20,355:INFO:Total runtime is 0.07444116274515787 minutes
2024-05-03 10:55:20,355:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:20,355:INFO:Initializing create_model()
2024-05-03 10:55:20,355:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:20,355:INFO:Checking exceptions
2024-05-03 10:55:20,357:INFO:Importing libraries
2024-05-03 10:55:20,357:INFO:Copying training dataset
2024-05-03 10:55:20,360:INFO:Defining folds
2024-05-03 10:55:20,360:INFO:Declaring metric variables
2024-05-03 10:55:20,360:INFO:Importing untrained model
2024-05-03 10:55:20,361:INFO:Naive Bayes Imported successfully
2024-05-03 10:55:20,361:INFO:Starting cross validation
2024-05-03 10:55:20,361:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:20,460:INFO:Calculating mean and std
2024-05-03 10:55:20,461:INFO:Creating metrics dataframe
2024-05-03 10:55:20,462:INFO:Uploading results into container
2024-05-03 10:55:20,462:INFO:Uploading model into container now
2024-05-03 10:55:20,463:INFO:_master_model_container: 3
2024-05-03 10:55:20,463:INFO:_display_container: 2
2024-05-03 10:55:20,463:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-05-03 10:55:20,463:INFO:create_model() successfully completed......................................
2024-05-03 10:55:20,580:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:20,580:INFO:Creating metrics dataframe
2024-05-03 10:55:20,583:INFO:Initializing Decision Tree Classifier
2024-05-03 10:55:20,583:INFO:Total runtime is 0.0782396117846171 minutes
2024-05-03 10:55:20,583:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:20,584:INFO:Initializing create_model()
2024-05-03 10:55:20,584:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:20,584:INFO:Checking exceptions
2024-05-03 10:55:20,584:INFO:Importing libraries
2024-05-03 10:55:20,584:INFO:Copying training dataset
2024-05-03 10:55:20,587:INFO:Defining folds
2024-05-03 10:55:20,587:INFO:Declaring metric variables
2024-05-03 10:55:20,588:INFO:Importing untrained model
2024-05-03 10:55:20,588:INFO:Decision Tree Classifier Imported successfully
2024-05-03 10:55:20,588:INFO:Starting cross validation
2024-05-03 10:55:20,589:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:20,679:INFO:Calculating mean and std
2024-05-03 10:55:20,679:INFO:Creating metrics dataframe
2024-05-03 10:55:20,681:INFO:Uploading results into container
2024-05-03 10:55:20,681:INFO:Uploading model into container now
2024-05-03 10:55:20,681:INFO:_master_model_container: 4
2024-05-03 10:55:20,681:INFO:_display_container: 2
2024-05-03 10:55:20,682:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=1615, splitter='best')
2024-05-03 10:55:20,682:INFO:create_model() successfully completed......................................
2024-05-03 10:55:20,792:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:20,792:INFO:Creating metrics dataframe
2024-05-03 10:55:20,794:INFO:Initializing SVM - Linear Kernel
2024-05-03 10:55:20,794:INFO:Total runtime is 0.08175611893335977 minutes
2024-05-03 10:55:20,794:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:20,794:INFO:Initializing create_model()
2024-05-03 10:55:20,794:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:20,794:INFO:Checking exceptions
2024-05-03 10:55:20,794:INFO:Importing libraries
2024-05-03 10:55:20,794:INFO:Copying training dataset
2024-05-03 10:55:20,798:INFO:Defining folds
2024-05-03 10:55:20,798:INFO:Declaring metric variables
2024-05-03 10:55:20,798:INFO:Importing untrained model
2024-05-03 10:55:20,799:INFO:SVM - Linear Kernel Imported successfully
2024-05-03 10:55:20,799:INFO:Starting cross validation
2024-05-03 10:55:20,799:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:20,851:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,852:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,853:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,854:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,854:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,855:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,860:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,863:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,868:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:20,898:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,902:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:20,914:INFO:Calculating mean and std
2024-05-03 10:55:20,914:INFO:Creating metrics dataframe
2024-05-03 10:55:20,915:INFO:Uploading results into container
2024-05-03 10:55:20,916:INFO:Uploading model into container now
2024-05-03 10:55:20,916:INFO:_master_model_container: 5
2024-05-03 10:55:20,916:INFO:_display_container: 2
2024-05-03 10:55:20,917:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=1615, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-05-03 10:55:20,917:INFO:create_model() successfully completed......................................
2024-05-03 10:55:21,019:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:21,019:INFO:Creating metrics dataframe
2024-05-03 10:55:21,022:INFO:Initializing Ridge Classifier
2024-05-03 10:55:21,022:INFO:Total runtime is 0.08554941018422443 minutes
2024-05-03 10:55:21,022:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:21,022:INFO:Initializing create_model()
2024-05-03 10:55:21,022:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:21,022:INFO:Checking exceptions
2024-05-03 10:55:21,022:INFO:Importing libraries
2024-05-03 10:55:21,022:INFO:Copying training dataset
2024-05-03 10:55:21,025:INFO:Defining folds
2024-05-03 10:55:21,025:INFO:Declaring metric variables
2024-05-03 10:55:21,025:INFO:Importing untrained model
2024-05-03 10:55:21,025:INFO:Ridge Classifier Imported successfully
2024-05-03 10:55:21,025:INFO:Starting cross validation
2024-05-03 10:55:21,026:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:21,070:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:21,070:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:21,070:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:21,070:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:21,070:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:21,071:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:21,072:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:21,094:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:21,094:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:21,105:INFO:Calculating mean and std
2024-05-03 10:55:21,105:INFO:Creating metrics dataframe
2024-05-03 10:55:21,107:INFO:Uploading results into container
2024-05-03 10:55:21,107:INFO:Uploading model into container now
2024-05-03 10:55:21,107:INFO:_master_model_container: 6
2024-05-03 10:55:21,107:INFO:_display_container: 2
2024-05-03 10:55:21,108:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=1615, solver='auto',
                tol=0.0001)
2024-05-03 10:55:21,108:INFO:create_model() successfully completed......................................
2024-05-03 10:55:21,209:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:21,209:INFO:Creating metrics dataframe
2024-05-03 10:55:21,211:INFO:Initializing Random Forest Classifier
2024-05-03 10:55:21,211:INFO:Total runtime is 0.08869974613189696 minutes
2024-05-03 10:55:21,211:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:21,212:INFO:Initializing create_model()
2024-05-03 10:55:21,212:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:21,212:INFO:Checking exceptions
2024-05-03 10:55:21,212:INFO:Importing libraries
2024-05-03 10:55:21,212:INFO:Copying training dataset
2024-05-03 10:55:21,214:INFO:Defining folds
2024-05-03 10:55:21,214:INFO:Declaring metric variables
2024-05-03 10:55:21,214:INFO:Importing untrained model
2024-05-03 10:55:21,214:INFO:Random Forest Classifier Imported successfully
2024-05-03 10:55:21,215:INFO:Starting cross validation
2024-05-03 10:55:21,215:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:21,842:INFO:Calculating mean and std
2024-05-03 10:55:21,842:INFO:Creating metrics dataframe
2024-05-03 10:55:21,844:INFO:Uploading results into container
2024-05-03 10:55:21,844:INFO:Uploading model into container now
2024-05-03 10:55:21,845:INFO:_master_model_container: 7
2024-05-03 10:55:21,845:INFO:_display_container: 2
2024-05-03 10:55:21,845:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=1615, verbose=0,
                       warm_start=False)
2024-05-03 10:55:21,845:INFO:create_model() successfully completed......................................
2024-05-03 10:55:21,962:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:21,962:INFO:Creating metrics dataframe
2024-05-03 10:55:21,965:INFO:Initializing Quadratic Discriminant Analysis
2024-05-03 10:55:21,965:INFO:Total runtime is 0.10126434961954751 minutes
2024-05-03 10:55:21,965:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:21,966:INFO:Initializing create_model()
2024-05-03 10:55:21,966:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:21,966:INFO:Checking exceptions
2024-05-03 10:55:21,966:INFO:Importing libraries
2024-05-03 10:55:21,966:INFO:Copying training dataset
2024-05-03 10:55:21,969:INFO:Defining folds
2024-05-03 10:55:21,969:INFO:Declaring metric variables
2024-05-03 10:55:21,969:INFO:Importing untrained model
2024-05-03 10:55:21,970:INFO:Quadratic Discriminant Analysis Imported successfully
2024-05-03 10:55:21,970:INFO:Starting cross validation
2024-05-03 10:55:21,971:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:22,017:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,017:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,017:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,026:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,028:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,033:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,038:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,046:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,052:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,054:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,067:INFO:Calculating mean and std
2024-05-03 10:55:22,067:INFO:Creating metrics dataframe
2024-05-03 10:55:22,069:INFO:Uploading results into container
2024-05-03 10:55:22,069:INFO:Uploading model into container now
2024-05-03 10:55:22,069:INFO:_master_model_container: 8
2024-05-03 10:55:22,070:INFO:_display_container: 2
2024-05-03 10:55:22,070:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-05-03 10:55:22,070:INFO:create_model() successfully completed......................................
2024-05-03 10:55:22,182:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:22,183:INFO:Creating metrics dataframe
2024-05-03 10:55:22,185:INFO:Initializing Ada Boost Classifier
2024-05-03 10:55:22,185:INFO:Total runtime is 0.10492821534474689 minutes
2024-05-03 10:55:22,186:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:22,186:INFO:Initializing create_model()
2024-05-03 10:55:22,186:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:22,186:INFO:Checking exceptions
2024-05-03 10:55:22,186:INFO:Importing libraries
2024-05-03 10:55:22,186:INFO:Copying training dataset
2024-05-03 10:55:22,188:INFO:Defining folds
2024-05-03 10:55:22,188:INFO:Declaring metric variables
2024-05-03 10:55:22,188:INFO:Importing untrained model
2024-05-03 10:55:22,189:INFO:Ada Boost Classifier Imported successfully
2024-05-03 10:55:22,189:INFO:Starting cross validation
2024-05-03 10:55:22,190:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:22,213:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 10:55:22,213:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 10:55:22,213:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 10:55:22,213:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 10:55:22,214:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 10:55:22,216:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 10:55:22,216:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 10:55:22,343:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,344:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,349:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,350:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,356:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,359:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:22,364:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:22,364:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,365:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,366:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 10:55:22,368:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 10:55:22,371:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:22,397:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,449:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,452:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:22,452:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:22,467:INFO:Calculating mean and std
2024-05-03 10:55:22,467:INFO:Creating metrics dataframe
2024-05-03 10:55:22,469:INFO:Uploading results into container
2024-05-03 10:55:22,469:INFO:Uploading model into container now
2024-05-03 10:55:22,470:INFO:_master_model_container: 9
2024-05-03 10:55:22,470:INFO:_display_container: 2
2024-05-03 10:55:22,470:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=1615)
2024-05-03 10:55:22,470:INFO:create_model() successfully completed......................................
2024-05-03 10:55:22,578:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:22,579:INFO:Creating metrics dataframe
2024-05-03 10:55:22,581:INFO:Initializing Gradient Boosting Classifier
2024-05-03 10:55:22,582:INFO:Total runtime is 0.11154098510742186 minutes
2024-05-03 10:55:22,582:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:22,582:INFO:Initializing create_model()
2024-05-03 10:55:22,582:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:22,582:INFO:Checking exceptions
2024-05-03 10:55:22,582:INFO:Importing libraries
2024-05-03 10:55:22,582:INFO:Copying training dataset
2024-05-03 10:55:22,585:INFO:Defining folds
2024-05-03 10:55:22,585:INFO:Declaring metric variables
2024-05-03 10:55:22,585:INFO:Importing untrained model
2024-05-03 10:55:22,585:INFO:Gradient Boosting Classifier Imported successfully
2024-05-03 10:55:22,586:INFO:Starting cross validation
2024-05-03 10:55:22,587:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:23,186:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,198:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,236:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,242:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,361:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,364:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,395:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,594:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,606:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,616:INFO:Calculating mean and std
2024-05-03 10:55:23,616:INFO:Creating metrics dataframe
2024-05-03 10:55:23,618:INFO:Uploading results into container
2024-05-03 10:55:23,619:INFO:Uploading model into container now
2024-05-03 10:55:23,619:INFO:_master_model_container: 10
2024-05-03 10:55:23,619:INFO:_display_container: 2
2024-05-03 10:55:23,619:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1615, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-05-03 10:55:23,619:INFO:create_model() successfully completed......................................
2024-05-03 10:55:23,723:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:23,723:INFO:Creating metrics dataframe
2024-05-03 10:55:23,726:INFO:Initializing Linear Discriminant Analysis
2024-05-03 10:55:23,726:INFO:Total runtime is 0.13061217466990152 minutes
2024-05-03 10:55:23,726:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:23,726:INFO:Initializing create_model()
2024-05-03 10:55:23,726:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:23,726:INFO:Checking exceptions
2024-05-03 10:55:23,726:INFO:Importing libraries
2024-05-03 10:55:23,727:INFO:Copying training dataset
2024-05-03 10:55:23,729:INFO:Defining folds
2024-05-03 10:55:23,729:INFO:Declaring metric variables
2024-05-03 10:55:23,729:INFO:Importing untrained model
2024-05-03 10:55:23,729:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 10:55:23,730:INFO:Starting cross validation
2024-05-03 10:55:23,730:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:23,767:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,767:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,767:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,767:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,767:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,767:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,767:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,784:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,793:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,794:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 10:55:23,812:INFO:Calculating mean and std
2024-05-03 10:55:23,812:INFO:Creating metrics dataframe
2024-05-03 10:55:23,815:INFO:Uploading results into container
2024-05-03 10:55:23,815:INFO:Uploading model into container now
2024-05-03 10:55:23,815:INFO:_master_model_container: 11
2024-05-03 10:55:23,815:INFO:_display_container: 2
2024-05-03 10:55:23,816:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 10:55:23,816:INFO:create_model() successfully completed......................................
2024-05-03 10:55:23,917:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:23,917:INFO:Creating metrics dataframe
2024-05-03 10:55:23,919:INFO:Initializing Extra Trees Classifier
2024-05-03 10:55:23,919:INFO:Total runtime is 0.13383518060048422 minutes
2024-05-03 10:55:23,919:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:23,920:INFO:Initializing create_model()
2024-05-03 10:55:23,920:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:23,920:INFO:Checking exceptions
2024-05-03 10:55:23,920:INFO:Importing libraries
2024-05-03 10:55:23,920:INFO:Copying training dataset
2024-05-03 10:55:23,922:INFO:Defining folds
2024-05-03 10:55:23,923:INFO:Declaring metric variables
2024-05-03 10:55:23,923:INFO:Importing untrained model
2024-05-03 10:55:23,923:INFO:Extra Trees Classifier Imported successfully
2024-05-03 10:55:23,923:INFO:Starting cross validation
2024-05-03 10:55:23,924:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:24,325:INFO:Calculating mean and std
2024-05-03 10:55:24,326:INFO:Creating metrics dataframe
2024-05-03 10:55:24,327:INFO:Uploading results into container
2024-05-03 10:55:24,328:INFO:Uploading model into container now
2024-05-03 10:55:24,328:INFO:_master_model_container: 12
2024-05-03 10:55:24,328:INFO:_display_container: 2
2024-05-03 10:55:24,328:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=1615, verbose=0,
                     warm_start=False)
2024-05-03 10:55:24,328:INFO:create_model() successfully completed......................................
2024-05-03 10:55:24,439:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:24,439:INFO:Creating metrics dataframe
2024-05-03 10:55:24,442:INFO:Initializing Light Gradient Boosting Machine
2024-05-03 10:55:24,442:INFO:Total runtime is 0.1425482511520386 minutes
2024-05-03 10:55:24,442:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:24,442:INFO:Initializing create_model()
2024-05-03 10:55:24,442:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:24,442:INFO:Checking exceptions
2024-05-03 10:55:24,442:INFO:Importing libraries
2024-05-03 10:55:24,442:INFO:Copying training dataset
2024-05-03 10:55:24,445:INFO:Defining folds
2024-05-03 10:55:24,445:INFO:Declaring metric variables
2024-05-03 10:55:24,445:INFO:Importing untrained model
2024-05-03 10:55:24,446:INFO:Light Gradient Boosting Machine Imported successfully
2024-05-03 10:55:24,446:INFO:Starting cross validation
2024-05-03 10:55:24,446:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:25,368:INFO:Calculating mean and std
2024-05-03 10:55:25,369:INFO:Creating metrics dataframe
2024-05-03 10:55:25,371:INFO:Uploading results into container
2024-05-03 10:55:25,371:INFO:Uploading model into container now
2024-05-03 10:55:25,372:INFO:_master_model_container: 13
2024-05-03 10:55:25,372:INFO:_display_container: 2
2024-05-03 10:55:25,372:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1615, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-05-03 10:55:25,372:INFO:create_model() successfully completed......................................
2024-05-03 10:55:25,504:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:25,504:INFO:Creating metrics dataframe
2024-05-03 10:55:25,507:INFO:Initializing Dummy Classifier
2024-05-03 10:55:25,507:INFO:Total runtime is 0.16029409567515057 minutes
2024-05-03 10:55:25,507:INFO:SubProcess create_model() called ==================================
2024-05-03 10:55:25,507:INFO:Initializing create_model()
2024-05-03 10:55:25,507:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:25,508:INFO:Checking exceptions
2024-05-03 10:55:25,508:INFO:Importing libraries
2024-05-03 10:55:25,508:INFO:Copying training dataset
2024-05-03 10:55:25,510:INFO:Defining folds
2024-05-03 10:55:25,510:INFO:Declaring metric variables
2024-05-03 10:55:25,511:INFO:Importing untrained model
2024-05-03 10:55:25,511:INFO:Dummy Classifier Imported successfully
2024-05-03 10:55:25,511:INFO:Starting cross validation
2024-05-03 10:55:25,512:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 10:55:25,542:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,544:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,544:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,547:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,547:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,548:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,549:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,563:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,569:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,570:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 10:55:25,576:INFO:Calculating mean and std
2024-05-03 10:55:25,576:INFO:Creating metrics dataframe
2024-05-03 10:55:25,578:INFO:Uploading results into container
2024-05-03 10:55:25,578:INFO:Uploading model into container now
2024-05-03 10:55:25,578:INFO:_master_model_container: 14
2024-05-03 10:55:25,579:INFO:_display_container: 2
2024-05-03 10:55:25,579:INFO:DummyClassifier(constant=None, random_state=1615, strategy='prior')
2024-05-03 10:55:25,579:INFO:create_model() successfully completed......................................
2024-05-03 10:55:25,697:INFO:SubProcess create_model() end ==================================
2024-05-03 10:55:25,697:INFO:Creating metrics dataframe
2024-05-03 10:55:25,701:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-05-03 10:55:25,703:INFO:Initializing create_model()
2024-05-03 10:55:25,703:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD42F910>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 10:55:25,703:INFO:Checking exceptions
2024-05-03 10:55:25,704:INFO:Importing libraries
2024-05-03 10:55:25,704:INFO:Copying training dataset
2024-05-03 10:55:25,707:INFO:Defining folds
2024-05-03 10:55:25,707:INFO:Declaring metric variables
2024-05-03 10:55:25,707:INFO:Importing untrained model
2024-05-03 10:55:25,707:INFO:Declaring custom model
2024-05-03 10:55:25,708:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 10:55:25,708:INFO:Cross validation set to False
2024-05-03 10:55:25,709:INFO:Fitting Model
2024-05-03 10:55:25,715:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 10:55:25,715:INFO:create_model() successfully completed......................................
2024-05-03 10:55:25,845:INFO:_master_model_container: 14
2024-05-03 10:55:25,845:INFO:_display_container: 2
2024-05-03 10:55:25,846:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 10:55:25,846:INFO:compare_models() successfully completed......................................
2024-05-03 11:00:13,225:INFO:PyCaret ClassificationExperiment
2024-05-03 11:00:13,225:INFO:Logging name: clf-default-name
2024-05-03 11:00:13,225:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-03 11:00:13,225:INFO:version 3.3.2
2024-05-03 11:00:13,225:INFO:Initializing setup()
2024-05-03 11:00:13,225:INFO:self.USI: a700
2024-05-03 11:00:13,225:INFO:self._variable_keys: {'n_jobs_param', '_available_plots', 'exp_name_log', 'X_train', 'pipeline', 'USI', '_ml_usecase', 'html_param', 'exp_id', 'y_test', 'y_train', 'target_param', 'fold_groups_param', 'X', 'X_test', 'gpu_n_jobs_param', 'idx', 'logging_param', 'fold_shuffle_param', 'fix_imbalance', 'seed', 'data', 'gpu_param', 'memory', 'log_plots_param', 'y', 'is_multiclass', 'fold_generator'}
2024-05-03 11:00:13,225:INFO:Checking environment
2024-05-03 11:00:13,225:INFO:python_version: 3.11.7
2024-05-03 11:00:13,225:INFO:python_build: ('tags/v3.11.7:fa7a6f2', 'Dec  4 2023 19:24:49')
2024-05-03 11:00:13,225:INFO:machine: AMD64
2024-05-03 11:00:13,225:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-03 11:00:13,230:INFO:Memory: svmem(total=16939401216, available=1717604352, percent=89.9, used=15221796864, free=1717604352)
2024-05-03 11:00:13,230:INFO:Physical Core: 4
2024-05-03 11:00:13,230:INFO:Logical Core: 8
2024-05-03 11:00:13,230:INFO:Checking libraries
2024-05-03 11:00:13,230:INFO:System:
2024-05-03 11:00:13,231:INFO:    python: 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]
2024-05-03 11:00:13,231:INFO:executable: Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Scripts\python.exe
2024-05-03 11:00:13,231:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-03 11:00:13,231:INFO:PyCaret required dependencies:
2024-05-03 11:00:13,231:INFO:                 pip: 23.2.1
2024-05-03 11:00:13,231:INFO:          setuptools: 68.2.0
2024-05-03 11:00:13,231:INFO:             pycaret: 3.3.2
2024-05-03 11:00:13,231:INFO:             IPython: 8.24.0
2024-05-03 11:00:13,231:INFO:          ipywidgets: 8.1.2
2024-05-03 11:00:13,231:INFO:                tqdm: 4.66.4
2024-05-03 11:00:13,231:INFO:               numpy: 1.26.4
2024-05-03 11:00:13,231:INFO:              pandas: 2.1.4
2024-05-03 11:00:13,231:INFO:              jinja2: 3.1.3
2024-05-03 11:00:13,231:INFO:               scipy: 1.11.4
2024-05-03 11:00:13,232:INFO:              joblib: 1.3.2
2024-05-03 11:00:13,232:INFO:             sklearn: 1.4.2
2024-05-03 11:00:13,232:INFO:                pyod: 1.1.3
2024-05-03 11:00:13,232:INFO:            imblearn: 0.12.2
2024-05-03 11:00:13,232:INFO:   category_encoders: 2.6.3
2024-05-03 11:00:13,232:INFO:            lightgbm: 4.3.0
2024-05-03 11:00:13,232:INFO:               numba: 0.58.1
2024-05-03 11:00:13,232:INFO:            requests: 2.31.0
2024-05-03 11:00:13,232:INFO:          matplotlib: 3.7.5
2024-05-03 11:00:13,232:INFO:          scikitplot: 0.3.7
2024-05-03 11:00:13,232:INFO:         yellowbrick: 1.5
2024-05-03 11:00:13,232:INFO:              plotly: 5.22.0
2024-05-03 11:00:13,232:INFO:    plotly-resampler: Not installed
2024-05-03 11:00:13,232:INFO:             kaleido: 0.2.1
2024-05-03 11:00:13,232:INFO:           schemdraw: 0.15
2024-05-03 11:00:13,232:INFO:         statsmodels: 0.14.2
2024-05-03 11:00:13,232:INFO:              sktime: 0.26.0
2024-05-03 11:00:13,233:INFO:               tbats: 1.1.3
2024-05-03 11:00:13,233:INFO:            pmdarima: 2.0.4
2024-05-03 11:00:13,233:INFO:              psutil: 5.9.8
2024-05-03 11:00:13,233:INFO:          markupsafe: 2.1.5
2024-05-03 11:00:13,233:INFO:             pickle5: Not installed
2024-05-03 11:00:13,233:INFO:         cloudpickle: 3.0.0
2024-05-03 11:00:13,233:INFO:         deprecation: 2.1.0
2024-05-03 11:00:13,233:INFO:              xxhash: 3.4.1
2024-05-03 11:00:13,233:INFO:           wurlitzer: Not installed
2024-05-03 11:00:13,233:INFO:PyCaret optional dependencies:
2024-05-03 11:00:13,233:INFO:                shap: Not installed
2024-05-03 11:00:13,233:INFO:           interpret: Not installed
2024-05-03 11:00:13,233:INFO:                umap: Not installed
2024-05-03 11:00:13,233:INFO:     ydata_profiling: 4.7.0
2024-05-03 11:00:13,233:INFO:  explainerdashboard: Not installed
2024-05-03 11:00:13,233:INFO:             autoviz: Not installed
2024-05-03 11:00:13,233:INFO:           fairlearn: Not installed
2024-05-03 11:00:13,233:INFO:          deepchecks: Not installed
2024-05-03 11:00:13,233:INFO:             xgboost: Not installed
2024-05-03 11:00:13,233:INFO:            catboost: Not installed
2024-05-03 11:00:13,233:INFO:              kmodes: Not installed
2024-05-03 11:00:13,233:INFO:             mlxtend: Not installed
2024-05-03 11:00:13,233:INFO:       statsforecast: Not installed
2024-05-03 11:00:13,233:INFO:        tune_sklearn: Not installed
2024-05-03 11:00:13,235:INFO:                 ray: Not installed
2024-05-03 11:00:13,235:INFO:            hyperopt: Not installed
2024-05-03 11:00:13,235:INFO:              optuna: Not installed
2024-05-03 11:00:13,235:INFO:               skopt: Not installed
2024-05-03 11:00:13,235:INFO:              mlflow: Not installed
2024-05-03 11:00:13,235:INFO:              gradio: Not installed
2024-05-03 11:00:13,235:INFO:             fastapi: Not installed
2024-05-03 11:00:13,235:INFO:             uvicorn: Not installed
2024-05-03 11:00:13,235:INFO:              m2cgen: Not installed
2024-05-03 11:00:13,235:INFO:           evidently: Not installed
2024-05-03 11:00:13,235:INFO:               fugue: Not installed
2024-05-03 11:00:13,235:INFO:           streamlit: 1.34.0
2024-05-03 11:00:13,235:INFO:             prophet: Not installed
2024-05-03 11:00:13,235:INFO:None
2024-05-03 11:00:13,235:INFO:Set up data.
2024-05-03 11:00:13,239:INFO:Set up folding strategy.
2024-05-03 11:00:13,239:INFO:Set up train/test split.
2024-05-03 11:00:13,243:INFO:Set up index.
2024-05-03 11:00:13,243:INFO:Assigning column types.
2024-05-03 11:00:13,246:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-05-03 11:00:13,293:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:00:13,294:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:00:13,322:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,366:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:00:13,367:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:00:13,395:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,395:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,396:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-05-03 11:00:13,443:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:00:13,469:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,469:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,510:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:00:13,539:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,539:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,539:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-05-03 11:00:13,603:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,603:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,666:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,666:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,667:INFO:Preparing preprocessing pipeline...
2024-05-03 11:00:13,668:INFO:Set up simple imputation.
2024-05-03 11:00:13,679:INFO:Finished creating preprocessing pipeline.
2024-05-03 11:00:13,682:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-05-03 11:00:13,682:INFO:Creating final display dataframe.
2024-05-03 11:00:13,728:INFO:Setup _display_container:                     Description             Value
0                    Session id              5450
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (210, 8)
4        Transformed data shape          (210, 8)
5   Transformed train set shape          (147, 8)
6    Transformed test set shape           (63, 8)
7              Numeric features                 7
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              a700
2024-05-03 11:00:13,803:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,803:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,865:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,866:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:00:13,867:INFO:setup() successfully completed in 0.64s...............
2024-05-03 11:00:13,869:INFO:Initializing compare_models()
2024-05-03 11:00:13,869:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-05-03 11:00:13,870:INFO:Checking exceptions
2024-05-03 11:00:13,872:INFO:Preparing display monitor
2024-05-03 11:00:13,873:INFO:Initializing Logistic Regression
2024-05-03 11:00:13,873:INFO:Total runtime is 0.0 minutes
2024-05-03 11:00:13,875:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:13,875:INFO:Initializing create_model()
2024-05-03 11:00:13,875:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:13,875:INFO:Checking exceptions
2024-05-03 11:00:13,875:INFO:Importing libraries
2024-05-03 11:00:13,875:INFO:Copying training dataset
2024-05-03 11:00:13,877:INFO:Defining folds
2024-05-03 11:00:13,877:INFO:Declaring metric variables
2024-05-03 11:00:13,877:INFO:Importing untrained model
2024-05-03 11:00:13,877:INFO:Logistic Regression Imported successfully
2024-05-03 11:00:13,877:INFO:Starting cross validation
2024-05-03 11:00:13,878:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:13,969:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:13,982:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:13,992:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:13,996:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,007:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,020:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,022:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,023:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,060:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,078:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,088:INFO:Calculating mean and std
2024-05-03 11:00:14,088:INFO:Creating metrics dataframe
2024-05-03 11:00:14,090:INFO:Uploading results into container
2024-05-03 11:00:14,090:INFO:Uploading model into container now
2024-05-03 11:00:14,090:INFO:_master_model_container: 1
2024-05-03 11:00:14,090:INFO:_display_container: 2
2024-05-03 11:00:14,091:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5450, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-05-03 11:00:14,091:INFO:create_model() successfully completed......................................
2024-05-03 11:00:14,193:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:14,193:INFO:Creating metrics dataframe
2024-05-03 11:00:14,196:INFO:Initializing K Neighbors Classifier
2024-05-03 11:00:14,196:INFO:Total runtime is 0.005378107229868571 minutes
2024-05-03 11:00:14,196:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:14,197:INFO:Initializing create_model()
2024-05-03 11:00:14,197:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:14,197:INFO:Checking exceptions
2024-05-03 11:00:14,197:INFO:Importing libraries
2024-05-03 11:00:14,197:INFO:Copying training dataset
2024-05-03 11:00:14,199:INFO:Defining folds
2024-05-03 11:00:14,199:INFO:Declaring metric variables
2024-05-03 11:00:14,199:INFO:Importing untrained model
2024-05-03 11:00:14,200:INFO:K Neighbors Classifier Imported successfully
2024-05-03 11:00:14,200:INFO:Starting cross validation
2024-05-03 11:00:14,201:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:14,338:INFO:Calculating mean and std
2024-05-03 11:00:14,338:INFO:Creating metrics dataframe
2024-05-03 11:00:14,340:INFO:Uploading results into container
2024-05-03 11:00:14,340:INFO:Uploading model into container now
2024-05-03 11:00:14,340:INFO:_master_model_container: 2
2024-05-03 11:00:14,340:INFO:_display_container: 2
2024-05-03 11:00:14,340:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-05-03 11:00:14,340:INFO:create_model() successfully completed......................................
2024-05-03 11:00:14,442:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:14,442:INFO:Creating metrics dataframe
2024-05-03 11:00:14,446:INFO:Initializing Naive Bayes
2024-05-03 11:00:14,447:INFO:Total runtime is 0.009563008944193523 minutes
2024-05-03 11:00:14,447:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:14,447:INFO:Initializing create_model()
2024-05-03 11:00:14,447:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:14,447:INFO:Checking exceptions
2024-05-03 11:00:14,447:INFO:Importing libraries
2024-05-03 11:00:14,447:INFO:Copying training dataset
2024-05-03 11:00:14,450:INFO:Defining folds
2024-05-03 11:00:14,450:INFO:Declaring metric variables
2024-05-03 11:00:14,450:INFO:Importing untrained model
2024-05-03 11:00:14,450:INFO:Naive Bayes Imported successfully
2024-05-03 11:00:14,451:INFO:Starting cross validation
2024-05-03 11:00:14,451:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:14,525:INFO:Calculating mean and std
2024-05-03 11:00:14,525:INFO:Creating metrics dataframe
2024-05-03 11:00:14,526:INFO:Uploading results into container
2024-05-03 11:00:14,526:INFO:Uploading model into container now
2024-05-03 11:00:14,527:INFO:_master_model_container: 3
2024-05-03 11:00:14,527:INFO:_display_container: 2
2024-05-03 11:00:14,527:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-05-03 11:00:14,527:INFO:create_model() successfully completed......................................
2024-05-03 11:00:14,633:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:14,633:INFO:Creating metrics dataframe
2024-05-03 11:00:14,635:INFO:Initializing Decision Tree Classifier
2024-05-03 11:00:14,636:INFO:Total runtime is 0.012725385030110678 minutes
2024-05-03 11:00:14,636:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:14,636:INFO:Initializing create_model()
2024-05-03 11:00:14,636:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:14,636:INFO:Checking exceptions
2024-05-03 11:00:14,636:INFO:Importing libraries
2024-05-03 11:00:14,636:INFO:Copying training dataset
2024-05-03 11:00:14,639:INFO:Defining folds
2024-05-03 11:00:14,639:INFO:Declaring metric variables
2024-05-03 11:00:14,639:INFO:Importing untrained model
2024-05-03 11:00:14,639:INFO:Decision Tree Classifier Imported successfully
2024-05-03 11:00:14,640:INFO:Starting cross validation
2024-05-03 11:00:14,640:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:14,717:INFO:Calculating mean and std
2024-05-03 11:00:14,718:INFO:Creating metrics dataframe
2024-05-03 11:00:14,719:INFO:Uploading results into container
2024-05-03 11:00:14,720:INFO:Uploading model into container now
2024-05-03 11:00:14,720:INFO:_master_model_container: 4
2024-05-03 11:00:14,720:INFO:_display_container: 2
2024-05-03 11:00:14,720:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=5450, splitter='best')
2024-05-03 11:00:14,720:INFO:create_model() successfully completed......................................
2024-05-03 11:00:14,821:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:14,821:INFO:Creating metrics dataframe
2024-05-03 11:00:14,823:INFO:Initializing SVM - Linear Kernel
2024-05-03 11:00:14,823:INFO:Total runtime is 0.015840379397074382 minutes
2024-05-03 11:00:14,823:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:14,824:INFO:Initializing create_model()
2024-05-03 11:00:14,824:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:14,824:INFO:Checking exceptions
2024-05-03 11:00:14,824:INFO:Importing libraries
2024-05-03 11:00:14,824:INFO:Copying training dataset
2024-05-03 11:00:14,827:INFO:Defining folds
2024-05-03 11:00:14,827:INFO:Declaring metric variables
2024-05-03 11:00:14,827:INFO:Importing untrained model
2024-05-03 11:00:14,827:INFO:SVM - Linear Kernel Imported successfully
2024-05-03 11:00:14,827:INFO:Starting cross validation
2024-05-03 11:00:14,829:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:14,873:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,873:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,873:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,877:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,877:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,879:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,880:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,892:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,914:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,915:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:14,919:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:14,929:INFO:Calculating mean and std
2024-05-03 11:00:14,929:INFO:Creating metrics dataframe
2024-05-03 11:00:14,930:INFO:Uploading results into container
2024-05-03 11:00:14,931:INFO:Uploading model into container now
2024-05-03 11:00:14,931:INFO:_master_model_container: 5
2024-05-03 11:00:14,931:INFO:_display_container: 2
2024-05-03 11:00:14,931:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5450, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-05-03 11:00:14,931:INFO:create_model() successfully completed......................................
2024-05-03 11:00:15,032:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:15,032:INFO:Creating metrics dataframe
2024-05-03 11:00:15,033:INFO:Initializing Ridge Classifier
2024-05-03 11:00:15,035:INFO:Total runtime is 0.01933811108271281 minutes
2024-05-03 11:00:15,035:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:15,035:INFO:Initializing create_model()
2024-05-03 11:00:15,035:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:15,035:INFO:Checking exceptions
2024-05-03 11:00:15,035:INFO:Importing libraries
2024-05-03 11:00:15,035:INFO:Copying training dataset
2024-05-03 11:00:15,038:INFO:Defining folds
2024-05-03 11:00:15,038:INFO:Declaring metric variables
2024-05-03 11:00:15,038:INFO:Importing untrained model
2024-05-03 11:00:15,038:INFO:Ridge Classifier Imported successfully
2024-05-03 11:00:15,039:INFO:Starting cross validation
2024-05-03 11:00:15,039:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:15,065:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,066:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,067:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,069:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,070:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,071:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,072:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,080:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,091:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,097:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,114:INFO:Calculating mean and std
2024-05-03 11:00:15,115:INFO:Creating metrics dataframe
2024-05-03 11:00:15,115:INFO:Uploading results into container
2024-05-03 11:00:15,116:INFO:Uploading model into container now
2024-05-03 11:00:15,116:INFO:_master_model_container: 6
2024-05-03 11:00:15,116:INFO:_display_container: 2
2024-05-03 11:00:15,116:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5450, solver='auto',
                tol=0.0001)
2024-05-03 11:00:15,116:INFO:create_model() successfully completed......................................
2024-05-03 11:00:15,218:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:15,218:INFO:Creating metrics dataframe
2024-05-03 11:00:15,220:INFO:Initializing Random Forest Classifier
2024-05-03 11:00:15,220:INFO:Total runtime is 0.022447431087493898 minutes
2024-05-03 11:00:15,220:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:15,221:INFO:Initializing create_model()
2024-05-03 11:00:15,221:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:15,221:INFO:Checking exceptions
2024-05-03 11:00:15,221:INFO:Importing libraries
2024-05-03 11:00:15,221:INFO:Copying training dataset
2024-05-03 11:00:15,223:INFO:Defining folds
2024-05-03 11:00:15,224:INFO:Declaring metric variables
2024-05-03 11:00:15,224:INFO:Importing untrained model
2024-05-03 11:00:15,224:INFO:Random Forest Classifier Imported successfully
2024-05-03 11:00:15,225:INFO:Starting cross validation
2024-05-03 11:00:15,225:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:15,697:INFO:Calculating mean and std
2024-05-03 11:00:15,697:INFO:Creating metrics dataframe
2024-05-03 11:00:15,699:INFO:Uploading results into container
2024-05-03 11:00:15,699:INFO:Uploading model into container now
2024-05-03 11:00:15,699:INFO:_master_model_container: 7
2024-05-03 11:00:15,699:INFO:_display_container: 2
2024-05-03 11:00:15,699:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5450, verbose=0,
                       warm_start=False)
2024-05-03 11:00:15,700:INFO:create_model() successfully completed......................................
2024-05-03 11:00:15,806:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:15,806:INFO:Creating metrics dataframe
2024-05-03 11:00:15,808:INFO:Initializing Quadratic Discriminant Analysis
2024-05-03 11:00:15,808:INFO:Total runtime is 0.032258462905883786 minutes
2024-05-03 11:00:15,809:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:15,809:INFO:Initializing create_model()
2024-05-03 11:00:15,809:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:15,809:INFO:Checking exceptions
2024-05-03 11:00:15,809:INFO:Importing libraries
2024-05-03 11:00:15,809:INFO:Copying training dataset
2024-05-03 11:00:15,812:INFO:Defining folds
2024-05-03 11:00:15,812:INFO:Declaring metric variables
2024-05-03 11:00:15,812:INFO:Importing untrained model
2024-05-03 11:00:15,812:INFO:Quadratic Discriminant Analysis Imported successfully
2024-05-03 11:00:15,812:INFO:Starting cross validation
2024-05-03 11:00:15,813:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:15,837:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,838:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,839:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,842:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,843:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,843:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,847:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,850:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,865:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,867:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:15,882:INFO:Calculating mean and std
2024-05-03 11:00:15,882:INFO:Creating metrics dataframe
2024-05-03 11:00:15,884:INFO:Uploading results into container
2024-05-03 11:00:15,885:INFO:Uploading model into container now
2024-05-03 11:00:15,885:INFO:_master_model_container: 8
2024-05-03 11:00:15,885:INFO:_display_container: 2
2024-05-03 11:00:15,885:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-05-03 11:00:15,885:INFO:create_model() successfully completed......................................
2024-05-03 11:00:15,993:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:15,993:INFO:Creating metrics dataframe
2024-05-03 11:00:15,995:INFO:Initializing Ada Boost Classifier
2024-05-03 11:00:15,995:INFO:Total runtime is 0.03536044359207153 minutes
2024-05-03 11:00:15,996:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:15,996:INFO:Initializing create_model()
2024-05-03 11:00:15,996:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:15,996:INFO:Checking exceptions
2024-05-03 11:00:15,996:INFO:Importing libraries
2024-05-03 11:00:15,996:INFO:Copying training dataset
2024-05-03 11:00:15,999:INFO:Defining folds
2024-05-03 11:00:15,999:INFO:Declaring metric variables
2024-05-03 11:00:15,999:INFO:Importing untrained model
2024-05-03 11:00:15,999:INFO:Ada Boost Classifier Imported successfully
2024-05-03 11:00:15,999:INFO:Starting cross validation
2024-05-03 11:00:15,999:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:16,015:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,016:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,017:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,018:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,020:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,021:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,023:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,027:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,145:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,146:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,148:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,152:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,152:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,154:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:16,155:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,156:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,160:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:16,161:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:16,166:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,170:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:00:16,172:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,176:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:16,247:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,249:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:16,256:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,259:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:16,267:INFO:Calculating mean and std
2024-05-03 11:00:16,268:INFO:Creating metrics dataframe
2024-05-03 11:00:16,269:INFO:Uploading results into container
2024-05-03 11:00:16,269:INFO:Uploading model into container now
2024-05-03 11:00:16,270:INFO:_master_model_container: 9
2024-05-03 11:00:16,270:INFO:_display_container: 2
2024-05-03 11:00:16,270:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=5450)
2024-05-03 11:00:16,270:INFO:create_model() successfully completed......................................
2024-05-03 11:00:16,372:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:16,372:INFO:Creating metrics dataframe
2024-05-03 11:00:16,374:INFO:Initializing Gradient Boosting Classifier
2024-05-03 11:00:16,374:INFO:Total runtime is 0.041679739952087395 minutes
2024-05-03 11:00:16,374:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:16,375:INFO:Initializing create_model()
2024-05-03 11:00:16,375:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:16,375:INFO:Checking exceptions
2024-05-03 11:00:16,375:INFO:Importing libraries
2024-05-03 11:00:16,375:INFO:Copying training dataset
2024-05-03 11:00:16,377:INFO:Defining folds
2024-05-03 11:00:16,377:INFO:Declaring metric variables
2024-05-03 11:00:16,377:INFO:Importing untrained model
2024-05-03 11:00:16,377:INFO:Gradient Boosting Classifier Imported successfully
2024-05-03 11:00:16,378:INFO:Starting cross validation
2024-05-03 11:00:16,378:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:16,846:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,847:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,851:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,856:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,888:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,891:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,896:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:16,914:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,169:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,177:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,192:INFO:Calculating mean and std
2024-05-03 11:00:17,193:INFO:Creating metrics dataframe
2024-05-03 11:00:17,193:INFO:Uploading results into container
2024-05-03 11:00:17,194:INFO:Uploading model into container now
2024-05-03 11:00:17,194:INFO:_master_model_container: 10
2024-05-03 11:00:17,194:INFO:_display_container: 2
2024-05-03 11:00:17,195:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5450, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-05-03 11:00:17,195:INFO:create_model() successfully completed......................................
2024-05-03 11:00:17,296:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:17,296:INFO:Creating metrics dataframe
2024-05-03 11:00:17,298:INFO:Initializing Linear Discriminant Analysis
2024-05-03 11:00:17,298:INFO:Total runtime is 0.05708266894022623 minutes
2024-05-03 11:00:17,298:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:17,298:INFO:Initializing create_model()
2024-05-03 11:00:17,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:17,298:INFO:Checking exceptions
2024-05-03 11:00:17,298:INFO:Importing libraries
2024-05-03 11:00:17,298:INFO:Copying training dataset
2024-05-03 11:00:17,301:INFO:Defining folds
2024-05-03 11:00:17,301:INFO:Declaring metric variables
2024-05-03 11:00:17,301:INFO:Importing untrained model
2024-05-03 11:00:17,301:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:00:17,302:INFO:Starting cross validation
2024-05-03 11:00:17,302:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:17,326:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,327:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,329:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,331:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,332:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,333:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,335:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,342:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,353:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,353:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:00:17,363:INFO:Calculating mean and std
2024-05-03 11:00:17,363:INFO:Creating metrics dataframe
2024-05-03 11:00:17,365:INFO:Uploading results into container
2024-05-03 11:00:17,365:INFO:Uploading model into container now
2024-05-03 11:00:17,365:INFO:_master_model_container: 11
2024-05-03 11:00:17,365:INFO:_display_container: 2
2024-05-03 11:00:17,366:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:00:17,366:INFO:create_model() successfully completed......................................
2024-05-03 11:00:17,469:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:17,469:INFO:Creating metrics dataframe
2024-05-03 11:00:17,471:INFO:Initializing Extra Trees Classifier
2024-05-03 11:00:17,472:INFO:Total runtime is 0.0599757432937622 minutes
2024-05-03 11:00:17,472:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:17,472:INFO:Initializing create_model()
2024-05-03 11:00:17,472:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:17,472:INFO:Checking exceptions
2024-05-03 11:00:17,472:INFO:Importing libraries
2024-05-03 11:00:17,472:INFO:Copying training dataset
2024-05-03 11:00:17,475:INFO:Defining folds
2024-05-03 11:00:17,475:INFO:Declaring metric variables
2024-05-03 11:00:17,475:INFO:Importing untrained model
2024-05-03 11:00:17,475:INFO:Extra Trees Classifier Imported successfully
2024-05-03 11:00:17,475:INFO:Starting cross validation
2024-05-03 11:00:17,476:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:17,875:INFO:Calculating mean and std
2024-05-03 11:00:17,875:INFO:Creating metrics dataframe
2024-05-03 11:00:17,876:INFO:Uploading results into container
2024-05-03 11:00:17,877:INFO:Uploading model into container now
2024-05-03 11:00:17,877:INFO:_master_model_container: 12
2024-05-03 11:00:17,877:INFO:_display_container: 2
2024-05-03 11:00:17,877:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=5450, verbose=0,
                     warm_start=False)
2024-05-03 11:00:17,877:INFO:create_model() successfully completed......................................
2024-05-03 11:00:17,991:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:17,991:INFO:Creating metrics dataframe
2024-05-03 11:00:17,993:INFO:Initializing Light Gradient Boosting Machine
2024-05-03 11:00:17,993:INFO:Total runtime is 0.0686613917350769 minutes
2024-05-03 11:00:17,993:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:17,993:INFO:Initializing create_model()
2024-05-03 11:00:17,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:17,993:INFO:Checking exceptions
2024-05-03 11:00:17,993:INFO:Importing libraries
2024-05-03 11:00:17,993:INFO:Copying training dataset
2024-05-03 11:00:17,995:INFO:Defining folds
2024-05-03 11:00:17,995:INFO:Declaring metric variables
2024-05-03 11:00:17,996:INFO:Importing untrained model
2024-05-03 11:00:17,996:INFO:Light Gradient Boosting Machine Imported successfully
2024-05-03 11:00:17,996:INFO:Starting cross validation
2024-05-03 11:00:17,997:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:18,772:INFO:Calculating mean and std
2024-05-03 11:00:18,773:INFO:Creating metrics dataframe
2024-05-03 11:00:18,774:INFO:Uploading results into container
2024-05-03 11:00:18,775:INFO:Uploading model into container now
2024-05-03 11:00:18,775:INFO:_master_model_container: 13
2024-05-03 11:00:18,775:INFO:_display_container: 2
2024-05-03 11:00:18,776:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5450, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-05-03 11:00:18,776:INFO:create_model() successfully completed......................................
2024-05-03 11:00:18,906:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:18,907:INFO:Creating metrics dataframe
2024-05-03 11:00:18,909:INFO:Initializing Dummy Classifier
2024-05-03 11:00:18,909:INFO:Total runtime is 0.08393394152323405 minutes
2024-05-03 11:00:18,909:INFO:SubProcess create_model() called ==================================
2024-05-03 11:00:18,909:INFO:Initializing create_model()
2024-05-03 11:00:18,909:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD286DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:18,909:INFO:Checking exceptions
2024-05-03 11:00:18,909:INFO:Importing libraries
2024-05-03 11:00:18,909:INFO:Copying training dataset
2024-05-03 11:00:18,912:INFO:Defining folds
2024-05-03 11:00:18,912:INFO:Declaring metric variables
2024-05-03 11:00:18,912:INFO:Importing untrained model
2024-05-03 11:00:18,912:INFO:Dummy Classifier Imported successfully
2024-05-03 11:00:18,913:INFO:Starting cross validation
2024-05-03 11:00:18,913:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:00:18,942:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,944:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,944:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,945:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,946:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,948:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,949:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,955:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,967:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,968:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:00:18,974:INFO:Calculating mean and std
2024-05-03 11:00:18,974:INFO:Creating metrics dataframe
2024-05-03 11:00:18,975:INFO:Uploading results into container
2024-05-03 11:00:18,975:INFO:Uploading model into container now
2024-05-03 11:00:18,976:INFO:_master_model_container: 14
2024-05-03 11:00:18,976:INFO:_display_container: 2
2024-05-03 11:00:18,976:INFO:DummyClassifier(constant=None, random_state=5450, strategy='prior')
2024-05-03 11:00:18,976:INFO:create_model() successfully completed......................................
2024-05-03 11:00:19,078:INFO:SubProcess create_model() end ==================================
2024-05-03 11:00:19,078:INFO:Creating metrics dataframe
2024-05-03 11:00:19,081:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-05-03 11:00:19,082:INFO:Initializing create_model()
2024-05-03 11:00:19,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD434590>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:00:19,082:INFO:Checking exceptions
2024-05-03 11:00:19,083:INFO:Importing libraries
2024-05-03 11:00:19,083:INFO:Copying training dataset
2024-05-03 11:00:19,085:INFO:Defining folds
2024-05-03 11:00:19,086:INFO:Declaring metric variables
2024-05-03 11:00:19,086:INFO:Importing untrained model
2024-05-03 11:00:19,086:INFO:Declaring custom model
2024-05-03 11:00:19,086:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:00:19,086:INFO:Cross validation set to False
2024-05-03 11:00:19,086:INFO:Fitting Model
2024-05-03 11:00:19,092:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:00:19,092:INFO:create_model() successfully completed......................................
2024-05-03 11:00:19,202:INFO:_master_model_container: 14
2024-05-03 11:00:19,202:INFO:_display_container: 2
2024-05-03 11:00:19,202:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:00:19,202:INFO:compare_models() successfully completed......................................
2024-05-03 11:01:16,206:INFO:PyCaret ClassificationExperiment
2024-05-03 11:01:16,206:INFO:Logging name: clf-default-name
2024-05-03 11:01:16,206:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-03 11:01:16,206:INFO:version 3.3.2
2024-05-03 11:01:16,206:INFO:Initializing setup()
2024-05-03 11:01:16,206:INFO:self.USI: c280
2024-05-03 11:01:16,206:INFO:self._variable_keys: {'n_jobs_param', '_available_plots', 'exp_name_log', 'X_train', 'pipeline', 'USI', '_ml_usecase', 'html_param', 'exp_id', 'y_test', 'y_train', 'target_param', 'fold_groups_param', 'X', 'X_test', 'gpu_n_jobs_param', 'idx', 'logging_param', 'fold_shuffle_param', 'fix_imbalance', 'seed', 'data', 'gpu_param', 'memory', 'log_plots_param', 'y', 'is_multiclass', 'fold_generator'}
2024-05-03 11:01:16,206:INFO:Checking environment
2024-05-03 11:01:16,206:INFO:python_version: 3.11.7
2024-05-03 11:01:16,206:INFO:python_build: ('tags/v3.11.7:fa7a6f2', 'Dec  4 2023 19:24:49')
2024-05-03 11:01:16,206:INFO:machine: AMD64
2024-05-03 11:01:16,207:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-03 11:01:16,210:INFO:Memory: svmem(total=16939401216, available=1721651200, percent=89.8, used=15217750016, free=1721651200)
2024-05-03 11:01:16,210:INFO:Physical Core: 4
2024-05-03 11:01:16,210:INFO:Logical Core: 8
2024-05-03 11:01:16,210:INFO:Checking libraries
2024-05-03 11:01:16,210:INFO:System:
2024-05-03 11:01:16,210:INFO:    python: 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]
2024-05-03 11:01:16,210:INFO:executable: Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Scripts\python.exe
2024-05-03 11:01:16,210:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-03 11:01:16,210:INFO:PyCaret required dependencies:
2024-05-03 11:01:16,211:INFO:                 pip: 23.2.1
2024-05-03 11:01:16,211:INFO:          setuptools: 68.2.0
2024-05-03 11:01:16,211:INFO:             pycaret: 3.3.2
2024-05-03 11:01:16,211:INFO:             IPython: 8.24.0
2024-05-03 11:01:16,211:INFO:          ipywidgets: 8.1.2
2024-05-03 11:01:16,211:INFO:                tqdm: 4.66.4
2024-05-03 11:01:16,211:INFO:               numpy: 1.26.4
2024-05-03 11:01:16,211:INFO:              pandas: 2.1.4
2024-05-03 11:01:16,211:INFO:              jinja2: 3.1.3
2024-05-03 11:01:16,211:INFO:               scipy: 1.11.4
2024-05-03 11:01:16,211:INFO:              joblib: 1.3.2
2024-05-03 11:01:16,211:INFO:             sklearn: 1.4.2
2024-05-03 11:01:16,211:INFO:                pyod: 1.1.3
2024-05-03 11:01:16,211:INFO:            imblearn: 0.12.2
2024-05-03 11:01:16,211:INFO:   category_encoders: 2.6.3
2024-05-03 11:01:16,211:INFO:            lightgbm: 4.3.0
2024-05-03 11:01:16,211:INFO:               numba: 0.58.1
2024-05-03 11:01:16,211:INFO:            requests: 2.31.0
2024-05-03 11:01:16,211:INFO:          matplotlib: 3.7.5
2024-05-03 11:01:16,211:INFO:          scikitplot: 0.3.7
2024-05-03 11:01:16,211:INFO:         yellowbrick: 1.5
2024-05-03 11:01:16,211:INFO:              plotly: 5.22.0
2024-05-03 11:01:16,211:INFO:    plotly-resampler: Not installed
2024-05-03 11:01:16,211:INFO:             kaleido: 0.2.1
2024-05-03 11:01:16,211:INFO:           schemdraw: 0.15
2024-05-03 11:01:16,211:INFO:         statsmodels: 0.14.2
2024-05-03 11:01:16,211:INFO:              sktime: 0.26.0
2024-05-03 11:01:16,211:INFO:               tbats: 1.1.3
2024-05-03 11:01:16,211:INFO:            pmdarima: 2.0.4
2024-05-03 11:01:16,211:INFO:              psutil: 5.9.8
2024-05-03 11:01:16,212:INFO:          markupsafe: 2.1.5
2024-05-03 11:01:16,212:INFO:             pickle5: Not installed
2024-05-03 11:01:16,212:INFO:         cloudpickle: 3.0.0
2024-05-03 11:01:16,212:INFO:         deprecation: 2.1.0
2024-05-03 11:01:16,212:INFO:              xxhash: 3.4.1
2024-05-03 11:01:16,212:INFO:           wurlitzer: Not installed
2024-05-03 11:01:16,212:INFO:PyCaret optional dependencies:
2024-05-03 11:01:16,212:INFO:                shap: Not installed
2024-05-03 11:01:16,212:INFO:           interpret: Not installed
2024-05-03 11:01:16,212:INFO:                umap: Not installed
2024-05-03 11:01:16,212:INFO:     ydata_profiling: 4.7.0
2024-05-03 11:01:16,212:INFO:  explainerdashboard: Not installed
2024-05-03 11:01:16,212:INFO:             autoviz: Not installed
2024-05-03 11:01:16,212:INFO:           fairlearn: Not installed
2024-05-03 11:01:16,212:INFO:          deepchecks: Not installed
2024-05-03 11:01:16,212:INFO:             xgboost: Not installed
2024-05-03 11:01:16,212:INFO:            catboost: Not installed
2024-05-03 11:01:16,212:INFO:              kmodes: Not installed
2024-05-03 11:01:16,212:INFO:             mlxtend: Not installed
2024-05-03 11:01:16,212:INFO:       statsforecast: Not installed
2024-05-03 11:01:16,212:INFO:        tune_sklearn: Not installed
2024-05-03 11:01:16,212:INFO:                 ray: Not installed
2024-05-03 11:01:16,212:INFO:            hyperopt: Not installed
2024-05-03 11:01:16,212:INFO:              optuna: Not installed
2024-05-03 11:01:16,212:INFO:               skopt: Not installed
2024-05-03 11:01:16,212:INFO:              mlflow: Not installed
2024-05-03 11:01:16,212:INFO:              gradio: Not installed
2024-05-03 11:01:16,212:INFO:             fastapi: Not installed
2024-05-03 11:01:16,212:INFO:             uvicorn: Not installed
2024-05-03 11:01:16,212:INFO:              m2cgen: Not installed
2024-05-03 11:01:16,213:INFO:           evidently: Not installed
2024-05-03 11:01:16,213:INFO:               fugue: Not installed
2024-05-03 11:01:16,213:INFO:           streamlit: 1.34.0
2024-05-03 11:01:16,213:INFO:             prophet: Not installed
2024-05-03 11:01:16,213:INFO:None
2024-05-03 11:01:16,213:INFO:Set up data.
2024-05-03 11:01:16,217:INFO:Set up folding strategy.
2024-05-03 11:01:16,217:INFO:Set up train/test split.
2024-05-03 11:01:16,221:INFO:Set up index.
2024-05-03 11:01:16,221:INFO:Assigning column types.
2024-05-03 11:01:16,227:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-05-03 11:01:16,276:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:01:16,277:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:01:16,302:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,303:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,348:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:01:16,350:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:01:16,378:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,379:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,379:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-05-03 11:01:16,425:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:01:16,452:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,452:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,494:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:01:16,518:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,518:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,518:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-05-03 11:01:16,595:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,595:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,659:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,660:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,661:INFO:Preparing preprocessing pipeline...
2024-05-03 11:01:16,662:INFO:Set up simple imputation.
2024-05-03 11:01:16,673:INFO:Finished creating preprocessing pipeline.
2024-05-03 11:01:16,674:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-05-03 11:01:16,674:INFO:Creating final display dataframe.
2024-05-03 11:01:16,725:INFO:Setup _display_container:                     Description             Value
0                    Session id              1522
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (210, 8)
4        Transformed data shape          (210, 8)
5   Transformed train set shape          (147, 8)
6    Transformed test set shape           (63, 8)
7              Numeric features                 7
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              c280
2024-05-03 11:01:16,791:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,792:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,855:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,856:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:01:16,856:INFO:setup() successfully completed in 0.65s...............
2024-05-03 11:01:16,859:INFO:Initializing compare_models()
2024-05-03 11:01:16,859:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-05-03 11:01:16,859:INFO:Checking exceptions
2024-05-03 11:01:16,862:INFO:Preparing display monitor
2024-05-03 11:01:16,863:INFO:Initializing Logistic Regression
2024-05-03 11:01:16,863:INFO:Total runtime is 0.0 minutes
2024-05-03 11:01:16,863:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:16,864:INFO:Initializing create_model()
2024-05-03 11:01:16,864:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:16,864:INFO:Checking exceptions
2024-05-03 11:01:16,864:INFO:Importing libraries
2024-05-03 11:01:16,864:INFO:Copying training dataset
2024-05-03 11:01:16,867:INFO:Defining folds
2024-05-03 11:01:16,867:INFO:Declaring metric variables
2024-05-03 11:01:16,867:INFO:Importing untrained model
2024-05-03 11:01:16,868:INFO:Logistic Regression Imported successfully
2024-05-03 11:01:16,868:INFO:Starting cross validation
2024-05-03 11:01:16,869:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:16,975:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:16,977:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:16,980:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:16,993:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:16,998:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,019:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,030:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,033:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,056:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,062:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,080:INFO:Calculating mean and std
2024-05-03 11:01:17,080:INFO:Creating metrics dataframe
2024-05-03 11:01:17,081:INFO:Uploading results into container
2024-05-03 11:01:17,083:INFO:Uploading model into container now
2024-05-03 11:01:17,083:INFO:_master_model_container: 1
2024-05-03 11:01:17,083:INFO:_display_container: 2
2024-05-03 11:01:17,083:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1522, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-05-03 11:01:17,083:INFO:create_model() successfully completed......................................
2024-05-03 11:01:17,192:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:17,192:INFO:Creating metrics dataframe
2024-05-03 11:01:17,194:INFO:Initializing K Neighbors Classifier
2024-05-03 11:01:17,194:INFO:Total runtime is 0.005511927604675293 minutes
2024-05-03 11:01:17,194:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:17,194:INFO:Initializing create_model()
2024-05-03 11:01:17,194:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:17,195:INFO:Checking exceptions
2024-05-03 11:01:17,195:INFO:Importing libraries
2024-05-03 11:01:17,195:INFO:Copying training dataset
2024-05-03 11:01:17,197:INFO:Defining folds
2024-05-03 11:01:17,197:INFO:Declaring metric variables
2024-05-03 11:01:17,197:INFO:Importing untrained model
2024-05-03 11:01:17,197:INFO:K Neighbors Classifier Imported successfully
2024-05-03 11:01:17,197:INFO:Starting cross validation
2024-05-03 11:01:17,198:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:17,335:INFO:Calculating mean and std
2024-05-03 11:01:17,335:INFO:Creating metrics dataframe
2024-05-03 11:01:17,337:INFO:Uploading results into container
2024-05-03 11:01:17,337:INFO:Uploading model into container now
2024-05-03 11:01:17,337:INFO:_master_model_container: 2
2024-05-03 11:01:17,337:INFO:_display_container: 2
2024-05-03 11:01:17,338:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-05-03 11:01:17,338:INFO:create_model() successfully completed......................................
2024-05-03 11:01:17,438:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:17,439:INFO:Creating metrics dataframe
2024-05-03 11:01:17,441:INFO:Initializing Naive Bayes
2024-05-03 11:01:17,441:INFO:Total runtime is 0.009630191326141357 minutes
2024-05-03 11:01:17,441:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:17,441:INFO:Initializing create_model()
2024-05-03 11:01:17,441:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:17,441:INFO:Checking exceptions
2024-05-03 11:01:17,441:INFO:Importing libraries
2024-05-03 11:01:17,441:INFO:Copying training dataset
2024-05-03 11:01:17,445:INFO:Defining folds
2024-05-03 11:01:17,445:INFO:Declaring metric variables
2024-05-03 11:01:17,445:INFO:Importing untrained model
2024-05-03 11:01:17,445:INFO:Naive Bayes Imported successfully
2024-05-03 11:01:17,445:INFO:Starting cross validation
2024-05-03 11:01:17,445:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:17,517:INFO:Calculating mean and std
2024-05-03 11:01:17,517:INFO:Creating metrics dataframe
2024-05-03 11:01:17,519:INFO:Uploading results into container
2024-05-03 11:01:17,519:INFO:Uploading model into container now
2024-05-03 11:01:17,519:INFO:_master_model_container: 3
2024-05-03 11:01:17,519:INFO:_display_container: 2
2024-05-03 11:01:17,519:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-05-03 11:01:17,519:INFO:create_model() successfully completed......................................
2024-05-03 11:01:17,623:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:17,623:INFO:Creating metrics dataframe
2024-05-03 11:01:17,624:INFO:Initializing Decision Tree Classifier
2024-05-03 11:01:17,624:INFO:Total runtime is 0.012692848841349283 minutes
2024-05-03 11:01:17,625:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:17,625:INFO:Initializing create_model()
2024-05-03 11:01:17,625:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:17,625:INFO:Checking exceptions
2024-05-03 11:01:17,625:INFO:Importing libraries
2024-05-03 11:01:17,625:INFO:Copying training dataset
2024-05-03 11:01:17,628:INFO:Defining folds
2024-05-03 11:01:17,628:INFO:Declaring metric variables
2024-05-03 11:01:17,628:INFO:Importing untrained model
2024-05-03 11:01:17,628:INFO:Decision Tree Classifier Imported successfully
2024-05-03 11:01:17,628:INFO:Starting cross validation
2024-05-03 11:01:17,629:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:17,702:INFO:Calculating mean and std
2024-05-03 11:01:17,703:INFO:Creating metrics dataframe
2024-05-03 11:01:17,703:INFO:Uploading results into container
2024-05-03 11:01:17,704:INFO:Uploading model into container now
2024-05-03 11:01:17,704:INFO:_master_model_container: 4
2024-05-03 11:01:17,704:INFO:_display_container: 2
2024-05-03 11:01:17,704:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=1522, splitter='best')
2024-05-03 11:01:17,704:INFO:create_model() successfully completed......................................
2024-05-03 11:01:17,803:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:17,804:INFO:Creating metrics dataframe
2024-05-03 11:01:17,805:INFO:Initializing SVM - Linear Kernel
2024-05-03 11:01:17,806:INFO:Total runtime is 0.01572490135828654 minutes
2024-05-03 11:01:17,806:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:17,806:INFO:Initializing create_model()
2024-05-03 11:01:17,806:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:17,806:INFO:Checking exceptions
2024-05-03 11:01:17,806:INFO:Importing libraries
2024-05-03 11:01:17,806:INFO:Copying training dataset
2024-05-03 11:01:17,809:INFO:Defining folds
2024-05-03 11:01:17,809:INFO:Declaring metric variables
2024-05-03 11:01:17,809:INFO:Importing untrained model
2024-05-03 11:01:17,809:INFO:SVM - Linear Kernel Imported successfully
2024-05-03 11:01:17,809:INFO:Starting cross validation
2024-05-03 11:01:17,810:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:17,854:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,855:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,858:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,858:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,858:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,859:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,867:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,895:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,898:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:17,910:INFO:Calculating mean and std
2024-05-03 11:01:17,910:INFO:Creating metrics dataframe
2024-05-03 11:01:17,911:INFO:Uploading results into container
2024-05-03 11:01:17,912:INFO:Uploading model into container now
2024-05-03 11:01:17,912:INFO:_master_model_container: 5
2024-05-03 11:01:17,912:INFO:_display_container: 2
2024-05-03 11:01:17,912:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=1522, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-05-03 11:01:17,912:INFO:create_model() successfully completed......................................
2024-05-03 11:01:18,012:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:18,012:INFO:Creating metrics dataframe
2024-05-03 11:01:18,015:INFO:Initializing Ridge Classifier
2024-05-03 11:01:18,015:INFO:Total runtime is 0.019202224413553872 minutes
2024-05-03 11:01:18,015:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:18,015:INFO:Initializing create_model()
2024-05-03 11:01:18,015:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:18,015:INFO:Checking exceptions
2024-05-03 11:01:18,015:INFO:Importing libraries
2024-05-03 11:01:18,015:INFO:Copying training dataset
2024-05-03 11:01:18,018:INFO:Defining folds
2024-05-03 11:01:18,018:INFO:Declaring metric variables
2024-05-03 11:01:18,018:INFO:Importing untrained model
2024-05-03 11:01:18,018:INFO:Ridge Classifier Imported successfully
2024-05-03 11:01:18,018:INFO:Starting cross validation
2024-05-03 11:01:18,019:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:18,045:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,046:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,047:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,049:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,051:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,052:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,053:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,064:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,072:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,072:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,083:INFO:Calculating mean and std
2024-05-03 11:01:18,084:INFO:Creating metrics dataframe
2024-05-03 11:01:18,085:INFO:Uploading results into container
2024-05-03 11:01:18,085:INFO:Uploading model into container now
2024-05-03 11:01:18,086:INFO:_master_model_container: 6
2024-05-03 11:01:18,086:INFO:_display_container: 2
2024-05-03 11:01:18,086:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=1522, solver='auto',
                tol=0.0001)
2024-05-03 11:01:18,086:INFO:create_model() successfully completed......................................
2024-05-03 11:01:18,186:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:18,186:INFO:Creating metrics dataframe
2024-05-03 11:01:18,188:INFO:Initializing Random Forest Classifier
2024-05-03 11:01:18,188:INFO:Total runtime is 0.022093538443247476 minutes
2024-05-03 11:01:18,188:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:18,188:INFO:Initializing create_model()
2024-05-03 11:01:18,188:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:18,188:INFO:Checking exceptions
2024-05-03 11:01:18,188:INFO:Importing libraries
2024-05-03 11:01:18,188:INFO:Copying training dataset
2024-05-03 11:01:18,191:INFO:Defining folds
2024-05-03 11:01:18,191:INFO:Declaring metric variables
2024-05-03 11:01:18,191:INFO:Importing untrained model
2024-05-03 11:01:18,191:INFO:Random Forest Classifier Imported successfully
2024-05-03 11:01:18,192:INFO:Starting cross validation
2024-05-03 11:01:18,192:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:18,675:INFO:Calculating mean and std
2024-05-03 11:01:18,675:INFO:Creating metrics dataframe
2024-05-03 11:01:18,677:INFO:Uploading results into container
2024-05-03 11:01:18,677:INFO:Uploading model into container now
2024-05-03 11:01:18,677:INFO:_master_model_container: 7
2024-05-03 11:01:18,677:INFO:_display_container: 2
2024-05-03 11:01:18,677:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=1522, verbose=0,
                       warm_start=False)
2024-05-03 11:01:18,678:INFO:create_model() successfully completed......................................
2024-05-03 11:01:18,788:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:18,788:INFO:Creating metrics dataframe
2024-05-03 11:01:18,790:INFO:Initializing Quadratic Discriminant Analysis
2024-05-03 11:01:18,790:INFO:Total runtime is 0.03211178382237752 minutes
2024-05-03 11:01:18,790:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:18,790:INFO:Initializing create_model()
2024-05-03 11:01:18,790:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:18,791:INFO:Checking exceptions
2024-05-03 11:01:18,791:INFO:Importing libraries
2024-05-03 11:01:18,791:INFO:Copying training dataset
2024-05-03 11:01:18,793:INFO:Defining folds
2024-05-03 11:01:18,793:INFO:Declaring metric variables
2024-05-03 11:01:18,793:INFO:Importing untrained model
2024-05-03 11:01:18,793:INFO:Quadratic Discriminant Analysis Imported successfully
2024-05-03 11:01:18,793:INFO:Starting cross validation
2024-05-03 11:01:18,793:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:18,817:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,819:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,820:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,821:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,823:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,825:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,827:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,834:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,843:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,845:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:18,855:INFO:Calculating mean and std
2024-05-03 11:01:18,857:INFO:Creating metrics dataframe
2024-05-03 11:01:18,858:INFO:Uploading results into container
2024-05-03 11:01:18,859:INFO:Uploading model into container now
2024-05-03 11:01:18,859:INFO:_master_model_container: 8
2024-05-03 11:01:18,859:INFO:_display_container: 2
2024-05-03 11:01:18,859:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-05-03 11:01:18,859:INFO:create_model() successfully completed......................................
2024-05-03 11:01:18,959:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:18,959:INFO:Creating metrics dataframe
2024-05-03 11:01:18,961:INFO:Initializing Ada Boost Classifier
2024-05-03 11:01:18,961:INFO:Total runtime is 0.034970672925313306 minutes
2024-05-03 11:01:18,962:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:18,962:INFO:Initializing create_model()
2024-05-03 11:01:18,962:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:18,962:INFO:Checking exceptions
2024-05-03 11:01:18,962:INFO:Importing libraries
2024-05-03 11:01:18,962:INFO:Copying training dataset
2024-05-03 11:01:18,965:INFO:Defining folds
2024-05-03 11:01:18,965:INFO:Declaring metric variables
2024-05-03 11:01:18,966:INFO:Importing untrained model
2024-05-03 11:01:18,966:INFO:Ada Boost Classifier Imported successfully
2024-05-03 11:01:18,966:INFO:Starting cross validation
2024-05-03 11:01:18,966:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:18,983:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:18,985:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:18,986:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:18,989:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:18,989:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:18,991:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:18,997:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:19,001:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:19,108:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,113:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:19,115:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,116:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,117:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,120:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:19,122:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:19,123:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:19,124:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,128:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,130:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:19,130:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:19,133:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:19,136:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,137:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:01:19,139:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,220:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,223:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:19,223:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,233:INFO:Calculating mean and std
2024-05-03 11:01:19,234:INFO:Creating metrics dataframe
2024-05-03 11:01:19,235:INFO:Uploading results into container
2024-05-03 11:01:19,235:INFO:Uploading model into container now
2024-05-03 11:01:19,235:INFO:_master_model_container: 9
2024-05-03 11:01:19,235:INFO:_display_container: 2
2024-05-03 11:01:19,236:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=1522)
2024-05-03 11:01:19,236:INFO:create_model() successfully completed......................................
2024-05-03 11:01:19,339:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:19,340:INFO:Creating metrics dataframe
2024-05-03 11:01:19,342:INFO:Initializing Gradient Boosting Classifier
2024-05-03 11:01:19,342:INFO:Total runtime is 0.041319262981414785 minutes
2024-05-03 11:01:19,342:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:19,343:INFO:Initializing create_model()
2024-05-03 11:01:19,343:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:19,343:INFO:Checking exceptions
2024-05-03 11:01:19,343:INFO:Importing libraries
2024-05-03 11:01:19,343:INFO:Copying training dataset
2024-05-03 11:01:19,345:INFO:Defining folds
2024-05-03 11:01:19,345:INFO:Declaring metric variables
2024-05-03 11:01:19,345:INFO:Importing untrained model
2024-05-03 11:01:19,347:INFO:Gradient Boosting Classifier Imported successfully
2024-05-03 11:01:19,347:INFO:Starting cross validation
2024-05-03 11:01:19,347:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:19,819:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,819:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,823:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,835:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,840:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,846:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,862:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:19,866:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,152:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,156:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,172:INFO:Calculating mean and std
2024-05-03 11:01:20,173:INFO:Creating metrics dataframe
2024-05-03 11:01:20,175:INFO:Uploading results into container
2024-05-03 11:01:20,175:INFO:Uploading model into container now
2024-05-03 11:01:20,175:INFO:_master_model_container: 10
2024-05-03 11:01:20,175:INFO:_display_container: 2
2024-05-03 11:01:20,176:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1522, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-05-03 11:01:20,176:INFO:create_model() successfully completed......................................
2024-05-03 11:01:20,277:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:20,277:INFO:Creating metrics dataframe
2024-05-03 11:01:20,279:INFO:Initializing Linear Discriminant Analysis
2024-05-03 11:01:20,279:INFO:Total runtime is 0.05693958997726439 minutes
2024-05-03 11:01:20,279:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:20,280:INFO:Initializing create_model()
2024-05-03 11:01:20,280:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:20,280:INFO:Checking exceptions
2024-05-03 11:01:20,280:INFO:Importing libraries
2024-05-03 11:01:20,280:INFO:Copying training dataset
2024-05-03 11:01:20,283:INFO:Defining folds
2024-05-03 11:01:20,283:INFO:Declaring metric variables
2024-05-03 11:01:20,283:INFO:Importing untrained model
2024-05-03 11:01:20,283:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:01:20,283:INFO:Starting cross validation
2024-05-03 11:01:20,284:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:20,309:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,309:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,312:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,312:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,315:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,315:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,316:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,325:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,334:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,335:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:01:20,346:INFO:Calculating mean and std
2024-05-03 11:01:20,346:INFO:Creating metrics dataframe
2024-05-03 11:01:20,348:INFO:Uploading results into container
2024-05-03 11:01:20,348:INFO:Uploading model into container now
2024-05-03 11:01:20,348:INFO:_master_model_container: 11
2024-05-03 11:01:20,348:INFO:_display_container: 2
2024-05-03 11:01:20,348:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:01:20,348:INFO:create_model() successfully completed......................................
2024-05-03 11:01:20,453:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:20,453:INFO:Creating metrics dataframe
2024-05-03 11:01:20,455:INFO:Initializing Extra Trees Classifier
2024-05-03 11:01:20,455:INFO:Total runtime is 0.059875404834747306 minutes
2024-05-03 11:01:20,456:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:20,456:INFO:Initializing create_model()
2024-05-03 11:01:20,456:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:20,456:INFO:Checking exceptions
2024-05-03 11:01:20,456:INFO:Importing libraries
2024-05-03 11:01:20,456:INFO:Copying training dataset
2024-05-03 11:01:20,459:INFO:Defining folds
2024-05-03 11:01:20,459:INFO:Declaring metric variables
2024-05-03 11:01:20,459:INFO:Importing untrained model
2024-05-03 11:01:20,459:INFO:Extra Trees Classifier Imported successfully
2024-05-03 11:01:20,459:INFO:Starting cross validation
2024-05-03 11:01:20,460:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:20,850:INFO:Calculating mean and std
2024-05-03 11:01:20,850:INFO:Creating metrics dataframe
2024-05-03 11:01:20,851:INFO:Uploading results into container
2024-05-03 11:01:20,852:INFO:Uploading model into container now
2024-05-03 11:01:20,852:INFO:_master_model_container: 12
2024-05-03 11:01:20,852:INFO:_display_container: 2
2024-05-03 11:01:20,852:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=1522, verbose=0,
                     warm_start=False)
2024-05-03 11:01:20,852:INFO:create_model() successfully completed......................................
2024-05-03 11:01:20,959:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:20,959:INFO:Creating metrics dataframe
2024-05-03 11:01:20,961:INFO:Initializing Light Gradient Boosting Machine
2024-05-03 11:01:20,961:INFO:Total runtime is 0.06830874284108479 minutes
2024-05-03 11:01:20,961:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:20,962:INFO:Initializing create_model()
2024-05-03 11:01:20,962:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:20,962:INFO:Checking exceptions
2024-05-03 11:01:20,962:INFO:Importing libraries
2024-05-03 11:01:20,962:INFO:Copying training dataset
2024-05-03 11:01:20,964:INFO:Defining folds
2024-05-03 11:01:20,964:INFO:Declaring metric variables
2024-05-03 11:01:20,964:INFO:Importing untrained model
2024-05-03 11:01:20,966:INFO:Light Gradient Boosting Machine Imported successfully
2024-05-03 11:01:20,966:INFO:Starting cross validation
2024-05-03 11:01:20,966:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:21,709:INFO:Calculating mean and std
2024-05-03 11:01:21,710:INFO:Creating metrics dataframe
2024-05-03 11:01:21,712:INFO:Uploading results into container
2024-05-03 11:01:21,712:INFO:Uploading model into container now
2024-05-03 11:01:21,712:INFO:_master_model_container: 13
2024-05-03 11:01:21,712:INFO:_display_container: 2
2024-05-03 11:01:21,713:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1522, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-05-03 11:01:21,713:INFO:create_model() successfully completed......................................
2024-05-03 11:01:21,846:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:21,846:INFO:Creating metrics dataframe
2024-05-03 11:01:21,848:INFO:Initializing Dummy Classifier
2024-05-03 11:01:21,848:INFO:Total runtime is 0.08308285872141519 minutes
2024-05-03 11:01:21,848:INFO:SubProcess create_model() called ==================================
2024-05-03 11:01:21,849:INFO:Initializing create_model()
2024-05-03 11:01:21,849:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD409B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:21,849:INFO:Checking exceptions
2024-05-03 11:01:21,849:INFO:Importing libraries
2024-05-03 11:01:21,849:INFO:Copying training dataset
2024-05-03 11:01:21,852:INFO:Defining folds
2024-05-03 11:01:21,852:INFO:Declaring metric variables
2024-05-03 11:01:21,852:INFO:Importing untrained model
2024-05-03 11:01:21,852:INFO:Dummy Classifier Imported successfully
2024-05-03 11:01:21,852:INFO:Starting cross validation
2024-05-03 11:01:21,853:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:01:21,882:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,883:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,885:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,886:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,888:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,889:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,892:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,898:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,904:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,908:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:01:21,913:INFO:Calculating mean and std
2024-05-03 11:01:21,913:INFO:Creating metrics dataframe
2024-05-03 11:01:21,915:INFO:Uploading results into container
2024-05-03 11:01:21,915:INFO:Uploading model into container now
2024-05-03 11:01:21,915:INFO:_master_model_container: 14
2024-05-03 11:01:21,915:INFO:_display_container: 2
2024-05-03 11:01:21,916:INFO:DummyClassifier(constant=None, random_state=1522, strategy='prior')
2024-05-03 11:01:21,916:INFO:create_model() successfully completed......................................
2024-05-03 11:01:22,018:INFO:SubProcess create_model() end ==================================
2024-05-03 11:01:22,018:INFO:Creating metrics dataframe
2024-05-03 11:01:22,021:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-05-03 11:01:22,022:INFO:Initializing create_model()
2024-05-03 11:01:22,022:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3065D0>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:01:22,022:INFO:Checking exceptions
2024-05-03 11:01:22,022:INFO:Importing libraries
2024-05-03 11:01:22,022:INFO:Copying training dataset
2024-05-03 11:01:22,025:INFO:Defining folds
2024-05-03 11:01:22,025:INFO:Declaring metric variables
2024-05-03 11:01:22,025:INFO:Importing untrained model
2024-05-03 11:01:22,025:INFO:Declaring custom model
2024-05-03 11:01:22,026:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:01:22,026:INFO:Cross validation set to False
2024-05-03 11:01:22,026:INFO:Fitting Model
2024-05-03 11:01:22,031:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:01:22,031:INFO:create_model() successfully completed......................................
2024-05-03 11:01:22,142:INFO:_master_model_container: 14
2024-05-03 11:01:22,142:INFO:_display_container: 2
2024-05-03 11:01:22,142:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:01:22,142:INFO:compare_models() successfully completed......................................
2024-05-03 11:04:30,817:INFO:PyCaret ClassificationExperiment
2024-05-03 11:04:30,818:INFO:Logging name: clf-default-name
2024-05-03 11:04:30,818:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-03 11:04:30,818:INFO:version 3.3.2
2024-05-03 11:04:30,818:INFO:Initializing setup()
2024-05-03 11:04:30,818:INFO:self.USI: 3632
2024-05-03 11:04:30,818:INFO:self._variable_keys: {'n_jobs_param', '_available_plots', 'exp_name_log', 'X_train', 'pipeline', 'USI', '_ml_usecase', 'html_param', 'exp_id', 'y_test', 'y_train', 'target_param', 'fold_groups_param', 'X', 'X_test', 'gpu_n_jobs_param', 'idx', 'logging_param', 'fold_shuffle_param', 'fix_imbalance', 'seed', 'data', 'gpu_param', 'memory', 'log_plots_param', 'y', 'is_multiclass', 'fold_generator'}
2024-05-03 11:04:30,818:INFO:Checking environment
2024-05-03 11:04:30,818:INFO:python_version: 3.11.7
2024-05-03 11:04:30,818:INFO:python_build: ('tags/v3.11.7:fa7a6f2', 'Dec  4 2023 19:24:49')
2024-05-03 11:04:30,818:INFO:machine: AMD64
2024-05-03 11:04:30,818:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-03 11:04:30,822:INFO:Memory: svmem(total=16939401216, available=2059390976, percent=87.8, used=14880010240, free=2059390976)
2024-05-03 11:04:30,822:INFO:Physical Core: 4
2024-05-03 11:04:30,822:INFO:Logical Core: 8
2024-05-03 11:04:30,822:INFO:Checking libraries
2024-05-03 11:04:30,822:INFO:System:
2024-05-03 11:04:30,822:INFO:    python: 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]
2024-05-03 11:04:30,822:INFO:executable: Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Scripts\python.exe
2024-05-03 11:04:30,823:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-03 11:04:30,823:INFO:PyCaret required dependencies:
2024-05-03 11:04:30,823:INFO:                 pip: 23.2.1
2024-05-03 11:04:30,823:INFO:          setuptools: 68.2.0
2024-05-03 11:04:30,823:INFO:             pycaret: 3.3.2
2024-05-03 11:04:30,823:INFO:             IPython: 8.24.0
2024-05-03 11:04:30,823:INFO:          ipywidgets: 8.1.2
2024-05-03 11:04:30,823:INFO:                tqdm: 4.66.4
2024-05-03 11:04:30,823:INFO:               numpy: 1.26.4
2024-05-03 11:04:30,823:INFO:              pandas: 2.1.4
2024-05-03 11:04:30,823:INFO:              jinja2: 3.1.3
2024-05-03 11:04:30,823:INFO:               scipy: 1.11.4
2024-05-03 11:04:30,823:INFO:              joblib: 1.3.2
2024-05-03 11:04:30,823:INFO:             sklearn: 1.4.2
2024-05-03 11:04:30,823:INFO:                pyod: 1.1.3
2024-05-03 11:04:30,823:INFO:            imblearn: 0.12.2
2024-05-03 11:04:30,823:INFO:   category_encoders: 2.6.3
2024-05-03 11:04:30,823:INFO:            lightgbm: 4.3.0
2024-05-03 11:04:30,823:INFO:               numba: 0.58.1
2024-05-03 11:04:30,823:INFO:            requests: 2.31.0
2024-05-03 11:04:30,823:INFO:          matplotlib: 3.7.5
2024-05-03 11:04:30,823:INFO:          scikitplot: 0.3.7
2024-05-03 11:04:30,823:INFO:         yellowbrick: 1.5
2024-05-03 11:04:30,823:INFO:              plotly: 5.22.0
2024-05-03 11:04:30,823:INFO:    plotly-resampler: Not installed
2024-05-03 11:04:30,823:INFO:             kaleido: 0.2.1
2024-05-03 11:04:30,824:INFO:           schemdraw: 0.15
2024-05-03 11:04:30,824:INFO:         statsmodels: 0.14.2
2024-05-03 11:04:30,824:INFO:              sktime: 0.26.0
2024-05-03 11:04:30,824:INFO:               tbats: 1.1.3
2024-05-03 11:04:30,824:INFO:            pmdarima: 2.0.4
2024-05-03 11:04:30,824:INFO:              psutil: 5.9.8
2024-05-03 11:04:30,824:INFO:          markupsafe: 2.1.5
2024-05-03 11:04:30,824:INFO:             pickle5: Not installed
2024-05-03 11:04:30,824:INFO:         cloudpickle: 3.0.0
2024-05-03 11:04:30,824:INFO:         deprecation: 2.1.0
2024-05-03 11:04:30,824:INFO:              xxhash: 3.4.1
2024-05-03 11:04:30,824:INFO:           wurlitzer: Not installed
2024-05-03 11:04:30,824:INFO:PyCaret optional dependencies:
2024-05-03 11:04:30,824:INFO:                shap: Not installed
2024-05-03 11:04:30,824:INFO:           interpret: Not installed
2024-05-03 11:04:30,824:INFO:                umap: Not installed
2024-05-03 11:04:30,824:INFO:     ydata_profiling: 4.7.0
2024-05-03 11:04:30,824:INFO:  explainerdashboard: Not installed
2024-05-03 11:04:30,824:INFO:             autoviz: Not installed
2024-05-03 11:04:30,824:INFO:           fairlearn: Not installed
2024-05-03 11:04:30,824:INFO:          deepchecks: Not installed
2024-05-03 11:04:30,824:INFO:             xgboost: Not installed
2024-05-03 11:04:30,824:INFO:            catboost: Not installed
2024-05-03 11:04:30,824:INFO:              kmodes: Not installed
2024-05-03 11:04:30,824:INFO:             mlxtend: Not installed
2024-05-03 11:04:30,824:INFO:       statsforecast: Not installed
2024-05-03 11:04:30,824:INFO:        tune_sklearn: Not installed
2024-05-03 11:04:30,824:INFO:                 ray: Not installed
2024-05-03 11:04:30,824:INFO:            hyperopt: Not installed
2024-05-03 11:04:30,824:INFO:              optuna: Not installed
2024-05-03 11:04:30,824:INFO:               skopt: Not installed
2024-05-03 11:04:30,824:INFO:              mlflow: Not installed
2024-05-03 11:04:30,824:INFO:              gradio: Not installed
2024-05-03 11:04:30,824:INFO:             fastapi: Not installed
2024-05-03 11:04:30,824:INFO:             uvicorn: Not installed
2024-05-03 11:04:30,824:INFO:              m2cgen: Not installed
2024-05-03 11:04:30,824:INFO:           evidently: Not installed
2024-05-03 11:04:30,824:INFO:               fugue: Not installed
2024-05-03 11:04:30,824:INFO:           streamlit: 1.34.0
2024-05-03 11:04:30,824:INFO:             prophet: Not installed
2024-05-03 11:04:30,824:INFO:None
2024-05-03 11:04:30,825:INFO:Set up data.
2024-05-03 11:04:30,828:INFO:Set up folding strategy.
2024-05-03 11:04:30,828:INFO:Set up train/test split.
2024-05-03 11:04:33,289:INFO:PyCaret ClassificationExperiment
2024-05-03 11:04:33,289:INFO:Logging name: clf-default-name
2024-05-03 11:04:33,289:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-03 11:04:33,289:INFO:version 3.3.2
2024-05-03 11:04:33,289:INFO:Initializing setup()
2024-05-03 11:04:33,289:INFO:self.USI: c8c1
2024-05-03 11:04:33,289:INFO:self._variable_keys: {'n_jobs_param', '_available_plots', 'exp_name_log', 'X_train', 'pipeline', 'USI', '_ml_usecase', 'html_param', 'exp_id', 'y_test', 'y_train', 'target_param', 'fold_groups_param', 'X', 'X_test', 'gpu_n_jobs_param', 'idx', 'logging_param', 'fold_shuffle_param', 'fix_imbalance', 'seed', 'data', 'gpu_param', 'memory', 'log_plots_param', 'y', 'is_multiclass', 'fold_generator'}
2024-05-03 11:04:33,289:INFO:Checking environment
2024-05-03 11:04:33,289:INFO:python_version: 3.11.7
2024-05-03 11:04:33,289:INFO:python_build: ('tags/v3.11.7:fa7a6f2', 'Dec  4 2023 19:24:49')
2024-05-03 11:04:33,289:INFO:machine: AMD64
2024-05-03 11:04:33,289:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-03 11:04:33,292:INFO:Memory: svmem(total=16939401216, available=2066726912, percent=87.8, used=14872674304, free=2066726912)
2024-05-03 11:04:33,294:INFO:Physical Core: 4
2024-05-03 11:04:33,294:INFO:Logical Core: 8
2024-05-03 11:04:33,294:INFO:Checking libraries
2024-05-03 11:04:33,294:INFO:System:
2024-05-03 11:04:33,294:INFO:    python: 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]
2024-05-03 11:04:33,294:INFO:executable: Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Scripts\python.exe
2024-05-03 11:04:33,294:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-03 11:04:33,294:INFO:PyCaret required dependencies:
2024-05-03 11:04:33,294:INFO:                 pip: 23.2.1
2024-05-03 11:04:33,294:INFO:          setuptools: 68.2.0
2024-05-03 11:04:33,294:INFO:             pycaret: 3.3.2
2024-05-03 11:04:33,294:INFO:             IPython: 8.24.0
2024-05-03 11:04:33,294:INFO:          ipywidgets: 8.1.2
2024-05-03 11:04:33,294:INFO:                tqdm: 4.66.4
2024-05-03 11:04:33,294:INFO:               numpy: 1.26.4
2024-05-03 11:04:33,294:INFO:              pandas: 2.1.4
2024-05-03 11:04:33,294:INFO:              jinja2: 3.1.3
2024-05-03 11:04:33,294:INFO:               scipy: 1.11.4
2024-05-03 11:04:33,294:INFO:              joblib: 1.3.2
2024-05-03 11:04:33,294:INFO:             sklearn: 1.4.2
2024-05-03 11:04:33,294:INFO:                pyod: 1.1.3
2024-05-03 11:04:33,294:INFO:            imblearn: 0.12.2
2024-05-03 11:04:33,294:INFO:   category_encoders: 2.6.3
2024-05-03 11:04:33,294:INFO:            lightgbm: 4.3.0
2024-05-03 11:04:33,294:INFO:               numba: 0.58.1
2024-05-03 11:04:33,294:INFO:            requests: 2.31.0
2024-05-03 11:04:33,294:INFO:          matplotlib: 3.7.5
2024-05-03 11:04:33,294:INFO:          scikitplot: 0.3.7
2024-05-03 11:04:33,294:INFO:         yellowbrick: 1.5
2024-05-03 11:04:33,294:INFO:              plotly: 5.22.0
2024-05-03 11:04:33,294:INFO:    plotly-resampler: Not installed
2024-05-03 11:04:33,294:INFO:             kaleido: 0.2.1
2024-05-03 11:04:33,294:INFO:           schemdraw: 0.15
2024-05-03 11:04:33,294:INFO:         statsmodels: 0.14.2
2024-05-03 11:04:33,294:INFO:              sktime: 0.26.0
2024-05-03 11:04:33,294:INFO:               tbats: 1.1.3
2024-05-03 11:04:33,294:INFO:            pmdarima: 2.0.4
2024-05-03 11:04:33,294:INFO:              psutil: 5.9.8
2024-05-03 11:04:33,294:INFO:          markupsafe: 2.1.5
2024-05-03 11:04:33,294:INFO:             pickle5: Not installed
2024-05-03 11:04:33,294:INFO:         cloudpickle: 3.0.0
2024-05-03 11:04:33,295:INFO:         deprecation: 2.1.0
2024-05-03 11:04:33,295:INFO:              xxhash: 3.4.1
2024-05-03 11:04:33,295:INFO:           wurlitzer: Not installed
2024-05-03 11:04:33,295:INFO:PyCaret optional dependencies:
2024-05-03 11:04:33,295:INFO:                shap: Not installed
2024-05-03 11:04:33,295:INFO:           interpret: Not installed
2024-05-03 11:04:33,295:INFO:                umap: Not installed
2024-05-03 11:04:33,295:INFO:     ydata_profiling: 4.7.0
2024-05-03 11:04:33,295:INFO:  explainerdashboard: Not installed
2024-05-03 11:04:33,295:INFO:             autoviz: Not installed
2024-05-03 11:04:33,295:INFO:           fairlearn: Not installed
2024-05-03 11:04:33,295:INFO:          deepchecks: Not installed
2024-05-03 11:04:33,295:INFO:             xgboost: Not installed
2024-05-03 11:04:33,295:INFO:            catboost: Not installed
2024-05-03 11:04:33,295:INFO:              kmodes: Not installed
2024-05-03 11:04:33,295:INFO:             mlxtend: Not installed
2024-05-03 11:04:33,295:INFO:       statsforecast: Not installed
2024-05-03 11:04:33,295:INFO:        tune_sklearn: Not installed
2024-05-03 11:04:33,295:INFO:                 ray: Not installed
2024-05-03 11:04:33,295:INFO:            hyperopt: Not installed
2024-05-03 11:04:33,295:INFO:              optuna: Not installed
2024-05-03 11:04:33,295:INFO:               skopt: Not installed
2024-05-03 11:04:33,295:INFO:              mlflow: Not installed
2024-05-03 11:04:33,295:INFO:              gradio: Not installed
2024-05-03 11:04:33,295:INFO:             fastapi: Not installed
2024-05-03 11:04:33,295:INFO:             uvicorn: Not installed
2024-05-03 11:04:33,295:INFO:              m2cgen: Not installed
2024-05-03 11:04:33,295:INFO:           evidently: Not installed
2024-05-03 11:04:33,295:INFO:               fugue: Not installed
2024-05-03 11:04:33,295:INFO:           streamlit: 1.34.0
2024-05-03 11:04:33,296:INFO:             prophet: Not installed
2024-05-03 11:04:33,296:INFO:None
2024-05-03 11:04:33,296:INFO:Set up data.
2024-05-03 11:04:33,299:INFO:Set up folding strategy.
2024-05-03 11:04:33,299:INFO:Set up train/test split.
2024-05-03 11:04:33,302:INFO:Set up index.
2024-05-03 11:04:33,302:INFO:Assigning column types.
2024-05-03 11:04:33,307:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-05-03 11:04:33,353:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:04:33,354:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:04:33,379:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,380:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,424:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:04:33,425:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:04:33,454:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,454:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,454:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-05-03 11:04:33,498:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:04:33,527:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,527:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,569:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:04:33,594:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,594:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,594:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-05-03 11:04:33,659:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,660:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,724:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,725:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,726:INFO:Preparing preprocessing pipeline...
2024-05-03 11:04:33,727:INFO:Set up simple imputation.
2024-05-03 11:04:33,739:INFO:Finished creating preprocessing pipeline.
2024-05-03 11:04:33,741:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-05-03 11:04:33,742:INFO:Creating final display dataframe.
2024-05-03 11:04:33,792:INFO:Setup _display_container:                     Description             Value
0                    Session id              5038
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (210, 8)
4        Transformed data shape          (210, 8)
5   Transformed train set shape          (147, 8)
6    Transformed test set shape           (63, 8)
7              Numeric features                 7
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              c8c1
2024-05-03 11:04:33,868:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,869:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,935:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,935:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:04:33,936:INFO:setup() successfully completed in 0.65s...............
2024-05-03 11:04:33,939:INFO:Initializing compare_models()
2024-05-03 11:04:33,939:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-05-03 11:04:33,939:INFO:Checking exceptions
2024-05-03 11:04:33,941:INFO:Preparing display monitor
2024-05-03 11:04:33,944:INFO:Initializing Logistic Regression
2024-05-03 11:04:33,944:INFO:Total runtime is 1.668532689412435e-05 minutes
2024-05-03 11:04:33,944:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:33,944:INFO:Initializing create_model()
2024-05-03 11:04:33,944:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:33,944:INFO:Checking exceptions
2024-05-03 11:04:33,944:INFO:Importing libraries
2024-05-03 11:04:33,944:INFO:Copying training dataset
2024-05-03 11:04:33,947:INFO:Defining folds
2024-05-03 11:04:33,947:INFO:Declaring metric variables
2024-05-03 11:04:33,947:INFO:Importing untrained model
2024-05-03 11:04:33,948:INFO:Logistic Regression Imported successfully
2024-05-03 11:04:33,948:INFO:Starting cross validation
2024-05-03 11:04:33,948:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:34,061:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,064:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,075:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,089:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,089:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,096:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,104:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,111:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,139:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,151:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,164:INFO:Calculating mean and std
2024-05-03 11:04:34,165:INFO:Creating metrics dataframe
2024-05-03 11:04:34,166:INFO:Uploading results into container
2024-05-03 11:04:34,167:INFO:Uploading model into container now
2024-05-03 11:04:34,167:INFO:_master_model_container: 1
2024-05-03 11:04:34,167:INFO:_display_container: 2
2024-05-03 11:04:34,167:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5038, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-05-03 11:04:34,168:INFO:create_model() successfully completed......................................
2024-05-03 11:04:34,274:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:34,274:INFO:Creating metrics dataframe
2024-05-03 11:04:34,276:INFO:Initializing K Neighbors Classifier
2024-05-03 11:04:34,276:INFO:Total runtime is 0.005552105108896891 minutes
2024-05-03 11:04:34,276:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:34,276:INFO:Initializing create_model()
2024-05-03 11:04:34,276:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:34,276:INFO:Checking exceptions
2024-05-03 11:04:34,276:INFO:Importing libraries
2024-05-03 11:04:34,277:INFO:Copying training dataset
2024-05-03 11:04:34,279:INFO:Defining folds
2024-05-03 11:04:34,279:INFO:Declaring metric variables
2024-05-03 11:04:34,279:INFO:Importing untrained model
2024-05-03 11:04:34,279:INFO:K Neighbors Classifier Imported successfully
2024-05-03 11:04:34,280:INFO:Starting cross validation
2024-05-03 11:04:34,280:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:34,410:INFO:Calculating mean and std
2024-05-03 11:04:34,410:INFO:Creating metrics dataframe
2024-05-03 11:04:34,412:INFO:Uploading results into container
2024-05-03 11:04:34,412:INFO:Uploading model into container now
2024-05-03 11:04:34,412:INFO:_master_model_container: 2
2024-05-03 11:04:34,412:INFO:_display_container: 2
2024-05-03 11:04:34,412:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-05-03 11:04:34,412:INFO:create_model() successfully completed......................................
2024-05-03 11:04:34,512:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:34,512:INFO:Creating metrics dataframe
2024-05-03 11:04:34,516:INFO:Initializing Naive Bayes
2024-05-03 11:04:34,516:INFO:Total runtime is 0.009550575415293375 minutes
2024-05-03 11:04:34,516:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:34,516:INFO:Initializing create_model()
2024-05-03 11:04:34,516:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:34,516:INFO:Checking exceptions
2024-05-03 11:04:34,516:INFO:Importing libraries
2024-05-03 11:04:34,516:INFO:Copying training dataset
2024-05-03 11:04:34,519:INFO:Defining folds
2024-05-03 11:04:34,519:INFO:Declaring metric variables
2024-05-03 11:04:34,519:INFO:Importing untrained model
2024-05-03 11:04:34,520:INFO:Naive Bayes Imported successfully
2024-05-03 11:04:34,520:INFO:Starting cross validation
2024-05-03 11:04:34,520:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:34,582:INFO:Calculating mean and std
2024-05-03 11:04:34,582:INFO:Creating metrics dataframe
2024-05-03 11:04:34,583:INFO:Uploading results into container
2024-05-03 11:04:34,583:INFO:Uploading model into container now
2024-05-03 11:04:34,584:INFO:_master_model_container: 3
2024-05-03 11:04:34,584:INFO:_display_container: 2
2024-05-03 11:04:34,584:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-05-03 11:04:34,584:INFO:create_model() successfully completed......................................
2024-05-03 11:04:34,690:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:34,690:INFO:Creating metrics dataframe
2024-05-03 11:04:34,692:INFO:Initializing Decision Tree Classifier
2024-05-03 11:04:34,692:INFO:Total runtime is 0.012482591470082599 minutes
2024-05-03 11:04:34,692:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:34,692:INFO:Initializing create_model()
2024-05-03 11:04:34,692:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:34,692:INFO:Checking exceptions
2024-05-03 11:04:34,693:INFO:Importing libraries
2024-05-03 11:04:34,693:INFO:Copying training dataset
2024-05-03 11:04:34,695:INFO:Defining folds
2024-05-03 11:04:34,695:INFO:Declaring metric variables
2024-05-03 11:04:34,696:INFO:Importing untrained model
2024-05-03 11:04:34,696:INFO:Decision Tree Classifier Imported successfully
2024-05-03 11:04:34,696:INFO:Starting cross validation
2024-05-03 11:04:34,696:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:34,771:INFO:Calculating mean and std
2024-05-03 11:04:34,771:INFO:Creating metrics dataframe
2024-05-03 11:04:34,773:INFO:Uploading results into container
2024-05-03 11:04:34,773:INFO:Uploading model into container now
2024-05-03 11:04:34,773:INFO:_master_model_container: 4
2024-05-03 11:04:34,773:INFO:_display_container: 2
2024-05-03 11:04:34,774:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=5038, splitter='best')
2024-05-03 11:04:34,774:INFO:create_model() successfully completed......................................
2024-05-03 11:04:34,874:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:34,874:INFO:Creating metrics dataframe
2024-05-03 11:04:34,876:INFO:Initializing SVM - Linear Kernel
2024-05-03 11:04:34,876:INFO:Total runtime is 0.015544080734252928 minutes
2024-05-03 11:04:34,877:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:34,877:INFO:Initializing create_model()
2024-05-03 11:04:34,877:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:34,877:INFO:Checking exceptions
2024-05-03 11:04:34,877:INFO:Importing libraries
2024-05-03 11:04:34,877:INFO:Copying training dataset
2024-05-03 11:04:34,880:INFO:Defining folds
2024-05-03 11:04:34,880:INFO:Declaring metric variables
2024-05-03 11:04:34,880:INFO:Importing untrained model
2024-05-03 11:04:34,880:INFO:SVM - Linear Kernel Imported successfully
2024-05-03 11:04:34,880:INFO:Starting cross validation
2024-05-03 11:04:34,881:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:34,924:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,926:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,926:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,926:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,930:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,931:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,933:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,947:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,966:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,967:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:34,979:INFO:Calculating mean and std
2024-05-03 11:04:34,980:INFO:Creating metrics dataframe
2024-05-03 11:04:34,981:INFO:Uploading results into container
2024-05-03 11:04:34,982:INFO:Uploading model into container now
2024-05-03 11:04:34,982:INFO:_master_model_container: 5
2024-05-03 11:04:34,982:INFO:_display_container: 2
2024-05-03 11:04:34,982:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5038, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-05-03 11:04:34,982:INFO:create_model() successfully completed......................................
2024-05-03 11:04:35,086:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:35,086:INFO:Creating metrics dataframe
2024-05-03 11:04:35,088:INFO:Initializing Ridge Classifier
2024-05-03 11:04:35,088:INFO:Total runtime is 0.0190852681795756 minutes
2024-05-03 11:04:35,088:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:35,088:INFO:Initializing create_model()
2024-05-03 11:04:35,088:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:35,088:INFO:Checking exceptions
2024-05-03 11:04:35,088:INFO:Importing libraries
2024-05-03 11:04:35,088:INFO:Copying training dataset
2024-05-03 11:04:35,091:INFO:Defining folds
2024-05-03 11:04:35,091:INFO:Declaring metric variables
2024-05-03 11:04:35,091:INFO:Importing untrained model
2024-05-03 11:04:35,091:INFO:Ridge Classifier Imported successfully
2024-05-03 11:04:35,091:INFO:Starting cross validation
2024-05-03 11:04:35,092:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:35,118:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,120:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,121:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,123:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,124:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,125:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,130:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,145:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,146:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,155:INFO:Calculating mean and std
2024-05-03 11:04:35,156:INFO:Creating metrics dataframe
2024-05-03 11:04:35,157:INFO:Uploading results into container
2024-05-03 11:04:35,157:INFO:Uploading model into container now
2024-05-03 11:04:35,158:INFO:_master_model_container: 6
2024-05-03 11:04:35,158:INFO:_display_container: 2
2024-05-03 11:04:35,158:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5038, solver='auto',
                tol=0.0001)
2024-05-03 11:04:35,158:INFO:create_model() successfully completed......................................
2024-05-03 11:04:35,267:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:35,267:INFO:Creating metrics dataframe
2024-05-03 11:04:35,269:INFO:Initializing Random Forest Classifier
2024-05-03 11:04:35,269:INFO:Total runtime is 0.022098314762115476 minutes
2024-05-03 11:04:35,269:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:35,269:INFO:Initializing create_model()
2024-05-03 11:04:35,270:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:35,270:INFO:Checking exceptions
2024-05-03 11:04:35,270:INFO:Importing libraries
2024-05-03 11:04:35,270:INFO:Copying training dataset
2024-05-03 11:04:35,272:INFO:Defining folds
2024-05-03 11:04:35,272:INFO:Declaring metric variables
2024-05-03 11:04:35,272:INFO:Importing untrained model
2024-05-03 11:04:35,273:INFO:Random Forest Classifier Imported successfully
2024-05-03 11:04:35,273:INFO:Starting cross validation
2024-05-03 11:04:35,274:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:35,746:INFO:Calculating mean and std
2024-05-03 11:04:35,747:INFO:Creating metrics dataframe
2024-05-03 11:04:35,748:INFO:Uploading results into container
2024-05-03 11:04:35,748:INFO:Uploading model into container now
2024-05-03 11:04:35,749:INFO:_master_model_container: 7
2024-05-03 11:04:35,749:INFO:_display_container: 2
2024-05-03 11:04:35,749:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5038, verbose=0,
                       warm_start=False)
2024-05-03 11:04:35,749:INFO:create_model() successfully completed......................................
2024-05-03 11:04:35,861:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:35,861:INFO:Creating metrics dataframe
2024-05-03 11:04:35,864:INFO:Initializing Quadratic Discriminant Analysis
2024-05-03 11:04:35,864:INFO:Total runtime is 0.03201992114384969 minutes
2024-05-03 11:04:35,864:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:35,865:INFO:Initializing create_model()
2024-05-03 11:04:35,865:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:35,865:INFO:Checking exceptions
2024-05-03 11:04:35,865:INFO:Importing libraries
2024-05-03 11:04:35,865:INFO:Copying training dataset
2024-05-03 11:04:35,868:INFO:Defining folds
2024-05-03 11:04:35,868:INFO:Declaring metric variables
2024-05-03 11:04:35,868:INFO:Importing untrained model
2024-05-03 11:04:35,869:INFO:Quadratic Discriminant Analysis Imported successfully
2024-05-03 11:04:35,869:INFO:Starting cross validation
2024-05-03 11:04:35,869:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:35,894:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,894:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,895:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,897:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,898:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,899:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,906:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,910:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,924:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,925:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:35,941:INFO:Calculating mean and std
2024-05-03 11:04:35,941:INFO:Creating metrics dataframe
2024-05-03 11:04:35,943:INFO:Uploading results into container
2024-05-03 11:04:35,943:INFO:Uploading model into container now
2024-05-03 11:04:35,943:INFO:_master_model_container: 8
2024-05-03 11:04:35,943:INFO:_display_container: 2
2024-05-03 11:04:35,943:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-05-03 11:04:35,943:INFO:create_model() successfully completed......................................
2024-05-03 11:04:36,044:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:36,044:INFO:Creating metrics dataframe
2024-05-03 11:04:36,046:INFO:Initializing Ada Boost Classifier
2024-05-03 11:04:36,046:INFO:Total runtime is 0.03506014744440714 minutes
2024-05-03 11:04:36,046:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:36,047:INFO:Initializing create_model()
2024-05-03 11:04:36,047:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:36,047:INFO:Checking exceptions
2024-05-03 11:04:36,047:INFO:Importing libraries
2024-05-03 11:04:36,047:INFO:Copying training dataset
2024-05-03 11:04:36,050:INFO:Defining folds
2024-05-03 11:04:36,050:INFO:Declaring metric variables
2024-05-03 11:04:36,050:INFO:Importing untrained model
2024-05-03 11:04:36,050:INFO:Ada Boost Classifier Imported successfully
2024-05-03 11:04:36,050:INFO:Starting cross validation
2024-05-03 11:04:36,051:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:36,067:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,068:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,069:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,071:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,072:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,073:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,075:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,078:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,195:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,200:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,204:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,206:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,211:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,212:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,216:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,217:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,222:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:04:36,295:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,300:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,317:INFO:Calculating mean and std
2024-05-03 11:04:36,318:INFO:Creating metrics dataframe
2024-05-03 11:04:36,319:INFO:Uploading results into container
2024-05-03 11:04:36,319:INFO:Uploading model into container now
2024-05-03 11:04:36,320:INFO:_master_model_container: 9
2024-05-03 11:04:36,320:INFO:_display_container: 2
2024-05-03 11:04:36,320:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=5038)
2024-05-03 11:04:36,320:INFO:create_model() successfully completed......................................
2024-05-03 11:04:36,421:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:36,421:INFO:Creating metrics dataframe
2024-05-03 11:04:36,423:INFO:Initializing Gradient Boosting Classifier
2024-05-03 11:04:36,423:INFO:Total runtime is 0.04132807652155558 minutes
2024-05-03 11:04:36,423:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:36,423:INFO:Initializing create_model()
2024-05-03 11:04:36,423:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:36,423:INFO:Checking exceptions
2024-05-03 11:04:36,423:INFO:Importing libraries
2024-05-03 11:04:36,423:INFO:Copying training dataset
2024-05-03 11:04:36,426:INFO:Defining folds
2024-05-03 11:04:36,426:INFO:Declaring metric variables
2024-05-03 11:04:36,426:INFO:Importing untrained model
2024-05-03 11:04:36,427:INFO:Gradient Boosting Classifier Imported successfully
2024-05-03 11:04:36,427:INFO:Starting cross validation
2024-05-03 11:04:36,427:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:36,918:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,921:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,923:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,926:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,928:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,940:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,941:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:36,956:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,237:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,245:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,261:INFO:Calculating mean and std
2024-05-03 11:04:37,262:INFO:Creating metrics dataframe
2024-05-03 11:04:37,262:INFO:Uploading results into container
2024-05-03 11:04:37,264:INFO:Uploading model into container now
2024-05-03 11:04:37,264:INFO:_master_model_container: 10
2024-05-03 11:04:37,264:INFO:_display_container: 2
2024-05-03 11:04:37,264:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5038, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-05-03 11:04:37,264:INFO:create_model() successfully completed......................................
2024-05-03 11:04:37,364:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:37,364:INFO:Creating metrics dataframe
2024-05-03 11:04:37,366:INFO:Initializing Linear Discriminant Analysis
2024-05-03 11:04:37,366:INFO:Total runtime is 0.05705305735270182 minutes
2024-05-03 11:04:37,366:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:37,366:INFO:Initializing create_model()
2024-05-03 11:04:37,366:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:37,366:INFO:Checking exceptions
2024-05-03 11:04:37,367:INFO:Importing libraries
2024-05-03 11:04:37,367:INFO:Copying training dataset
2024-05-03 11:04:37,369:INFO:Defining folds
2024-05-03 11:04:37,369:INFO:Declaring metric variables
2024-05-03 11:04:37,369:INFO:Importing untrained model
2024-05-03 11:04:37,370:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:04:37,370:INFO:Starting cross validation
2024-05-03 11:04:37,370:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:37,395:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,396:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,397:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,398:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,401:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,401:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,403:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,412:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,426:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,427:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:04:37,442:INFO:Calculating mean and std
2024-05-03 11:04:37,442:INFO:Creating metrics dataframe
2024-05-03 11:04:37,444:INFO:Uploading results into container
2024-05-03 11:04:37,444:INFO:Uploading model into container now
2024-05-03 11:04:37,445:INFO:_master_model_container: 11
2024-05-03 11:04:37,445:INFO:_display_container: 2
2024-05-03 11:04:37,445:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:04:37,445:INFO:create_model() successfully completed......................................
2024-05-03 11:04:37,545:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:37,545:INFO:Creating metrics dataframe
2024-05-03 11:04:37,547:INFO:Initializing Extra Trees Classifier
2024-05-03 11:04:37,547:INFO:Total runtime is 0.060067089398701985 minutes
2024-05-03 11:04:37,547:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:37,548:INFO:Initializing create_model()
2024-05-03 11:04:37,548:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:37,548:INFO:Checking exceptions
2024-05-03 11:04:37,548:INFO:Importing libraries
2024-05-03 11:04:37,548:INFO:Copying training dataset
2024-05-03 11:04:37,550:INFO:Defining folds
2024-05-03 11:04:37,551:INFO:Declaring metric variables
2024-05-03 11:04:37,551:INFO:Importing untrained model
2024-05-03 11:04:37,551:INFO:Extra Trees Classifier Imported successfully
2024-05-03 11:04:37,551:INFO:Starting cross validation
2024-05-03 11:04:37,552:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:37,955:INFO:Calculating mean and std
2024-05-03 11:04:37,956:INFO:Creating metrics dataframe
2024-05-03 11:04:37,957:INFO:Uploading results into container
2024-05-03 11:04:37,957:INFO:Uploading model into container now
2024-05-03 11:04:37,957:INFO:_master_model_container: 12
2024-05-03 11:04:37,958:INFO:_display_container: 2
2024-05-03 11:04:37,958:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=5038, verbose=0,
                     warm_start=False)
2024-05-03 11:04:37,958:INFO:create_model() successfully completed......................................
2024-05-03 11:04:38,076:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:38,076:INFO:Creating metrics dataframe
2024-05-03 11:04:38,078:INFO:Initializing Light Gradient Boosting Machine
2024-05-03 11:04:38,078:INFO:Total runtime is 0.06891925732294718 minutes
2024-05-03 11:04:38,078:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:38,078:INFO:Initializing create_model()
2024-05-03 11:04:38,078:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:38,078:INFO:Checking exceptions
2024-05-03 11:04:38,078:INFO:Importing libraries
2024-05-03 11:04:38,078:INFO:Copying training dataset
2024-05-03 11:04:38,081:INFO:Defining folds
2024-05-03 11:04:38,081:INFO:Declaring metric variables
2024-05-03 11:04:38,081:INFO:Importing untrained model
2024-05-03 11:04:38,082:INFO:Light Gradient Boosting Machine Imported successfully
2024-05-03 11:04:38,082:INFO:Starting cross validation
2024-05-03 11:04:38,082:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:41,363:INFO:Calculating mean and std
2024-05-03 11:04:41,364:INFO:Creating metrics dataframe
2024-05-03 11:04:41,366:INFO:Uploading results into container
2024-05-03 11:04:41,367:INFO:Uploading model into container now
2024-05-03 11:04:41,367:INFO:_master_model_container: 13
2024-05-03 11:04:41,367:INFO:_display_container: 2
2024-05-03 11:04:41,368:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5038, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-05-03 11:04:41,368:INFO:create_model() successfully completed......................................
2024-05-03 11:04:41,506:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:41,506:INFO:Creating metrics dataframe
2024-05-03 11:04:41,508:INFO:Initializing Dummy Classifier
2024-05-03 11:04:41,508:INFO:Total runtime is 0.12608733574549358 minutes
2024-05-03 11:04:41,509:INFO:SubProcess create_model() called ==================================
2024-05-03 11:04:41,509:INFO:Initializing create_model()
2024-05-03 11:04:41,509:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD40B790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:41,509:INFO:Checking exceptions
2024-05-03 11:04:41,509:INFO:Importing libraries
2024-05-03 11:04:41,509:INFO:Copying training dataset
2024-05-03 11:04:41,514:INFO:Defining folds
2024-05-03 11:04:41,514:INFO:Declaring metric variables
2024-05-03 11:04:41,514:INFO:Importing untrained model
2024-05-03 11:04:41,515:INFO:Dummy Classifier Imported successfully
2024-05-03 11:04:41,515:INFO:Starting cross validation
2024-05-03 11:04:41,515:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:04:41,544:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,546:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,547:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,548:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,548:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,558:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,571:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,573:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,577:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,577:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:04:41,587:INFO:Calculating mean and std
2024-05-03 11:04:41,587:INFO:Creating metrics dataframe
2024-05-03 11:04:41,589:INFO:Uploading results into container
2024-05-03 11:04:41,589:INFO:Uploading model into container now
2024-05-03 11:04:41,589:INFO:_master_model_container: 14
2024-05-03 11:04:41,589:INFO:_display_container: 2
2024-05-03 11:04:41,589:INFO:DummyClassifier(constant=None, random_state=5038, strategy='prior')
2024-05-03 11:04:41,589:INFO:create_model() successfully completed......................................
2024-05-03 11:04:41,699:INFO:SubProcess create_model() end ==================================
2024-05-03 11:04:41,699:INFO:Creating metrics dataframe
2024-05-03 11:04:41,703:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-05-03 11:04:41,704:INFO:Initializing create_model()
2024-05-03 11:04:41,704:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3920D0>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:04:41,704:INFO:Checking exceptions
2024-05-03 11:04:41,705:INFO:Importing libraries
2024-05-03 11:04:41,705:INFO:Copying training dataset
2024-05-03 11:04:41,708:INFO:Defining folds
2024-05-03 11:04:41,708:INFO:Declaring metric variables
2024-05-03 11:04:41,708:INFO:Importing untrained model
2024-05-03 11:04:41,708:INFO:Declaring custom model
2024-05-03 11:04:41,709:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:04:41,709:INFO:Cross validation set to False
2024-05-03 11:04:41,709:INFO:Fitting Model
2024-05-03 11:04:41,717:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:04:41,718:INFO:create_model() successfully completed......................................
2024-05-03 11:04:41,879:INFO:_master_model_container: 14
2024-05-03 11:04:41,879:INFO:_display_container: 2
2024-05-03 11:04:41,879:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:04:41,879:INFO:compare_models() successfully completed......................................
2024-05-03 11:04:41,885:INFO:Initializing save_model()
2024-05-03 11:04:41,885:INFO:save_model(model=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), model_name=Models/best_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-05-03 11:04:41,886:INFO:Adding model into prep_pipe
2024-05-03 11:04:41,889:INFO:Models/best_model.pkl saved in current working directory
2024-05-03 11:04:41,894:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('trained_model',
                 LinearDiscriminantAnalysis(covariance_estimator=None,
                                            n_components=None, priors=None,
                                            shrinkage=None, solver='svd',
                                            store_covariance=False,
                                            tol=0.0001))],
         verbose=False)
2024-05-03 11:04:41,894:INFO:save_model() successfully completed......................................
2024-05-03 11:06:25,555:INFO:PyCaret ClassificationExperiment
2024-05-03 11:06:25,555:INFO:Logging name: clf-default-name
2024-05-03 11:06:25,555:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-03 11:06:25,556:INFO:version 3.3.2
2024-05-03 11:06:25,556:INFO:Initializing setup()
2024-05-03 11:06:25,556:INFO:self.USI: 5f5e
2024-05-03 11:06:25,556:INFO:self._variable_keys: {'n_jobs_param', '_available_plots', 'exp_name_log', 'X_train', 'pipeline', 'USI', '_ml_usecase', 'html_param', 'exp_id', 'y_test', 'y_train', 'target_param', 'fold_groups_param', 'X', 'X_test', 'gpu_n_jobs_param', 'idx', 'logging_param', 'fold_shuffle_param', 'fix_imbalance', 'seed', 'data', 'gpu_param', 'memory', 'log_plots_param', 'y', 'is_multiclass', 'fold_generator'}
2024-05-03 11:06:25,556:INFO:Checking environment
2024-05-03 11:06:25,556:INFO:python_version: 3.11.7
2024-05-03 11:06:25,556:INFO:python_build: ('tags/v3.11.7:fa7a6f2', 'Dec  4 2023 19:24:49')
2024-05-03 11:06:25,556:INFO:machine: AMD64
2024-05-03 11:06:25,556:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-03 11:06:25,560:INFO:Memory: svmem(total=16939401216, available=1702903808, percent=89.9, used=15236497408, free=1702903808)
2024-05-03 11:06:25,560:INFO:Physical Core: 4
2024-05-03 11:06:25,560:INFO:Logical Core: 8
2024-05-03 11:06:25,560:INFO:Checking libraries
2024-05-03 11:06:25,560:INFO:System:
2024-05-03 11:06:25,560:INFO:    python: 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]
2024-05-03 11:06:25,560:INFO:executable: Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Scripts\python.exe
2024-05-03 11:06:25,560:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-03 11:06:25,560:INFO:PyCaret required dependencies:
2024-05-03 11:06:25,560:INFO:                 pip: 23.2.1
2024-05-03 11:06:25,560:INFO:          setuptools: 68.2.0
2024-05-03 11:06:25,560:INFO:             pycaret: 3.3.2
2024-05-03 11:06:25,560:INFO:             IPython: 8.24.0
2024-05-03 11:06:25,560:INFO:          ipywidgets: 8.1.2
2024-05-03 11:06:25,560:INFO:                tqdm: 4.66.4
2024-05-03 11:06:25,560:INFO:               numpy: 1.26.4
2024-05-03 11:06:25,560:INFO:              pandas: 2.1.4
2024-05-03 11:06:25,561:INFO:              jinja2: 3.1.3
2024-05-03 11:06:25,561:INFO:               scipy: 1.11.4
2024-05-03 11:06:25,561:INFO:              joblib: 1.3.2
2024-05-03 11:06:25,561:INFO:             sklearn: 1.4.2
2024-05-03 11:06:25,561:INFO:                pyod: 1.1.3
2024-05-03 11:06:25,561:INFO:            imblearn: 0.12.2
2024-05-03 11:06:25,561:INFO:   category_encoders: 2.6.3
2024-05-03 11:06:25,561:INFO:            lightgbm: 4.3.0
2024-05-03 11:06:25,561:INFO:               numba: 0.58.1
2024-05-03 11:06:25,561:INFO:            requests: 2.31.0
2024-05-03 11:06:25,561:INFO:          matplotlib: 3.7.5
2024-05-03 11:06:25,561:INFO:          scikitplot: 0.3.7
2024-05-03 11:06:25,561:INFO:         yellowbrick: 1.5
2024-05-03 11:06:25,561:INFO:              plotly: 5.22.0
2024-05-03 11:06:25,561:INFO:    plotly-resampler: Not installed
2024-05-03 11:06:25,561:INFO:             kaleido: 0.2.1
2024-05-03 11:06:25,561:INFO:           schemdraw: 0.15
2024-05-03 11:06:25,561:INFO:         statsmodels: 0.14.2
2024-05-03 11:06:25,561:INFO:              sktime: 0.26.0
2024-05-03 11:06:25,561:INFO:               tbats: 1.1.3
2024-05-03 11:06:25,561:INFO:            pmdarima: 2.0.4
2024-05-03 11:06:25,561:INFO:              psutil: 5.9.8
2024-05-03 11:06:25,561:INFO:          markupsafe: 2.1.5
2024-05-03 11:06:25,561:INFO:             pickle5: Not installed
2024-05-03 11:06:25,561:INFO:         cloudpickle: 3.0.0
2024-05-03 11:06:25,561:INFO:         deprecation: 2.1.0
2024-05-03 11:06:25,561:INFO:              xxhash: 3.4.1
2024-05-03 11:06:25,561:INFO:           wurlitzer: Not installed
2024-05-03 11:06:25,562:INFO:PyCaret optional dependencies:
2024-05-03 11:06:25,562:INFO:                shap: Not installed
2024-05-03 11:06:25,562:INFO:           interpret: Not installed
2024-05-03 11:06:25,562:INFO:                umap: Not installed
2024-05-03 11:06:25,562:INFO:     ydata_profiling: 4.7.0
2024-05-03 11:06:25,562:INFO:  explainerdashboard: Not installed
2024-05-03 11:06:25,562:INFO:             autoviz: Not installed
2024-05-03 11:06:25,562:INFO:           fairlearn: Not installed
2024-05-03 11:06:25,562:INFO:          deepchecks: Not installed
2024-05-03 11:06:25,562:INFO:             xgboost: Not installed
2024-05-03 11:06:25,562:INFO:            catboost: Not installed
2024-05-03 11:06:25,562:INFO:              kmodes: Not installed
2024-05-03 11:06:25,562:INFO:             mlxtend: Not installed
2024-05-03 11:06:25,562:INFO:       statsforecast: Not installed
2024-05-03 11:06:25,562:INFO:        tune_sklearn: Not installed
2024-05-03 11:06:25,562:INFO:                 ray: Not installed
2024-05-03 11:06:25,562:INFO:            hyperopt: Not installed
2024-05-03 11:06:25,562:INFO:              optuna: Not installed
2024-05-03 11:06:25,562:INFO:               skopt: Not installed
2024-05-03 11:06:25,563:INFO:              mlflow: Not installed
2024-05-03 11:06:25,563:INFO:              gradio: Not installed
2024-05-03 11:06:25,563:INFO:             fastapi: Not installed
2024-05-03 11:06:25,563:INFO:             uvicorn: Not installed
2024-05-03 11:06:25,563:INFO:              m2cgen: Not installed
2024-05-03 11:06:25,564:INFO:           evidently: Not installed
2024-05-03 11:06:25,564:INFO:               fugue: Not installed
2024-05-03 11:06:25,564:INFO:           streamlit: 1.34.0
2024-05-03 11:06:25,564:INFO:             prophet: Not installed
2024-05-03 11:06:25,564:INFO:None
2024-05-03 11:06:25,564:INFO:Set up data.
2024-05-03 11:06:25,567:INFO:Set up folding strategy.
2024-05-03 11:06:25,567:INFO:Set up train/test split.
2024-05-03 11:06:25,570:INFO:Set up index.
2024-05-03 11:06:25,570:INFO:Assigning column types.
2024-05-03 11:06:25,574:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-05-03 11:06:25,618:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:06:25,619:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:06:25,645:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,645:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,692:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:06:25,693:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:06:25,720:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,720:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,721:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-05-03 11:06:25,766:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:06:25,794:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,795:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,835:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:06:25,861:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,861:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,861:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-05-03 11:06:25,925:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,925:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,991:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,992:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:25,993:INFO:Preparing preprocessing pipeline...
2024-05-03 11:06:25,993:INFO:Set up simple imputation.
2024-05-03 11:06:26,004:INFO:Finished creating preprocessing pipeline.
2024-05-03 11:06:26,007:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-05-03 11:06:26,007:INFO:Creating final display dataframe.
2024-05-03 11:06:26,055:INFO:Setup _display_container:                     Description             Value
0                    Session id               318
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (210, 8)
4        Transformed data shape          (210, 8)
5   Transformed train set shape          (147, 8)
6    Transformed test set shape           (63, 8)
7              Numeric features                 7
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              5f5e
2024-05-03 11:06:26,124:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:26,124:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:26,192:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:26,192:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:06:26,194:INFO:setup() successfully completed in 0.64s...............
2024-05-03 11:06:26,196:INFO:Initializing compare_models()
2024-05-03 11:06:26,196:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-05-03 11:06:26,196:INFO:Checking exceptions
2024-05-03 11:06:26,198:INFO:Preparing display monitor
2024-05-03 11:06:26,200:INFO:Initializing Logistic Regression
2024-05-03 11:06:26,201:INFO:Total runtime is 1.618464787801107e-05 minutes
2024-05-03 11:06:26,201:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:26,201:INFO:Initializing create_model()
2024-05-03 11:06:26,201:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:26,201:INFO:Checking exceptions
2024-05-03 11:06:26,201:INFO:Importing libraries
2024-05-03 11:06:26,201:INFO:Copying training dataset
2024-05-03 11:06:26,203:INFO:Defining folds
2024-05-03 11:06:26,203:INFO:Declaring metric variables
2024-05-03 11:06:26,204:INFO:Importing untrained model
2024-05-03 11:06:26,204:INFO:Logistic Regression Imported successfully
2024-05-03 11:06:26,204:INFO:Starting cross validation
2024-05-03 11:06:26,205:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:26,326:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,327:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,328:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,332:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,337:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,365:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,369:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,374:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,409:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,418:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:26,431:INFO:Calculating mean and std
2024-05-03 11:06:26,431:INFO:Creating metrics dataframe
2024-05-03 11:06:26,432:INFO:Uploading results into container
2024-05-03 11:06:26,433:INFO:Uploading model into container now
2024-05-03 11:06:26,433:INFO:_master_model_container: 1
2024-05-03 11:06:26,433:INFO:_display_container: 2
2024-05-03 11:06:26,433:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=318, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-05-03 11:06:26,434:INFO:create_model() successfully completed......................................
2024-05-03 11:06:26,541:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:26,541:INFO:Creating metrics dataframe
2024-05-03 11:06:26,543:INFO:Initializing K Neighbors Classifier
2024-05-03 11:06:26,543:INFO:Total runtime is 0.005724740028381348 minutes
2024-05-03 11:06:26,543:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:26,543:INFO:Initializing create_model()
2024-05-03 11:06:26,543:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:26,543:INFO:Checking exceptions
2024-05-03 11:06:26,543:INFO:Importing libraries
2024-05-03 11:06:26,543:INFO:Copying training dataset
2024-05-03 11:06:26,546:INFO:Defining folds
2024-05-03 11:06:26,546:INFO:Declaring metric variables
2024-05-03 11:06:26,546:INFO:Importing untrained model
2024-05-03 11:06:26,546:INFO:K Neighbors Classifier Imported successfully
2024-05-03 11:06:26,546:INFO:Starting cross validation
2024-05-03 11:06:26,548:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:26,688:INFO:Calculating mean and std
2024-05-03 11:06:26,688:INFO:Creating metrics dataframe
2024-05-03 11:06:26,690:INFO:Uploading results into container
2024-05-03 11:06:26,690:INFO:Uploading model into container now
2024-05-03 11:06:26,690:INFO:_master_model_container: 2
2024-05-03 11:06:26,690:INFO:_display_container: 2
2024-05-03 11:06:26,691:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-05-03 11:06:26,691:INFO:create_model() successfully completed......................................
2024-05-03 11:06:26,799:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:26,799:INFO:Creating metrics dataframe
2024-05-03 11:06:26,802:INFO:Initializing Naive Bayes
2024-05-03 11:06:26,802:INFO:Total runtime is 0.010029864311218262 minutes
2024-05-03 11:06:26,802:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:26,802:INFO:Initializing create_model()
2024-05-03 11:06:26,802:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:26,802:INFO:Checking exceptions
2024-05-03 11:06:26,802:INFO:Importing libraries
2024-05-03 11:06:26,802:INFO:Copying training dataset
2024-05-03 11:06:26,805:INFO:Defining folds
2024-05-03 11:06:26,805:INFO:Declaring metric variables
2024-05-03 11:06:26,805:INFO:Importing untrained model
2024-05-03 11:06:26,805:INFO:Naive Bayes Imported successfully
2024-05-03 11:06:26,805:INFO:Starting cross validation
2024-05-03 11:06:26,806:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:26,878:INFO:Calculating mean and std
2024-05-03 11:06:26,878:INFO:Creating metrics dataframe
2024-05-03 11:06:26,880:INFO:Uploading results into container
2024-05-03 11:06:26,881:INFO:Uploading model into container now
2024-05-03 11:06:26,881:INFO:_master_model_container: 3
2024-05-03 11:06:26,881:INFO:_display_container: 2
2024-05-03 11:06:26,882:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-05-03 11:06:26,882:INFO:create_model() successfully completed......................................
2024-05-03 11:06:26,990:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:26,990:INFO:Creating metrics dataframe
2024-05-03 11:06:26,992:INFO:Initializing Decision Tree Classifier
2024-05-03 11:06:26,992:INFO:Total runtime is 0.013200771808624268 minutes
2024-05-03 11:06:26,992:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:26,992:INFO:Initializing create_model()
2024-05-03 11:06:26,992:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:26,992:INFO:Checking exceptions
2024-05-03 11:06:26,992:INFO:Importing libraries
2024-05-03 11:06:26,993:INFO:Copying training dataset
2024-05-03 11:06:26,995:INFO:Defining folds
2024-05-03 11:06:26,995:INFO:Declaring metric variables
2024-05-03 11:06:26,995:INFO:Importing untrained model
2024-05-03 11:06:26,996:INFO:Decision Tree Classifier Imported successfully
2024-05-03 11:06:26,996:INFO:Starting cross validation
2024-05-03 11:06:26,996:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:27,074:INFO:Calculating mean and std
2024-05-03 11:06:27,074:INFO:Creating metrics dataframe
2024-05-03 11:06:27,076:INFO:Uploading results into container
2024-05-03 11:06:27,076:INFO:Uploading model into container now
2024-05-03 11:06:27,076:INFO:_master_model_container: 4
2024-05-03 11:06:27,076:INFO:_display_container: 2
2024-05-03 11:06:27,076:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=318, splitter='best')
2024-05-03 11:06:27,077:INFO:create_model() successfully completed......................................
2024-05-03 11:06:27,177:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:27,177:INFO:Creating metrics dataframe
2024-05-03 11:06:27,179:INFO:Initializing SVM - Linear Kernel
2024-05-03 11:06:27,179:INFO:Total runtime is 0.016313815116882326 minutes
2024-05-03 11:06:27,179:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:27,179:INFO:Initializing create_model()
2024-05-03 11:06:27,179:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:27,179:INFO:Checking exceptions
2024-05-03 11:06:27,179:INFO:Importing libraries
2024-05-03 11:06:27,179:INFO:Copying training dataset
2024-05-03 11:06:27,182:INFO:Defining folds
2024-05-03 11:06:27,182:INFO:Declaring metric variables
2024-05-03 11:06:27,182:INFO:Importing untrained model
2024-05-03 11:06:27,183:INFO:SVM - Linear Kernel Imported successfully
2024-05-03 11:06:27,183:INFO:Starting cross validation
2024-05-03 11:06:27,183:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:27,226:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,227:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,229:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,232:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,236:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,239:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,242:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:27,254:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,270:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,272:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,279:INFO:Calculating mean and std
2024-05-03 11:06:27,280:INFO:Creating metrics dataframe
2024-05-03 11:06:27,281:INFO:Uploading results into container
2024-05-03 11:06:27,281:INFO:Uploading model into container now
2024-05-03 11:06:27,282:INFO:_master_model_container: 5
2024-05-03 11:06:27,282:INFO:_display_container: 2
2024-05-03 11:06:27,282:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=318, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-05-03 11:06:27,282:INFO:create_model() successfully completed......................................
2024-05-03 11:06:27,383:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:27,383:INFO:Creating metrics dataframe
2024-05-03 11:06:27,385:INFO:Initializing Ridge Classifier
2024-05-03 11:06:27,385:INFO:Total runtime is 0.01975907882054647 minutes
2024-05-03 11:06:27,385:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:27,385:INFO:Initializing create_model()
2024-05-03 11:06:27,385:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:27,385:INFO:Checking exceptions
2024-05-03 11:06:27,385:INFO:Importing libraries
2024-05-03 11:06:27,385:INFO:Copying training dataset
2024-05-03 11:06:27,388:INFO:Defining folds
2024-05-03 11:06:27,388:INFO:Declaring metric variables
2024-05-03 11:06:27,388:INFO:Importing untrained model
2024-05-03 11:06:27,389:INFO:Ridge Classifier Imported successfully
2024-05-03 11:06:27,389:INFO:Starting cross validation
2024-05-03 11:06:27,389:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:27,416:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,416:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,418:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,419:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,420:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,422:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,424:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,424:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,447:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,448:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:27,463:INFO:Calculating mean and std
2024-05-03 11:06:27,464:INFO:Creating metrics dataframe
2024-05-03 11:06:27,465:INFO:Uploading results into container
2024-05-03 11:06:27,465:INFO:Uploading model into container now
2024-05-03 11:06:27,466:INFO:_master_model_container: 6
2024-05-03 11:06:27,466:INFO:_display_container: 2
2024-05-03 11:06:27,466:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=318, solver='auto',
                tol=0.0001)
2024-05-03 11:06:27,466:INFO:create_model() successfully completed......................................
2024-05-03 11:06:27,567:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:27,567:INFO:Creating metrics dataframe
2024-05-03 11:06:27,569:INFO:Initializing Random Forest Classifier
2024-05-03 11:06:27,570:INFO:Total runtime is 0.022817742824554448 minutes
2024-05-03 11:06:27,570:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:27,570:INFO:Initializing create_model()
2024-05-03 11:06:27,570:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:27,570:INFO:Checking exceptions
2024-05-03 11:06:27,570:INFO:Importing libraries
2024-05-03 11:06:27,570:INFO:Copying training dataset
2024-05-03 11:06:27,572:INFO:Defining folds
2024-05-03 11:06:27,573:INFO:Declaring metric variables
2024-05-03 11:06:27,573:INFO:Importing untrained model
2024-05-03 11:06:27,573:INFO:Random Forest Classifier Imported successfully
2024-05-03 11:06:27,573:INFO:Starting cross validation
2024-05-03 11:06:27,574:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:28,149:INFO:Calculating mean and std
2024-05-03 11:06:28,150:INFO:Creating metrics dataframe
2024-05-03 11:06:28,151:INFO:Uploading results into container
2024-05-03 11:06:28,152:INFO:Uploading model into container now
2024-05-03 11:06:28,152:INFO:_master_model_container: 7
2024-05-03 11:06:28,152:INFO:_display_container: 2
2024-05-03 11:06:28,152:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=318, verbose=0,
                       warm_start=False)
2024-05-03 11:06:28,152:INFO:create_model() successfully completed......................................
2024-05-03 11:06:28,266:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:28,266:INFO:Creating metrics dataframe
2024-05-03 11:06:28,269:INFO:Initializing Quadratic Discriminant Analysis
2024-05-03 11:06:28,269:INFO:Total runtime is 0.03448588450749716 minutes
2024-05-03 11:06:28,269:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:28,269:INFO:Initializing create_model()
2024-05-03 11:06:28,269:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:28,269:INFO:Checking exceptions
2024-05-03 11:06:28,269:INFO:Importing libraries
2024-05-03 11:06:28,269:INFO:Copying training dataset
2024-05-03 11:06:28,272:INFO:Defining folds
2024-05-03 11:06:28,272:INFO:Declaring metric variables
2024-05-03 11:06:28,272:INFO:Importing untrained model
2024-05-03 11:06:28,274:INFO:Quadratic Discriminant Analysis Imported successfully
2024-05-03 11:06:28,274:INFO:Starting cross validation
2024-05-03 11:06:28,274:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:28,302:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,303:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,306:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,306:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,307:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,309:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,321:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,330:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,334:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,335:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,345:INFO:Calculating mean and std
2024-05-03 11:06:28,345:INFO:Creating metrics dataframe
2024-05-03 11:06:28,347:INFO:Uploading results into container
2024-05-03 11:06:28,347:INFO:Uploading model into container now
2024-05-03 11:06:28,347:INFO:_master_model_container: 8
2024-05-03 11:06:28,347:INFO:_display_container: 2
2024-05-03 11:06:28,347:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-05-03 11:06:28,348:INFO:create_model() successfully completed......................................
2024-05-03 11:06:28,448:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:28,448:INFO:Creating metrics dataframe
2024-05-03 11:06:28,450:INFO:Initializing Ada Boost Classifier
2024-05-03 11:06:28,450:INFO:Total runtime is 0.03749745686848959 minutes
2024-05-03 11:06:28,450:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:28,451:INFO:Initializing create_model()
2024-05-03 11:06:28,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:28,451:INFO:Checking exceptions
2024-05-03 11:06:28,451:INFO:Importing libraries
2024-05-03 11:06:28,451:INFO:Copying training dataset
2024-05-03 11:06:28,453:INFO:Defining folds
2024-05-03 11:06:28,453:INFO:Declaring metric variables
2024-05-03 11:06:28,453:INFO:Importing untrained model
2024-05-03 11:06:28,453:INFO:Ada Boost Classifier Imported successfully
2024-05-03 11:06:28,454:INFO:Starting cross validation
2024-05-03 11:06:28,454:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:28,470:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,472:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,475:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,476:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,477:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,479:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,480:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,482:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,619:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,622:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,624:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:28,626:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,628:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:28,644:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,646:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:06:28,660:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,666:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:28,671:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,672:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,677:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:28,690:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,690:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,696:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:28,699:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:28,771:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,775:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:28,782:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:28,786:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:28,840:INFO:Calculating mean and std
2024-05-03 11:06:28,841:INFO:Creating metrics dataframe
2024-05-03 11:06:28,844:INFO:Uploading results into container
2024-05-03 11:06:28,844:INFO:Uploading model into container now
2024-05-03 11:06:28,845:INFO:_master_model_container: 9
2024-05-03 11:06:28,845:INFO:_display_container: 2
2024-05-03 11:06:28,845:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=318)
2024-05-03 11:06:28,845:INFO:create_model() successfully completed......................................
2024-05-03 11:06:28,978:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:28,979:INFO:Creating metrics dataframe
2024-05-03 11:06:28,982:INFO:Initializing Gradient Boosting Classifier
2024-05-03 11:06:28,982:INFO:Total runtime is 0.04636403719584148 minutes
2024-05-03 11:06:28,982:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:28,982:INFO:Initializing create_model()
2024-05-03 11:06:28,983:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:28,983:INFO:Checking exceptions
2024-05-03 11:06:28,983:INFO:Importing libraries
2024-05-03 11:06:28,983:INFO:Copying training dataset
2024-05-03 11:06:28,986:INFO:Defining folds
2024-05-03 11:06:28,986:INFO:Declaring metric variables
2024-05-03 11:06:28,986:INFO:Importing untrained model
2024-05-03 11:06:28,987:INFO:Gradient Boosting Classifier Imported successfully
2024-05-03 11:06:28,987:INFO:Starting cross validation
2024-05-03 11:06:28,988:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:29,508:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,511:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,520:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,522:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,626:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,654:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,655:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,664:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,854:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,856:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:29,869:INFO:Calculating mean and std
2024-05-03 11:06:29,870:INFO:Creating metrics dataframe
2024-05-03 11:06:29,871:INFO:Uploading results into container
2024-05-03 11:06:29,871:INFO:Uploading model into container now
2024-05-03 11:06:29,872:INFO:_master_model_container: 10
2024-05-03 11:06:29,872:INFO:_display_container: 2
2024-05-03 11:06:29,872:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=318, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-05-03 11:06:29,872:INFO:create_model() successfully completed......................................
2024-05-03 11:06:29,973:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:29,973:INFO:Creating metrics dataframe
2024-05-03 11:06:29,975:INFO:Initializing Linear Discriminant Analysis
2024-05-03 11:06:29,975:INFO:Total runtime is 0.06292724609375 minutes
2024-05-03 11:06:29,975:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:29,976:INFO:Initializing create_model()
2024-05-03 11:06:29,976:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:29,976:INFO:Checking exceptions
2024-05-03 11:06:29,976:INFO:Importing libraries
2024-05-03 11:06:29,976:INFO:Copying training dataset
2024-05-03 11:06:29,978:INFO:Defining folds
2024-05-03 11:06:29,979:INFO:Declaring metric variables
2024-05-03 11:06:29,979:INFO:Importing untrained model
2024-05-03 11:06:29,979:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:06:29,979:INFO:Starting cross validation
2024-05-03 11:06:29,980:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:30,006:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,006:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,008:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,011:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,011:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,013:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,014:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,022:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,031:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,031:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:06:30,041:INFO:Calculating mean and std
2024-05-03 11:06:30,041:INFO:Creating metrics dataframe
2024-05-03 11:06:30,042:INFO:Uploading results into container
2024-05-03 11:06:30,042:INFO:Uploading model into container now
2024-05-03 11:06:30,043:INFO:_master_model_container: 11
2024-05-03 11:06:30,043:INFO:_display_container: 2
2024-05-03 11:06:30,043:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:06:30,043:INFO:create_model() successfully completed......................................
2024-05-03 11:06:30,143:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:30,143:INFO:Creating metrics dataframe
2024-05-03 11:06:30,144:INFO:Initializing Extra Trees Classifier
2024-05-03 11:06:30,144:INFO:Total runtime is 0.06574098666508993 minutes
2024-05-03 11:06:30,145:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:30,145:INFO:Initializing create_model()
2024-05-03 11:06:30,145:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:30,145:INFO:Checking exceptions
2024-05-03 11:06:30,145:INFO:Importing libraries
2024-05-03 11:06:30,145:INFO:Copying training dataset
2024-05-03 11:06:30,147:INFO:Defining folds
2024-05-03 11:06:30,147:INFO:Declaring metric variables
2024-05-03 11:06:30,147:INFO:Importing untrained model
2024-05-03 11:06:30,149:INFO:Extra Trees Classifier Imported successfully
2024-05-03 11:06:30,149:INFO:Starting cross validation
2024-05-03 11:06:30,149:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:30,559:INFO:Calculating mean and std
2024-05-03 11:06:30,559:INFO:Creating metrics dataframe
2024-05-03 11:06:30,561:INFO:Uploading results into container
2024-05-03 11:06:30,561:INFO:Uploading model into container now
2024-05-03 11:06:30,561:INFO:_master_model_container: 12
2024-05-03 11:06:30,561:INFO:_display_container: 2
2024-05-03 11:06:30,562:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=318, verbose=0,
                     warm_start=False)
2024-05-03 11:06:30,562:INFO:create_model() successfully completed......................................
2024-05-03 11:06:30,672:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:30,672:INFO:Creating metrics dataframe
2024-05-03 11:06:30,675:INFO:Initializing Light Gradient Boosting Machine
2024-05-03 11:06:30,676:INFO:Total runtime is 0.07459747393925985 minutes
2024-05-03 11:06:30,676:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:30,676:INFO:Initializing create_model()
2024-05-03 11:06:30,676:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:30,676:INFO:Checking exceptions
2024-05-03 11:06:30,676:INFO:Importing libraries
2024-05-03 11:06:30,676:INFO:Copying training dataset
2024-05-03 11:06:30,679:INFO:Defining folds
2024-05-03 11:06:30,679:INFO:Declaring metric variables
2024-05-03 11:06:30,679:INFO:Importing untrained model
2024-05-03 11:06:30,680:INFO:Light Gradient Boosting Machine Imported successfully
2024-05-03 11:06:30,680:INFO:Starting cross validation
2024-05-03 11:06:30,681:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:34,100:INFO:Calculating mean and std
2024-05-03 11:06:34,100:INFO:Creating metrics dataframe
2024-05-03 11:06:34,102:INFO:Uploading results into container
2024-05-03 11:06:34,102:INFO:Uploading model into container now
2024-05-03 11:06:34,104:INFO:_master_model_container: 13
2024-05-03 11:06:34,104:INFO:_display_container: 2
2024-05-03 11:06:34,104:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=318, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-05-03 11:06:34,104:INFO:create_model() successfully completed......................................
2024-05-03 11:06:34,267:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:34,267:INFO:Creating metrics dataframe
2024-05-03 11:06:34,269:INFO:Initializing Dummy Classifier
2024-05-03 11:06:34,269:INFO:Total runtime is 0.1344935933748881 minutes
2024-05-03 11:06:34,270:INFO:SubProcess create_model() called ==================================
2024-05-03 11:06:34,270:INFO:Initializing create_model()
2024-05-03 11:06:34,270:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD33EDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:34,270:INFO:Checking exceptions
2024-05-03 11:06:34,270:INFO:Importing libraries
2024-05-03 11:06:34,270:INFO:Copying training dataset
2024-05-03 11:06:34,275:INFO:Defining folds
2024-05-03 11:06:34,275:INFO:Declaring metric variables
2024-05-03 11:06:34,275:INFO:Importing untrained model
2024-05-03 11:06:34,275:INFO:Dummy Classifier Imported successfully
2024-05-03 11:06:34,276:INFO:Starting cross validation
2024-05-03 11:06:34,276:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:06:34,310:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,310:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,314:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,314:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,317:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,317:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,322:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,349:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,350:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,353:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:06:34,369:INFO:Calculating mean and std
2024-05-03 11:06:34,370:INFO:Creating metrics dataframe
2024-05-03 11:06:34,372:INFO:Uploading results into container
2024-05-03 11:06:34,374:INFO:Uploading model into container now
2024-05-03 11:06:34,374:INFO:_master_model_container: 14
2024-05-03 11:06:34,374:INFO:_display_container: 2
2024-05-03 11:06:34,375:INFO:DummyClassifier(constant=None, random_state=318, strategy='prior')
2024-05-03 11:06:34,375:INFO:create_model() successfully completed......................................
2024-05-03 11:06:34,619:INFO:SubProcess create_model() end ==================================
2024-05-03 11:06:34,619:INFO:Creating metrics dataframe
2024-05-03 11:06:34,625:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-05-03 11:06:34,626:INFO:Initializing create_model()
2024-05-03 11:06:34,627:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD357FD0>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:06:34,627:INFO:Checking exceptions
2024-05-03 11:06:34,627:INFO:Importing libraries
2024-05-03 11:06:34,627:INFO:Copying training dataset
2024-05-03 11:06:34,630:INFO:Defining folds
2024-05-03 11:06:34,631:INFO:Declaring metric variables
2024-05-03 11:06:34,631:INFO:Importing untrained model
2024-05-03 11:06:34,631:INFO:Declaring custom model
2024-05-03 11:06:34,632:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:06:34,633:INFO:Cross validation set to False
2024-05-03 11:06:34,633:INFO:Fitting Model
2024-05-03 11:06:34,639:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:06:34,639:INFO:create_model() successfully completed......................................
2024-05-03 11:06:34,772:INFO:_master_model_container: 14
2024-05-03 11:06:34,772:INFO:_display_container: 2
2024-05-03 11:06:34,772:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:06:34,772:INFO:compare_models() successfully completed......................................
2024-05-03 11:07:01,916:INFO:PyCaret ClassificationExperiment
2024-05-03 11:07:01,916:INFO:Logging name: clf-default-name
2024-05-03 11:07:01,916:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-03 11:07:01,916:INFO:version 3.3.2
2024-05-03 11:07:01,916:INFO:Initializing setup()
2024-05-03 11:07:01,916:INFO:self.USI: eb21
2024-05-03 11:07:01,916:INFO:self._variable_keys: {'n_jobs_param', '_available_plots', 'exp_name_log', 'X_train', 'pipeline', 'USI', '_ml_usecase', 'html_param', 'exp_id', 'y_test', 'y_train', 'target_param', 'fold_groups_param', 'X', 'X_test', 'gpu_n_jobs_param', 'idx', 'logging_param', 'fold_shuffle_param', 'fix_imbalance', 'seed', 'data', 'gpu_param', 'memory', 'log_plots_param', 'y', 'is_multiclass', 'fold_generator'}
2024-05-03 11:07:01,917:INFO:Checking environment
2024-05-03 11:07:01,917:INFO:python_version: 3.11.7
2024-05-03 11:07:01,917:INFO:python_build: ('tags/v3.11.7:fa7a6f2', 'Dec  4 2023 19:24:49')
2024-05-03 11:07:01,917:INFO:machine: AMD64
2024-05-03 11:07:01,917:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-03 11:07:01,922:INFO:Memory: svmem(total=16939401216, available=1735860224, percent=89.8, used=15203540992, free=1735860224)
2024-05-03 11:07:01,922:INFO:Physical Core: 4
2024-05-03 11:07:01,922:INFO:Logical Core: 8
2024-05-03 11:07:01,922:INFO:Checking libraries
2024-05-03 11:07:01,922:INFO:System:
2024-05-03 11:07:01,922:INFO:    python: 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]
2024-05-03 11:07:01,922:INFO:executable: Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Scripts\python.exe
2024-05-03 11:07:01,922:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-03 11:07:01,922:INFO:PyCaret required dependencies:
2024-05-03 11:07:01,923:INFO:                 pip: 23.2.1
2024-05-03 11:07:01,923:INFO:          setuptools: 68.2.0
2024-05-03 11:07:01,923:INFO:             pycaret: 3.3.2
2024-05-03 11:07:01,923:INFO:             IPython: 8.24.0
2024-05-03 11:07:01,923:INFO:          ipywidgets: 8.1.2
2024-05-03 11:07:01,923:INFO:                tqdm: 4.66.4
2024-05-03 11:07:01,923:INFO:               numpy: 1.26.4
2024-05-03 11:07:01,923:INFO:              pandas: 2.1.4
2024-05-03 11:07:01,923:INFO:              jinja2: 3.1.3
2024-05-03 11:07:01,923:INFO:               scipy: 1.11.4
2024-05-03 11:07:01,923:INFO:              joblib: 1.3.2
2024-05-03 11:07:01,923:INFO:             sklearn: 1.4.2
2024-05-03 11:07:01,923:INFO:                pyod: 1.1.3
2024-05-03 11:07:01,923:INFO:            imblearn: 0.12.2
2024-05-03 11:07:01,923:INFO:   category_encoders: 2.6.3
2024-05-03 11:07:01,923:INFO:            lightgbm: 4.3.0
2024-05-03 11:07:01,923:INFO:               numba: 0.58.1
2024-05-03 11:07:01,923:INFO:            requests: 2.31.0
2024-05-03 11:07:01,924:INFO:          matplotlib: 3.7.5
2024-05-03 11:07:01,924:INFO:          scikitplot: 0.3.7
2024-05-03 11:07:01,924:INFO:         yellowbrick: 1.5
2024-05-03 11:07:01,924:INFO:              plotly: 5.22.0
2024-05-03 11:07:01,924:INFO:    plotly-resampler: Not installed
2024-05-03 11:07:01,924:INFO:             kaleido: 0.2.1
2024-05-03 11:07:01,924:INFO:           schemdraw: 0.15
2024-05-03 11:07:01,924:INFO:         statsmodels: 0.14.2
2024-05-03 11:07:01,924:INFO:              sktime: 0.26.0
2024-05-03 11:07:01,924:INFO:               tbats: 1.1.3
2024-05-03 11:07:01,924:INFO:            pmdarima: 2.0.4
2024-05-03 11:07:01,924:INFO:              psutil: 5.9.8
2024-05-03 11:07:01,924:INFO:          markupsafe: 2.1.5
2024-05-03 11:07:01,924:INFO:             pickle5: Not installed
2024-05-03 11:07:01,924:INFO:         cloudpickle: 3.0.0
2024-05-03 11:07:01,924:INFO:         deprecation: 2.1.0
2024-05-03 11:07:01,924:INFO:              xxhash: 3.4.1
2024-05-03 11:07:01,924:INFO:           wurlitzer: Not installed
2024-05-03 11:07:01,924:INFO:PyCaret optional dependencies:
2024-05-03 11:07:01,924:INFO:                shap: Not installed
2024-05-03 11:07:01,924:INFO:           interpret: Not installed
2024-05-03 11:07:01,924:INFO:                umap: Not installed
2024-05-03 11:07:01,924:INFO:     ydata_profiling: 4.7.0
2024-05-03 11:07:01,924:INFO:  explainerdashboard: Not installed
2024-05-03 11:07:01,924:INFO:             autoviz: Not installed
2024-05-03 11:07:01,924:INFO:           fairlearn: Not installed
2024-05-03 11:07:01,924:INFO:          deepchecks: Not installed
2024-05-03 11:07:01,924:INFO:             xgboost: Not installed
2024-05-03 11:07:01,924:INFO:            catboost: Not installed
2024-05-03 11:07:01,924:INFO:              kmodes: Not installed
2024-05-03 11:07:01,924:INFO:             mlxtend: Not installed
2024-05-03 11:07:01,924:INFO:       statsforecast: Not installed
2024-05-03 11:07:01,924:INFO:        tune_sklearn: Not installed
2024-05-03 11:07:01,924:INFO:                 ray: Not installed
2024-05-03 11:07:01,925:INFO:            hyperopt: Not installed
2024-05-03 11:07:01,925:INFO:              optuna: Not installed
2024-05-03 11:07:01,925:INFO:               skopt: Not installed
2024-05-03 11:07:01,925:INFO:              mlflow: Not installed
2024-05-03 11:07:01,925:INFO:              gradio: Not installed
2024-05-03 11:07:01,925:INFO:             fastapi: Not installed
2024-05-03 11:07:01,925:INFO:             uvicorn: Not installed
2024-05-03 11:07:01,926:INFO:              m2cgen: Not installed
2024-05-03 11:07:01,926:INFO:           evidently: Not installed
2024-05-03 11:07:01,926:INFO:               fugue: Not installed
2024-05-03 11:07:01,926:INFO:           streamlit: 1.34.0
2024-05-03 11:07:01,926:INFO:             prophet: Not installed
2024-05-03 11:07:01,926:INFO:None
2024-05-03 11:07:01,926:INFO:Set up data.
2024-05-03 11:07:01,929:INFO:Set up folding strategy.
2024-05-03 11:07:01,929:INFO:Set up train/test split.
2024-05-03 11:07:01,933:INFO:Set up index.
2024-05-03 11:07:01,933:INFO:Assigning column types.
2024-05-03 11:07:01,939:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-05-03 11:07:01,993:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:07:01,995:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:07:02,025:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,025:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,070:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:07:02,071:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:07:02,097:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,097:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,097:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-05-03 11:07:02,139:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:07:02,166:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,166:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,206:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:07:02,232:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,232:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,232:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-05-03 11:07:02,297:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,297:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,361:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,361:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,362:INFO:Preparing preprocessing pipeline...
2024-05-03 11:07:02,362:INFO:Set up simple imputation.
2024-05-03 11:07:02,373:INFO:Finished creating preprocessing pipeline.
2024-05-03 11:07:02,375:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-05-03 11:07:02,376:INFO:Creating final display dataframe.
2024-05-03 11:07:02,423:INFO:Setup _display_container:                     Description             Value
0                    Session id              4054
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (210, 8)
4        Transformed data shape          (210, 8)
5   Transformed train set shape          (147, 8)
6    Transformed test set shape           (63, 8)
7              Numeric features                 7
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              eb21
2024-05-03 11:07:02,489:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,489:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,551:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,551:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:07:02,552:INFO:setup() successfully completed in 0.64s...............
2024-05-03 11:07:02,555:INFO:Initializing compare_models()
2024-05-03 11:07:02,555:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-05-03 11:07:02,555:INFO:Checking exceptions
2024-05-03 11:07:02,557:INFO:Preparing display monitor
2024-05-03 11:07:02,559:INFO:Initializing Logistic Regression
2024-05-03 11:07:02,559:INFO:Total runtime is 0.0 minutes
2024-05-03 11:07:02,559:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:02,559:INFO:Initializing create_model()
2024-05-03 11:07:02,559:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:02,559:INFO:Checking exceptions
2024-05-03 11:07:02,559:INFO:Importing libraries
2024-05-03 11:07:02,560:INFO:Copying training dataset
2024-05-03 11:07:02,562:INFO:Defining folds
2024-05-03 11:07:02,562:INFO:Declaring metric variables
2024-05-03 11:07:02,562:INFO:Importing untrained model
2024-05-03 11:07:02,562:INFO:Logistic Regression Imported successfully
2024-05-03 11:07:02,562:INFO:Starting cross validation
2024-05-03 11:07:02,563:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:02,664:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,672:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,683:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,690:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,704:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,715:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,719:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,721:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,752:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,756:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:02,767:INFO:Calculating mean and std
2024-05-03 11:07:02,769:INFO:Creating metrics dataframe
2024-05-03 11:07:02,770:INFO:Uploading results into container
2024-05-03 11:07:02,771:INFO:Uploading model into container now
2024-05-03 11:07:02,771:INFO:_master_model_container: 1
2024-05-03 11:07:02,771:INFO:_display_container: 2
2024-05-03 11:07:02,771:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=4054, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-05-03 11:07:02,771:INFO:create_model() successfully completed......................................
2024-05-03 11:07:02,882:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:02,882:INFO:Creating metrics dataframe
2024-05-03 11:07:02,885:INFO:Initializing K Neighbors Classifier
2024-05-03 11:07:02,885:INFO:Total runtime is 0.00543281634648641 minutes
2024-05-03 11:07:02,885:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:02,885:INFO:Initializing create_model()
2024-05-03 11:07:02,885:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:02,886:INFO:Checking exceptions
2024-05-03 11:07:02,886:INFO:Importing libraries
2024-05-03 11:07:02,886:INFO:Copying training dataset
2024-05-03 11:07:02,888:INFO:Defining folds
2024-05-03 11:07:02,888:INFO:Declaring metric variables
2024-05-03 11:07:02,889:INFO:Importing untrained model
2024-05-03 11:07:02,889:INFO:K Neighbors Classifier Imported successfully
2024-05-03 11:07:02,889:INFO:Starting cross validation
2024-05-03 11:07:02,890:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:03,053:INFO:Calculating mean and std
2024-05-03 11:07:03,053:INFO:Creating metrics dataframe
2024-05-03 11:07:03,054:INFO:Uploading results into container
2024-05-03 11:07:03,054:INFO:Uploading model into container now
2024-05-03 11:07:03,055:INFO:_master_model_container: 2
2024-05-03 11:07:03,055:INFO:_display_container: 2
2024-05-03 11:07:03,055:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-05-03 11:07:03,055:INFO:create_model() successfully completed......................................
2024-05-03 11:07:03,159:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:03,160:INFO:Creating metrics dataframe
2024-05-03 11:07:03,162:INFO:Initializing Naive Bayes
2024-05-03 11:07:03,162:INFO:Total runtime is 0.010063374042510988 minutes
2024-05-03 11:07:03,162:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:03,162:INFO:Initializing create_model()
2024-05-03 11:07:03,162:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:03,162:INFO:Checking exceptions
2024-05-03 11:07:03,162:INFO:Importing libraries
2024-05-03 11:07:03,162:INFO:Copying training dataset
2024-05-03 11:07:03,166:INFO:Defining folds
2024-05-03 11:07:03,166:INFO:Declaring metric variables
2024-05-03 11:07:03,166:INFO:Importing untrained model
2024-05-03 11:07:03,166:INFO:Naive Bayes Imported successfully
2024-05-03 11:07:03,166:INFO:Starting cross validation
2024-05-03 11:07:03,167:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:03,241:INFO:Calculating mean and std
2024-05-03 11:07:03,241:INFO:Creating metrics dataframe
2024-05-03 11:07:03,242:INFO:Uploading results into container
2024-05-03 11:07:03,243:INFO:Uploading model into container now
2024-05-03 11:07:03,243:INFO:_master_model_container: 3
2024-05-03 11:07:03,243:INFO:_display_container: 2
2024-05-03 11:07:03,244:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-05-03 11:07:03,244:INFO:create_model() successfully completed......................................
2024-05-03 11:07:03,349:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:03,349:INFO:Creating metrics dataframe
2024-05-03 11:07:03,352:INFO:Initializing Decision Tree Classifier
2024-05-03 11:07:03,352:INFO:Total runtime is 0.013221995035807293 minutes
2024-05-03 11:07:03,352:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:03,352:INFO:Initializing create_model()
2024-05-03 11:07:03,352:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:03,352:INFO:Checking exceptions
2024-05-03 11:07:03,352:INFO:Importing libraries
2024-05-03 11:07:03,352:INFO:Copying training dataset
2024-05-03 11:07:03,357:INFO:Defining folds
2024-05-03 11:07:03,357:INFO:Declaring metric variables
2024-05-03 11:07:03,358:INFO:Importing untrained model
2024-05-03 11:07:03,358:INFO:Decision Tree Classifier Imported successfully
2024-05-03 11:07:03,358:INFO:Starting cross validation
2024-05-03 11:07:03,359:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:03,445:INFO:Calculating mean and std
2024-05-03 11:07:03,445:INFO:Creating metrics dataframe
2024-05-03 11:07:03,447:INFO:Uploading results into container
2024-05-03 11:07:03,447:INFO:Uploading model into container now
2024-05-03 11:07:03,448:INFO:_master_model_container: 4
2024-05-03 11:07:03,448:INFO:_display_container: 2
2024-05-03 11:07:03,448:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=4054, splitter='best')
2024-05-03 11:07:03,448:INFO:create_model() successfully completed......................................
2024-05-03 11:07:03,552:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:03,552:INFO:Creating metrics dataframe
2024-05-03 11:07:03,554:INFO:Initializing SVM - Linear Kernel
2024-05-03 11:07:03,554:INFO:Total runtime is 0.016593428452809655 minutes
2024-05-03 11:07:03,554:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:03,554:INFO:Initializing create_model()
2024-05-03 11:07:03,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:03,554:INFO:Checking exceptions
2024-05-03 11:07:03,554:INFO:Importing libraries
2024-05-03 11:07:03,555:INFO:Copying training dataset
2024-05-03 11:07:03,558:INFO:Defining folds
2024-05-03 11:07:03,558:INFO:Declaring metric variables
2024-05-03 11:07:03,558:INFO:Importing untrained model
2024-05-03 11:07:03,558:INFO:SVM - Linear Kernel Imported successfully
2024-05-03 11:07:03,559:INFO:Starting cross validation
2024-05-03 11:07:03,559:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:03,608:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,608:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,608:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,612:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,614:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,618:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:03,619:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:03,624:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,627:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,654:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,654:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,666:INFO:Calculating mean and std
2024-05-03 11:07:03,667:INFO:Creating metrics dataframe
2024-05-03 11:07:03,669:INFO:Uploading results into container
2024-05-03 11:07:03,669:INFO:Uploading model into container now
2024-05-03 11:07:03,670:INFO:_master_model_container: 5
2024-05-03 11:07:03,670:INFO:_display_container: 2
2024-05-03 11:07:03,670:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=4054, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-05-03 11:07:03,671:INFO:create_model() successfully completed......................................
2024-05-03 11:07:03,778:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:03,778:INFO:Creating metrics dataframe
2024-05-03 11:07:03,780:INFO:Initializing Ridge Classifier
2024-05-03 11:07:03,780:INFO:Total runtime is 0.020356337229410812 minutes
2024-05-03 11:07:03,780:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:03,781:INFO:Initializing create_model()
2024-05-03 11:07:03,781:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:03,781:INFO:Checking exceptions
2024-05-03 11:07:03,781:INFO:Importing libraries
2024-05-03 11:07:03,781:INFO:Copying training dataset
2024-05-03 11:07:03,784:INFO:Defining folds
2024-05-03 11:07:03,784:INFO:Declaring metric variables
2024-05-03 11:07:03,784:INFO:Importing untrained model
2024-05-03 11:07:03,785:INFO:Ridge Classifier Imported successfully
2024-05-03 11:07:03,785:INFO:Starting cross validation
2024-05-03 11:07:03,785:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:03,815:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,820:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,822:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,822:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,822:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,824:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,832:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,855:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,857:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,857:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:03,868:INFO:Calculating mean and std
2024-05-03 11:07:03,868:INFO:Creating metrics dataframe
2024-05-03 11:07:03,870:INFO:Uploading results into container
2024-05-03 11:07:03,870:INFO:Uploading model into container now
2024-05-03 11:07:03,870:INFO:_master_model_container: 6
2024-05-03 11:07:03,870:INFO:_display_container: 2
2024-05-03 11:07:03,871:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=4054, solver='auto',
                tol=0.0001)
2024-05-03 11:07:03,871:INFO:create_model() successfully completed......................................
2024-05-03 11:07:03,981:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:03,981:INFO:Creating metrics dataframe
2024-05-03 11:07:03,984:INFO:Initializing Random Forest Classifier
2024-05-03 11:07:03,984:INFO:Total runtime is 0.02375192244847616 minutes
2024-05-03 11:07:03,984:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:03,985:INFO:Initializing create_model()
2024-05-03 11:07:03,985:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:03,985:INFO:Checking exceptions
2024-05-03 11:07:03,985:INFO:Importing libraries
2024-05-03 11:07:03,985:INFO:Copying training dataset
2024-05-03 11:07:03,988:INFO:Defining folds
2024-05-03 11:07:03,988:INFO:Declaring metric variables
2024-05-03 11:07:03,988:INFO:Importing untrained model
2024-05-03 11:07:03,988:INFO:Random Forest Classifier Imported successfully
2024-05-03 11:07:03,989:INFO:Starting cross validation
2024-05-03 11:07:03,989:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:04,521:INFO:Calculating mean and std
2024-05-03 11:07:04,522:INFO:Creating metrics dataframe
2024-05-03 11:07:04,523:INFO:Uploading results into container
2024-05-03 11:07:04,524:INFO:Uploading model into container now
2024-05-03 11:07:04,524:INFO:_master_model_container: 7
2024-05-03 11:07:04,524:INFO:_display_container: 2
2024-05-03 11:07:04,524:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=4054, verbose=0,
                       warm_start=False)
2024-05-03 11:07:04,524:INFO:create_model() successfully completed......................................
2024-05-03 11:07:04,630:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:04,630:INFO:Creating metrics dataframe
2024-05-03 11:07:04,632:INFO:Initializing Quadratic Discriminant Analysis
2024-05-03 11:07:04,632:INFO:Total runtime is 0.034551298618316656 minutes
2024-05-03 11:07:04,632:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:04,633:INFO:Initializing create_model()
2024-05-03 11:07:04,633:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:04,633:INFO:Checking exceptions
2024-05-03 11:07:04,633:INFO:Importing libraries
2024-05-03 11:07:04,633:INFO:Copying training dataset
2024-05-03 11:07:04,636:INFO:Defining folds
2024-05-03 11:07:04,636:INFO:Declaring metric variables
2024-05-03 11:07:04,636:INFO:Importing untrained model
2024-05-03 11:07:04,636:INFO:Quadratic Discriminant Analysis Imported successfully
2024-05-03 11:07:04,636:INFO:Starting cross validation
2024-05-03 11:07:04,637:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:04,661:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,664:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,666:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,666:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,668:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,669:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,670:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,677:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,690:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,694:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,710:INFO:Calculating mean and std
2024-05-03 11:07:04,710:INFO:Creating metrics dataframe
2024-05-03 11:07:04,712:INFO:Uploading results into container
2024-05-03 11:07:04,712:INFO:Uploading model into container now
2024-05-03 11:07:04,712:INFO:_master_model_container: 8
2024-05-03 11:07:04,712:INFO:_display_container: 2
2024-05-03 11:07:04,712:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-05-03 11:07:04,712:INFO:create_model() successfully completed......................................
2024-05-03 11:07:04,814:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:04,815:INFO:Creating metrics dataframe
2024-05-03 11:07:04,817:INFO:Initializing Ada Boost Classifier
2024-05-03 11:07:04,817:INFO:Total runtime is 0.03764167229334514 minutes
2024-05-03 11:07:04,817:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:04,817:INFO:Initializing create_model()
2024-05-03 11:07:04,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:04,817:INFO:Checking exceptions
2024-05-03 11:07:04,817:INFO:Importing libraries
2024-05-03 11:07:04,817:INFO:Copying training dataset
2024-05-03 11:07:04,820:INFO:Defining folds
2024-05-03 11:07:04,820:INFO:Declaring metric variables
2024-05-03 11:07:04,820:INFO:Importing untrained model
2024-05-03 11:07:04,820:INFO:Ada Boost Classifier Imported successfully
2024-05-03 11:07:04,820:INFO:Starting cross validation
2024-05-03 11:07:04,821:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:04,835:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:04,838:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:04,839:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:04,842:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:04,842:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:04,843:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:04,849:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:04,859:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:04,971:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,972:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,982:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,983:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,987:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:04,990:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:04,994:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:04,994:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:04,995:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:07:05,002:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,002:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,005:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,007:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:05,092:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,092:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,110:INFO:Calculating mean and std
2024-05-03 11:07:05,111:INFO:Creating metrics dataframe
2024-05-03 11:07:05,112:INFO:Uploading results into container
2024-05-03 11:07:05,112:INFO:Uploading model into container now
2024-05-03 11:07:05,113:INFO:_master_model_container: 9
2024-05-03 11:07:05,113:INFO:_display_container: 2
2024-05-03 11:07:05,113:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=4054)
2024-05-03 11:07:05,113:INFO:create_model() successfully completed......................................
2024-05-03 11:07:05,221:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:05,221:INFO:Creating metrics dataframe
2024-05-03 11:07:05,223:INFO:Initializing Gradient Boosting Classifier
2024-05-03 11:07:05,223:INFO:Total runtime is 0.04440861940383912 minutes
2024-05-03 11:07:05,223:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:05,223:INFO:Initializing create_model()
2024-05-03 11:07:05,223:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:05,223:INFO:Checking exceptions
2024-05-03 11:07:05,223:INFO:Importing libraries
2024-05-03 11:07:05,223:INFO:Copying training dataset
2024-05-03 11:07:05,227:INFO:Defining folds
2024-05-03 11:07:05,228:INFO:Declaring metric variables
2024-05-03 11:07:05,228:INFO:Importing untrained model
2024-05-03 11:07:05,228:INFO:Gradient Boosting Classifier Imported successfully
2024-05-03 11:07:05,228:INFO:Starting cross validation
2024-05-03 11:07:05,229:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:05,735:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,736:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,742:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,751:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,758:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,760:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,782:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:05,802:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,089:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,091:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,099:INFO:Calculating mean and std
2024-05-03 11:07:06,100:INFO:Creating metrics dataframe
2024-05-03 11:07:06,101:INFO:Uploading results into container
2024-05-03 11:07:06,101:INFO:Uploading model into container now
2024-05-03 11:07:06,101:INFO:_master_model_container: 10
2024-05-03 11:07:06,102:INFO:_display_container: 2
2024-05-03 11:07:06,102:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=4054, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-05-03 11:07:06,102:INFO:create_model() successfully completed......................................
2024-05-03 11:07:06,218:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:06,218:INFO:Creating metrics dataframe
2024-05-03 11:07:06,221:INFO:Initializing Linear Discriminant Analysis
2024-05-03 11:07:06,221:INFO:Total runtime is 0.061042869091033944 minutes
2024-05-03 11:07:06,221:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:06,221:INFO:Initializing create_model()
2024-05-03 11:07:06,221:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:06,221:INFO:Checking exceptions
2024-05-03 11:07:06,221:INFO:Importing libraries
2024-05-03 11:07:06,221:INFO:Copying training dataset
2024-05-03 11:07:06,224:INFO:Defining folds
2024-05-03 11:07:06,224:INFO:Declaring metric variables
2024-05-03 11:07:06,224:INFO:Importing untrained model
2024-05-03 11:07:06,224:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:07:06,225:INFO:Starting cross validation
2024-05-03 11:07:06,225:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:06,249:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,249:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,251:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,255:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,256:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,258:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,263:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,277:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,279:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:07:06,296:INFO:Calculating mean and std
2024-05-03 11:07:06,296:INFO:Creating metrics dataframe
2024-05-03 11:07:06,298:INFO:Uploading results into container
2024-05-03 11:07:06,298:INFO:Uploading model into container now
2024-05-03 11:07:06,298:INFO:_master_model_container: 11
2024-05-03 11:07:06,298:INFO:_display_container: 2
2024-05-03 11:07:06,299:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:07:06,299:INFO:create_model() successfully completed......................................
2024-05-03 11:07:06,405:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:06,405:INFO:Creating metrics dataframe
2024-05-03 11:07:06,407:INFO:Initializing Extra Trees Classifier
2024-05-03 11:07:06,407:INFO:Total runtime is 0.06413429975509645 minutes
2024-05-03 11:07:06,407:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:06,408:INFO:Initializing create_model()
2024-05-03 11:07:06,408:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:06,408:INFO:Checking exceptions
2024-05-03 11:07:06,408:INFO:Importing libraries
2024-05-03 11:07:06,408:INFO:Copying training dataset
2024-05-03 11:07:06,410:INFO:Defining folds
2024-05-03 11:07:06,411:INFO:Declaring metric variables
2024-05-03 11:07:06,411:INFO:Importing untrained model
2024-05-03 11:07:06,411:INFO:Extra Trees Classifier Imported successfully
2024-05-03 11:07:06,411:INFO:Starting cross validation
2024-05-03 11:07:06,412:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:06,821:INFO:Calculating mean and std
2024-05-03 11:07:06,822:INFO:Creating metrics dataframe
2024-05-03 11:07:06,823:INFO:Uploading results into container
2024-05-03 11:07:06,823:INFO:Uploading model into container now
2024-05-03 11:07:06,823:INFO:_master_model_container: 12
2024-05-03 11:07:06,823:INFO:_display_container: 2
2024-05-03 11:07:06,824:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=4054, verbose=0,
                     warm_start=False)
2024-05-03 11:07:06,824:INFO:create_model() successfully completed......................................
2024-05-03 11:07:06,933:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:06,933:INFO:Creating metrics dataframe
2024-05-03 11:07:06,936:INFO:Initializing Light Gradient Boosting Machine
2024-05-03 11:07:06,936:INFO:Total runtime is 0.07295281489690146 minutes
2024-05-03 11:07:06,936:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:06,936:INFO:Initializing create_model()
2024-05-03 11:07:06,936:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:06,936:INFO:Checking exceptions
2024-05-03 11:07:06,936:INFO:Importing libraries
2024-05-03 11:07:06,936:INFO:Copying training dataset
2024-05-03 11:07:06,939:INFO:Defining folds
2024-05-03 11:07:06,939:INFO:Declaring metric variables
2024-05-03 11:07:06,939:INFO:Importing untrained model
2024-05-03 11:07:06,940:INFO:Light Gradient Boosting Machine Imported successfully
2024-05-03 11:07:06,940:INFO:Starting cross validation
2024-05-03 11:07:06,940:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:10,904:INFO:Calculating mean and std
2024-05-03 11:07:10,905:INFO:Creating metrics dataframe
2024-05-03 11:07:10,907:INFO:Uploading results into container
2024-05-03 11:07:10,907:INFO:Uploading model into container now
2024-05-03 11:07:10,908:INFO:_master_model_container: 13
2024-05-03 11:07:10,908:INFO:_display_container: 2
2024-05-03 11:07:10,908:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=4054, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-05-03 11:07:10,908:INFO:create_model() successfully completed......................................
2024-05-03 11:07:11,034:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:11,035:INFO:Creating metrics dataframe
2024-05-03 11:07:11,037:INFO:Initializing Dummy Classifier
2024-05-03 11:07:11,037:INFO:Total runtime is 0.1412996768951416 minutes
2024-05-03 11:07:11,037:INFO:SubProcess create_model() called ==================================
2024-05-03 11:07:11,037:INFO:Initializing create_model()
2024-05-03 11:07:11,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDBC863D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:11,037:INFO:Checking exceptions
2024-05-03 11:07:11,037:INFO:Importing libraries
2024-05-03 11:07:11,037:INFO:Copying training dataset
2024-05-03 11:07:11,040:INFO:Defining folds
2024-05-03 11:07:11,040:INFO:Declaring metric variables
2024-05-03 11:07:11,040:INFO:Importing untrained model
2024-05-03 11:07:11,040:INFO:Dummy Classifier Imported successfully
2024-05-03 11:07:11,041:INFO:Starting cross validation
2024-05-03 11:07:11,041:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:07:11,072:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,072:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,072:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,075:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,077:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,077:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,078:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,085:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,096:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,099:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:07:11,114:INFO:Calculating mean and std
2024-05-03 11:07:11,114:INFO:Creating metrics dataframe
2024-05-03 11:07:11,116:INFO:Uploading results into container
2024-05-03 11:07:11,116:INFO:Uploading model into container now
2024-05-03 11:07:11,116:INFO:_master_model_container: 14
2024-05-03 11:07:11,116:INFO:_display_container: 2
2024-05-03 11:07:11,116:INFO:DummyClassifier(constant=None, random_state=4054, strategy='prior')
2024-05-03 11:07:11,116:INFO:create_model() successfully completed......................................
2024-05-03 11:07:11,220:INFO:SubProcess create_model() end ==================================
2024-05-03 11:07:11,221:INFO:Creating metrics dataframe
2024-05-03 11:07:11,223:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-05-03 11:07:11,224:INFO:Initializing create_model()
2024-05-03 11:07:11,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD356F90>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:07:11,224:INFO:Checking exceptions
2024-05-03 11:07:11,225:INFO:Importing libraries
2024-05-03 11:07:11,225:INFO:Copying training dataset
2024-05-03 11:07:11,227:INFO:Defining folds
2024-05-03 11:07:11,227:INFO:Declaring metric variables
2024-05-03 11:07:11,228:INFO:Importing untrained model
2024-05-03 11:07:11,228:INFO:Declaring custom model
2024-05-03 11:07:11,228:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:07:11,228:INFO:Cross validation set to False
2024-05-03 11:07:11,228:INFO:Fitting Model
2024-05-03 11:07:11,234:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:07:11,234:INFO:create_model() successfully completed......................................
2024-05-03 11:07:11,353:INFO:_master_model_container: 14
2024-05-03 11:07:11,353:INFO:_display_container: 2
2024-05-03 11:07:11,353:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:07:11,353:INFO:compare_models() successfully completed......................................
2024-05-03 11:07:11,359:INFO:Initializing save_model()
2024-05-03 11:07:11,360:INFO:save_model(model=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), model_name=best_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-05-03 11:07:11,360:INFO:Adding model into prep_pipe
2024-05-03 11:07:11,362:INFO:best_model.pkl saved in current working directory
2024-05-03 11:07:11,365:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('trained_model',
                 LinearDiscriminantAnalysis(covariance_estimator=None,
                                            n_components=None, priors=None,
                                            shrinkage=None, solver='svd',
                                            store_covariance=False,
                                            tol=0.0001))],
         verbose=False)
2024-05-03 11:07:11,365:INFO:save_model() successfully completed......................................
2024-05-03 11:08:14,401:INFO:PyCaret ClassificationExperiment
2024-05-03 11:08:14,401:INFO:Logging name: clf-default-name
2024-05-03 11:08:14,401:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-03 11:08:14,401:INFO:version 3.3.2
2024-05-03 11:08:14,401:INFO:Initializing setup()
2024-05-03 11:08:14,402:INFO:self.USI: d72c
2024-05-03 11:08:14,402:INFO:self._variable_keys: {'n_jobs_param', '_available_plots', 'exp_name_log', 'X_train', 'pipeline', 'USI', '_ml_usecase', 'html_param', 'exp_id', 'y_test', 'y_train', 'target_param', 'fold_groups_param', 'X', 'X_test', 'gpu_n_jobs_param', 'idx', 'logging_param', 'fold_shuffle_param', 'fix_imbalance', 'seed', 'data', 'gpu_param', 'memory', 'log_plots_param', 'y', 'is_multiclass', 'fold_generator'}
2024-05-03 11:08:14,402:INFO:Checking environment
2024-05-03 11:08:14,402:INFO:python_version: 3.11.7
2024-05-03 11:08:14,402:INFO:python_build: ('tags/v3.11.7:fa7a6f2', 'Dec  4 2023 19:24:49')
2024-05-03 11:08:14,402:INFO:machine: AMD64
2024-05-03 11:08:14,402:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-03 11:08:14,406:INFO:Memory: svmem(total=16939401216, available=1735507968, percent=89.8, used=15203893248, free=1735507968)
2024-05-03 11:08:14,406:INFO:Physical Core: 4
2024-05-03 11:08:14,406:INFO:Logical Core: 8
2024-05-03 11:08:14,406:INFO:Checking libraries
2024-05-03 11:08:14,406:INFO:System:
2024-05-03 11:08:14,406:INFO:    python: 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]
2024-05-03 11:08:14,406:INFO:executable: Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Scripts\python.exe
2024-05-03 11:08:14,406:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-03 11:08:14,406:INFO:PyCaret required dependencies:
2024-05-03 11:08:14,406:INFO:                 pip: 23.2.1
2024-05-03 11:08:14,406:INFO:          setuptools: 68.2.0
2024-05-03 11:08:14,406:INFO:             pycaret: 3.3.2
2024-05-03 11:08:14,406:INFO:             IPython: 8.24.0
2024-05-03 11:08:14,406:INFO:          ipywidgets: 8.1.2
2024-05-03 11:08:14,406:INFO:                tqdm: 4.66.4
2024-05-03 11:08:14,406:INFO:               numpy: 1.26.4
2024-05-03 11:08:14,406:INFO:              pandas: 2.1.4
2024-05-03 11:08:14,406:INFO:              jinja2: 3.1.3
2024-05-03 11:08:14,406:INFO:               scipy: 1.11.4
2024-05-03 11:08:14,407:INFO:              joblib: 1.3.2
2024-05-03 11:08:14,407:INFO:             sklearn: 1.4.2
2024-05-03 11:08:14,407:INFO:                pyod: 1.1.3
2024-05-03 11:08:14,407:INFO:            imblearn: 0.12.2
2024-05-03 11:08:14,407:INFO:   category_encoders: 2.6.3
2024-05-03 11:08:14,407:INFO:            lightgbm: 4.3.0
2024-05-03 11:08:14,407:INFO:               numba: 0.58.1
2024-05-03 11:08:14,407:INFO:            requests: 2.31.0
2024-05-03 11:08:14,407:INFO:          matplotlib: 3.7.5
2024-05-03 11:08:14,407:INFO:          scikitplot: 0.3.7
2024-05-03 11:08:14,407:INFO:         yellowbrick: 1.5
2024-05-03 11:08:14,407:INFO:              plotly: 5.22.0
2024-05-03 11:08:14,407:INFO:    plotly-resampler: Not installed
2024-05-03 11:08:14,407:INFO:             kaleido: 0.2.1
2024-05-03 11:08:14,407:INFO:           schemdraw: 0.15
2024-05-03 11:08:14,407:INFO:         statsmodels: 0.14.2
2024-05-03 11:08:14,407:INFO:              sktime: 0.26.0
2024-05-03 11:08:14,407:INFO:               tbats: 1.1.3
2024-05-03 11:08:14,407:INFO:            pmdarima: 2.0.4
2024-05-03 11:08:14,407:INFO:              psutil: 5.9.8
2024-05-03 11:08:14,407:INFO:          markupsafe: 2.1.5
2024-05-03 11:08:14,407:INFO:             pickle5: Not installed
2024-05-03 11:08:14,407:INFO:         cloudpickle: 3.0.0
2024-05-03 11:08:14,407:INFO:         deprecation: 2.1.0
2024-05-03 11:08:14,407:INFO:              xxhash: 3.4.1
2024-05-03 11:08:14,407:INFO:           wurlitzer: Not installed
2024-05-03 11:08:14,407:INFO:PyCaret optional dependencies:
2024-05-03 11:08:14,407:INFO:                shap: Not installed
2024-05-03 11:08:14,407:INFO:           interpret: Not installed
2024-05-03 11:08:14,408:INFO:                umap: Not installed
2024-05-03 11:08:14,408:INFO:     ydata_profiling: 4.7.0
2024-05-03 11:08:14,408:INFO:  explainerdashboard: Not installed
2024-05-03 11:08:14,408:INFO:             autoviz: Not installed
2024-05-03 11:08:14,408:INFO:           fairlearn: Not installed
2024-05-03 11:08:14,408:INFO:          deepchecks: Not installed
2024-05-03 11:08:14,408:INFO:             xgboost: Not installed
2024-05-03 11:08:14,408:INFO:            catboost: Not installed
2024-05-03 11:08:14,408:INFO:              kmodes: Not installed
2024-05-03 11:08:14,408:INFO:             mlxtend: Not installed
2024-05-03 11:08:14,408:INFO:       statsforecast: Not installed
2024-05-03 11:08:14,408:INFO:        tune_sklearn: Not installed
2024-05-03 11:08:14,408:INFO:                 ray: Not installed
2024-05-03 11:08:14,408:INFO:            hyperopt: Not installed
2024-05-03 11:08:14,408:INFO:              optuna: Not installed
2024-05-03 11:08:14,408:INFO:               skopt: Not installed
2024-05-03 11:08:14,408:INFO:              mlflow: Not installed
2024-05-03 11:08:14,408:INFO:              gradio: Not installed
2024-05-03 11:08:14,408:INFO:             fastapi: Not installed
2024-05-03 11:08:14,408:INFO:             uvicorn: Not installed
2024-05-03 11:08:14,408:INFO:              m2cgen: Not installed
2024-05-03 11:08:14,408:INFO:           evidently: Not installed
2024-05-03 11:08:14,408:INFO:               fugue: Not installed
2024-05-03 11:08:14,408:INFO:           streamlit: 1.34.0
2024-05-03 11:08:14,408:INFO:             prophet: Not installed
2024-05-03 11:08:14,408:INFO:None
2024-05-03 11:08:14,408:INFO:Set up data.
2024-05-03 11:08:14,411:INFO:Set up folding strategy.
2024-05-03 11:08:14,411:INFO:Set up train/test split.
2024-05-03 11:08:14,415:INFO:Set up index.
2024-05-03 11:08:14,415:INFO:Assigning column types.
2024-05-03 11:08:14,422:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-05-03 11:08:14,471:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:08:14,472:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:08:14,501:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,501:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,547:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-03 11:08:14,548:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:08:14,576:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,576:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,577:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-05-03 11:08:14,622:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:08:14,650:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,650:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,689:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-03 11:08:14,714:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,714:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,714:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-05-03 11:08:14,779:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,779:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,843:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,843:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,844:INFO:Preparing preprocessing pipeline...
2024-05-03 11:08:14,844:INFO:Set up simple imputation.
2024-05-03 11:08:14,856:INFO:Finished creating preprocessing pipeline.
2024-05-03 11:08:14,859:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-05-03 11:08:14,859:INFO:Creating final display dataframe.
2024-05-03 11:08:14,905:INFO:Setup _display_container:                     Description             Value
0                    Session id              7833
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (210, 8)
4        Transformed data shape          (210, 8)
5   Transformed train set shape          (147, 8)
6    Transformed test set shape           (63, 8)
7              Numeric features                 7
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              d72c
2024-05-03 11:08:14,972:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:14,973:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:15,035:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:15,036:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-03 11:08:15,037:INFO:setup() successfully completed in 0.64s...............
2024-05-03 11:08:15,040:INFO:Initializing compare_models()
2024-05-03 11:08:15,040:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-05-03 11:08:15,040:INFO:Checking exceptions
2024-05-03 11:08:15,043:INFO:Preparing display monitor
2024-05-03 11:08:15,044:INFO:Initializing Logistic Regression
2024-05-03 11:08:15,044:INFO:Total runtime is 0.0 minutes
2024-05-03 11:08:15,044:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:15,044:INFO:Initializing create_model()
2024-05-03 11:08:15,044:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:15,044:INFO:Checking exceptions
2024-05-03 11:08:15,044:INFO:Importing libraries
2024-05-03 11:08:15,045:INFO:Copying training dataset
2024-05-03 11:08:15,047:INFO:Defining folds
2024-05-03 11:08:15,047:INFO:Declaring metric variables
2024-05-03 11:08:15,047:INFO:Importing untrained model
2024-05-03 11:08:15,047:INFO:Logistic Regression Imported successfully
2024-05-03 11:08:15,048:INFO:Starting cross validation
2024-05-03 11:08:15,048:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:15,168:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,172:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,173:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,179:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,183:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,206:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,209:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,209:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,242:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,262:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:15,277:INFO:Calculating mean and std
2024-05-03 11:08:15,278:INFO:Creating metrics dataframe
2024-05-03 11:08:15,279:INFO:Uploading results into container
2024-05-03 11:08:15,279:INFO:Uploading model into container now
2024-05-03 11:08:15,280:INFO:_master_model_container: 1
2024-05-03 11:08:15,280:INFO:_display_container: 2
2024-05-03 11:08:15,280:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=7833, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-05-03 11:08:15,280:INFO:create_model() successfully completed......................................
2024-05-03 11:08:15,384:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:15,384:INFO:Creating metrics dataframe
2024-05-03 11:08:15,385:INFO:Initializing K Neighbors Classifier
2024-05-03 11:08:15,386:INFO:Total runtime is 0.00569232702255249 minutes
2024-05-03 11:08:15,386:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:15,386:INFO:Initializing create_model()
2024-05-03 11:08:15,386:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:15,386:INFO:Checking exceptions
2024-05-03 11:08:15,386:INFO:Importing libraries
2024-05-03 11:08:15,386:INFO:Copying training dataset
2024-05-03 11:08:15,388:INFO:Defining folds
2024-05-03 11:08:15,389:INFO:Declaring metric variables
2024-05-03 11:08:15,389:INFO:Importing untrained model
2024-05-03 11:08:15,389:INFO:K Neighbors Classifier Imported successfully
2024-05-03 11:08:15,389:INFO:Starting cross validation
2024-05-03 11:08:15,390:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:15,530:INFO:Calculating mean and std
2024-05-03 11:08:15,530:INFO:Creating metrics dataframe
2024-05-03 11:08:15,532:INFO:Uploading results into container
2024-05-03 11:08:15,532:INFO:Uploading model into container now
2024-05-03 11:08:15,532:INFO:_master_model_container: 2
2024-05-03 11:08:15,532:INFO:_display_container: 2
2024-05-03 11:08:15,532:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-05-03 11:08:15,532:INFO:create_model() successfully completed......................................
2024-05-03 11:08:15,635:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:15,635:INFO:Creating metrics dataframe
2024-05-03 11:08:15,637:INFO:Initializing Naive Bayes
2024-05-03 11:08:15,637:INFO:Total runtime is 0.009868725140889486 minutes
2024-05-03 11:08:15,638:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:15,638:INFO:Initializing create_model()
2024-05-03 11:08:15,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:15,638:INFO:Checking exceptions
2024-05-03 11:08:15,638:INFO:Importing libraries
2024-05-03 11:08:15,638:INFO:Copying training dataset
2024-05-03 11:08:15,641:INFO:Defining folds
2024-05-03 11:08:15,641:INFO:Declaring metric variables
2024-05-03 11:08:15,641:INFO:Importing untrained model
2024-05-03 11:08:15,641:INFO:Naive Bayes Imported successfully
2024-05-03 11:08:15,642:INFO:Starting cross validation
2024-05-03 11:08:15,642:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:15,712:INFO:Calculating mean and std
2024-05-03 11:08:15,713:INFO:Creating metrics dataframe
2024-05-03 11:08:15,714:INFO:Uploading results into container
2024-05-03 11:08:15,714:INFO:Uploading model into container now
2024-05-03 11:08:15,714:INFO:_master_model_container: 3
2024-05-03 11:08:15,714:INFO:_display_container: 2
2024-05-03 11:08:15,714:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-05-03 11:08:15,715:INFO:create_model() successfully completed......................................
2024-05-03 11:08:15,818:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:15,818:INFO:Creating metrics dataframe
2024-05-03 11:08:15,820:INFO:Initializing Decision Tree Classifier
2024-05-03 11:08:15,821:INFO:Total runtime is 0.012949546178181967 minutes
2024-05-03 11:08:15,821:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:15,821:INFO:Initializing create_model()
2024-05-03 11:08:15,821:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:15,821:INFO:Checking exceptions
2024-05-03 11:08:15,821:INFO:Importing libraries
2024-05-03 11:08:15,821:INFO:Copying training dataset
2024-05-03 11:08:15,825:INFO:Defining folds
2024-05-03 11:08:15,825:INFO:Declaring metric variables
2024-05-03 11:08:15,825:INFO:Importing untrained model
2024-05-03 11:08:15,825:INFO:Decision Tree Classifier Imported successfully
2024-05-03 11:08:15,825:INFO:Starting cross validation
2024-05-03 11:08:15,826:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:15,900:INFO:Calculating mean and std
2024-05-03 11:08:15,901:INFO:Creating metrics dataframe
2024-05-03 11:08:15,902:INFO:Uploading results into container
2024-05-03 11:08:15,902:INFO:Uploading model into container now
2024-05-03 11:08:15,902:INFO:_master_model_container: 4
2024-05-03 11:08:15,902:INFO:_display_container: 2
2024-05-03 11:08:15,903:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=7833, splitter='best')
2024-05-03 11:08:15,903:INFO:create_model() successfully completed......................................
2024-05-03 11:08:16,013:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:16,013:INFO:Creating metrics dataframe
2024-05-03 11:08:16,015:INFO:Initializing SVM - Linear Kernel
2024-05-03 11:08:16,015:INFO:Total runtime is 0.016181902090708414 minutes
2024-05-03 11:08:16,016:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:16,016:INFO:Initializing create_model()
2024-05-03 11:08:16,016:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:16,016:INFO:Checking exceptions
2024-05-03 11:08:16,016:INFO:Importing libraries
2024-05-03 11:08:16,016:INFO:Copying training dataset
2024-05-03 11:08:16,019:INFO:Defining folds
2024-05-03 11:08:16,019:INFO:Declaring metric variables
2024-05-03 11:08:16,019:INFO:Importing untrained model
2024-05-03 11:08:16,019:INFO:SVM - Linear Kernel Imported successfully
2024-05-03 11:08:16,019:INFO:Starting cross validation
2024-05-03 11:08:16,020:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:16,068:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,068:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,068:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,071:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,071:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,072:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,078:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,109:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,109:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,111:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:16,123:INFO:Calculating mean and std
2024-05-03 11:08:16,123:INFO:Creating metrics dataframe
2024-05-03 11:08:16,124:INFO:Uploading results into container
2024-05-03 11:08:16,125:INFO:Uploading model into container now
2024-05-03 11:08:16,125:INFO:_master_model_container: 5
2024-05-03 11:08:16,125:INFO:_display_container: 2
2024-05-03 11:08:16,125:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=7833, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-05-03 11:08:16,125:INFO:create_model() successfully completed......................................
2024-05-03 11:08:16,227:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:16,228:INFO:Creating metrics dataframe
2024-05-03 11:08:16,230:INFO:Initializing Ridge Classifier
2024-05-03 11:08:16,230:INFO:Total runtime is 0.01975429058074951 minutes
2024-05-03 11:08:16,230:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:16,230:INFO:Initializing create_model()
2024-05-03 11:08:16,230:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:16,230:INFO:Checking exceptions
2024-05-03 11:08:16,230:INFO:Importing libraries
2024-05-03 11:08:16,230:INFO:Copying training dataset
2024-05-03 11:08:16,232:INFO:Defining folds
2024-05-03 11:08:16,232:INFO:Declaring metric variables
2024-05-03 11:08:16,232:INFO:Importing untrained model
2024-05-03 11:08:16,233:INFO:Ridge Classifier Imported successfully
2024-05-03 11:08:16,233:INFO:Starting cross validation
2024-05-03 11:08:16,233:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:16,261:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,262:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,262:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,263:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,264:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,270:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,271:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,278:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,286:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,288:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:16,297:INFO:Calculating mean and std
2024-05-03 11:08:16,297:INFO:Creating metrics dataframe
2024-05-03 11:08:16,298:INFO:Uploading results into container
2024-05-03 11:08:16,299:INFO:Uploading model into container now
2024-05-03 11:08:16,299:INFO:_master_model_container: 6
2024-05-03 11:08:16,299:INFO:_display_container: 2
2024-05-03 11:08:16,299:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=7833, solver='auto',
                tol=0.0001)
2024-05-03 11:08:16,299:INFO:create_model() successfully completed......................................
2024-05-03 11:08:16,405:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:16,405:INFO:Creating metrics dataframe
2024-05-03 11:08:16,407:INFO:Initializing Random Forest Classifier
2024-05-03 11:08:16,407:INFO:Total runtime is 0.022711288928985596 minutes
2024-05-03 11:08:16,408:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:16,408:INFO:Initializing create_model()
2024-05-03 11:08:16,408:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:16,408:INFO:Checking exceptions
2024-05-03 11:08:16,408:INFO:Importing libraries
2024-05-03 11:08:16,408:INFO:Copying training dataset
2024-05-03 11:08:16,410:INFO:Defining folds
2024-05-03 11:08:16,410:INFO:Declaring metric variables
2024-05-03 11:08:16,411:INFO:Importing untrained model
2024-05-03 11:08:16,411:INFO:Random Forest Classifier Imported successfully
2024-05-03 11:08:16,411:INFO:Starting cross validation
2024-05-03 11:08:16,412:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:16,892:INFO:Calculating mean and std
2024-05-03 11:08:16,893:INFO:Creating metrics dataframe
2024-05-03 11:08:16,894:INFO:Uploading results into container
2024-05-03 11:08:16,894:INFO:Uploading model into container now
2024-05-03 11:08:16,895:INFO:_master_model_container: 7
2024-05-03 11:08:16,895:INFO:_display_container: 2
2024-05-03 11:08:16,895:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=7833, verbose=0,
                       warm_start=False)
2024-05-03 11:08:16,895:INFO:create_model() successfully completed......................................
2024-05-03 11:08:17,005:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:17,006:INFO:Creating metrics dataframe
2024-05-03 11:08:17,008:INFO:Initializing Quadratic Discriminant Analysis
2024-05-03 11:08:17,008:INFO:Total runtime is 0.03273030519485474 minutes
2024-05-03 11:08:17,008:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:17,008:INFO:Initializing create_model()
2024-05-03 11:08:17,008:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:17,008:INFO:Checking exceptions
2024-05-03 11:08:17,008:INFO:Importing libraries
2024-05-03 11:08:17,008:INFO:Copying training dataset
2024-05-03 11:08:17,011:INFO:Defining folds
2024-05-03 11:08:17,011:INFO:Declaring metric variables
2024-05-03 11:08:17,011:INFO:Importing untrained model
2024-05-03 11:08:17,011:INFO:Quadratic Discriminant Analysis Imported successfully
2024-05-03 11:08:17,011:INFO:Starting cross validation
2024-05-03 11:08:17,012:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:17,036:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,037:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,040:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,040:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,040:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,043:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,044:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,052:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,066:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,068:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,084:INFO:Calculating mean and std
2024-05-03 11:08:17,084:INFO:Creating metrics dataframe
2024-05-03 11:08:17,086:INFO:Uploading results into container
2024-05-03 11:08:17,086:INFO:Uploading model into container now
2024-05-03 11:08:17,086:INFO:_master_model_container: 8
2024-05-03 11:08:17,086:INFO:_display_container: 2
2024-05-03 11:08:17,087:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-05-03 11:08:17,087:INFO:create_model() successfully completed......................................
2024-05-03 11:08:17,190:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:17,190:INFO:Creating metrics dataframe
2024-05-03 11:08:17,192:INFO:Initializing Ada Boost Classifier
2024-05-03 11:08:17,192:INFO:Total runtime is 0.035794985294342045 minutes
2024-05-03 11:08:17,192:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:17,192:INFO:Initializing create_model()
2024-05-03 11:08:17,192:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:17,192:INFO:Checking exceptions
2024-05-03 11:08:17,192:INFO:Importing libraries
2024-05-03 11:08:17,192:INFO:Copying training dataset
2024-05-03 11:08:17,196:INFO:Defining folds
2024-05-03 11:08:17,196:INFO:Declaring metric variables
2024-05-03 11:08:17,196:INFO:Importing untrained model
2024-05-03 11:08:17,196:INFO:Ada Boost Classifier Imported successfully
2024-05-03 11:08:17,196:INFO:Starting cross validation
2024-05-03 11:08:17,197:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:17,211:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,213:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,214:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,216:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,218:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,219:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,220:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,224:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,343:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,345:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,348:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,348:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,354:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,358:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,363:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,365:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-03 11:08:17,368:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,372:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,377:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:17,439:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,446:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:17,449:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:17,454:INFO:Calculating mean and std
2024-05-03 11:08:17,455:INFO:Creating metrics dataframe
2024-05-03 11:08:17,456:INFO:Uploading results into container
2024-05-03 11:08:17,456:INFO:Uploading model into container now
2024-05-03 11:08:17,457:INFO:_master_model_container: 9
2024-05-03 11:08:17,457:INFO:_display_container: 2
2024-05-03 11:08:17,457:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=7833)
2024-05-03 11:08:17,457:INFO:create_model() successfully completed......................................
2024-05-03 11:08:17,567:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:17,567:INFO:Creating metrics dataframe
2024-05-03 11:08:17,569:INFO:Initializing Gradient Boosting Classifier
2024-05-03 11:08:17,569:INFO:Total runtime is 0.04208336273829143 minutes
2024-05-03 11:08:17,569:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:17,570:INFO:Initializing create_model()
2024-05-03 11:08:17,570:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:17,570:INFO:Checking exceptions
2024-05-03 11:08:17,570:INFO:Importing libraries
2024-05-03 11:08:17,570:INFO:Copying training dataset
2024-05-03 11:08:17,572:INFO:Defining folds
2024-05-03 11:08:17,572:INFO:Declaring metric variables
2024-05-03 11:08:17,573:INFO:Importing untrained model
2024-05-03 11:08:17,573:INFO:Gradient Boosting Classifier Imported successfully
2024-05-03 11:08:17,574:INFO:Starting cross validation
2024-05-03 11:08:17,574:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:18,046:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,063:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,065:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,090:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,091:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,104:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,106:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,118:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,383:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,388:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,402:INFO:Calculating mean and std
2024-05-03 11:08:18,403:INFO:Creating metrics dataframe
2024-05-03 11:08:18,404:INFO:Uploading results into container
2024-05-03 11:08:18,404:INFO:Uploading model into container now
2024-05-03 11:08:18,405:INFO:_master_model_container: 10
2024-05-03 11:08:18,405:INFO:_display_container: 2
2024-05-03 11:08:18,405:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7833, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-05-03 11:08:18,405:INFO:create_model() successfully completed......................................
2024-05-03 11:08:18,508:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:18,508:INFO:Creating metrics dataframe
2024-05-03 11:08:18,510:INFO:Initializing Linear Discriminant Analysis
2024-05-03 11:08:18,510:INFO:Total runtime is 0.057764995098114016 minutes
2024-05-03 11:08:18,511:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:18,511:INFO:Initializing create_model()
2024-05-03 11:08:18,511:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:18,511:INFO:Checking exceptions
2024-05-03 11:08:18,511:INFO:Importing libraries
2024-05-03 11:08:18,511:INFO:Copying training dataset
2024-05-03 11:08:18,514:INFO:Defining folds
2024-05-03 11:08:18,514:INFO:Declaring metric variables
2024-05-03 11:08:18,514:INFO:Importing untrained model
2024-05-03 11:08:18,515:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:08:18,515:INFO:Starting cross validation
2024-05-03 11:08:18,515:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:18,538:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,540:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,541:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,545:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,545:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,546:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,547:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,556:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,568:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,570:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-03 11:08:18,585:INFO:Calculating mean and std
2024-05-03 11:08:18,585:INFO:Creating metrics dataframe
2024-05-03 11:08:18,587:INFO:Uploading results into container
2024-05-03 11:08:18,587:INFO:Uploading model into container now
2024-05-03 11:08:18,587:INFO:_master_model_container: 11
2024-05-03 11:08:18,587:INFO:_display_container: 2
2024-05-03 11:08:18,588:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:08:18,588:INFO:create_model() successfully completed......................................
2024-05-03 11:08:18,692:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:18,692:INFO:Creating metrics dataframe
2024-05-03 11:08:18,694:INFO:Initializing Extra Trees Classifier
2024-05-03 11:08:18,695:INFO:Total runtime is 0.06084601879119873 minutes
2024-05-03 11:08:18,695:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:18,695:INFO:Initializing create_model()
2024-05-03 11:08:18,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:18,695:INFO:Checking exceptions
2024-05-03 11:08:18,695:INFO:Importing libraries
2024-05-03 11:08:18,695:INFO:Copying training dataset
2024-05-03 11:08:18,698:INFO:Defining folds
2024-05-03 11:08:18,698:INFO:Declaring metric variables
2024-05-03 11:08:18,698:INFO:Importing untrained model
2024-05-03 11:08:18,699:INFO:Extra Trees Classifier Imported successfully
2024-05-03 11:08:18,699:INFO:Starting cross validation
2024-05-03 11:08:18,699:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:19,102:INFO:Calculating mean and std
2024-05-03 11:08:19,102:INFO:Creating metrics dataframe
2024-05-03 11:08:19,103:INFO:Uploading results into container
2024-05-03 11:08:19,104:INFO:Uploading model into container now
2024-05-03 11:08:19,104:INFO:_master_model_container: 12
2024-05-03 11:08:19,104:INFO:_display_container: 2
2024-05-03 11:08:19,105:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=7833, verbose=0,
                     warm_start=False)
2024-05-03 11:08:19,105:INFO:create_model() successfully completed......................................
2024-05-03 11:08:19,214:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:19,214:INFO:Creating metrics dataframe
2024-05-03 11:08:19,216:INFO:Initializing Light Gradient Boosting Machine
2024-05-03 11:08:19,216:INFO:Total runtime is 0.06952163775761923 minutes
2024-05-03 11:08:19,216:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:19,217:INFO:Initializing create_model()
2024-05-03 11:08:19,217:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:19,217:INFO:Checking exceptions
2024-05-03 11:08:19,217:INFO:Importing libraries
2024-05-03 11:08:19,217:INFO:Copying training dataset
2024-05-03 11:08:19,220:INFO:Defining folds
2024-05-03 11:08:19,220:INFO:Declaring metric variables
2024-05-03 11:08:19,220:INFO:Importing untrained model
2024-05-03 11:08:19,220:INFO:Light Gradient Boosting Machine Imported successfully
2024-05-03 11:08:19,220:INFO:Starting cross validation
2024-05-03 11:08:19,221:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:21,859:INFO:Calculating mean and std
2024-05-03 11:08:21,860:INFO:Creating metrics dataframe
2024-05-03 11:08:21,862:INFO:Uploading results into container
2024-05-03 11:08:21,862:INFO:Uploading model into container now
2024-05-03 11:08:21,863:INFO:_master_model_container: 13
2024-05-03 11:08:21,863:INFO:_display_container: 2
2024-05-03 11:08:21,863:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7833, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-05-03 11:08:21,864:INFO:create_model() successfully completed......................................
2024-05-03 11:08:21,995:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:21,995:INFO:Creating metrics dataframe
2024-05-03 11:08:21,997:INFO:Initializing Dummy Classifier
2024-05-03 11:08:21,998:INFO:Total runtime is 0.11588126023610434 minutes
2024-05-03 11:08:21,998:INFO:SubProcess create_model() called ==================================
2024-05-03 11:08:21,998:INFO:Initializing create_model()
2024-05-03 11:08:21,998:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019EDD446DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:21,998:INFO:Checking exceptions
2024-05-03 11:08:21,998:INFO:Importing libraries
2024-05-03 11:08:21,998:INFO:Copying training dataset
2024-05-03 11:08:22,001:INFO:Defining folds
2024-05-03 11:08:22,001:INFO:Declaring metric variables
2024-05-03 11:08:22,001:INFO:Importing untrained model
2024-05-03 11:08:22,001:INFO:Dummy Classifier Imported successfully
2024-05-03 11:08:22,002:INFO:Starting cross validation
2024-05-03 11:08:22,002:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-03 11:08:22,033:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,034:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,035:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,036:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,038:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,043:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,049:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,060:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,062:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,064:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-03 11:08:22,072:INFO:Calculating mean and std
2024-05-03 11:08:22,072:INFO:Creating metrics dataframe
2024-05-03 11:08:22,073:INFO:Uploading results into container
2024-05-03 11:08:22,073:INFO:Uploading model into container now
2024-05-03 11:08:22,074:INFO:_master_model_container: 14
2024-05-03 11:08:22,074:INFO:_display_container: 2
2024-05-03 11:08:22,074:INFO:DummyClassifier(constant=None, random_state=7833, strategy='prior')
2024-05-03 11:08:22,074:INFO:create_model() successfully completed......................................
2024-05-03 11:08:22,192:INFO:SubProcess create_model() end ==================================
2024-05-03 11:08:22,192:INFO:Creating metrics dataframe
2024-05-03 11:08:22,194:WARNING:Z:\01 - Travail\- Cours - ESME\Inge 2\MasterProject\pythonProject\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-05-03 11:08:22,195:INFO:Initializing create_model()
2024-05-03 11:08:22,195:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019EDD3571D0>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-03 11:08:22,195:INFO:Checking exceptions
2024-05-03 11:08:22,196:INFO:Importing libraries
2024-05-03 11:08:22,196:INFO:Copying training dataset
2024-05-03 11:08:22,199:INFO:Defining folds
2024-05-03 11:08:22,199:INFO:Declaring metric variables
2024-05-03 11:08:22,199:INFO:Importing untrained model
2024-05-03 11:08:22,199:INFO:Declaring custom model
2024-05-03 11:08:22,200:INFO:Linear Discriminant Analysis Imported successfully
2024-05-03 11:08:22,201:INFO:Cross validation set to False
2024-05-03 11:08:22,201:INFO:Fitting Model
2024-05-03 11:08:22,213:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:08:22,213:INFO:create_model() successfully completed......................................
2024-05-03 11:08:22,355:INFO:_master_model_container: 14
2024-05-03 11:08:22,355:INFO:_display_container: 2
2024-05-03 11:08:22,355:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-05-03 11:08:22,355:INFO:compare_models() successfully completed......................................
2024-05-03 11:08:22,365:INFO:Initializing save_model()
2024-05-03 11:08:22,366:INFO:save_model(model=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), model_name=Models/best_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\clerc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-05-03 11:08:22,366:INFO:Adding model into prep_pipe
2024-05-03 11:08:22,376:INFO:Models/best_model.pkl saved in current working directory
2024-05-03 11:08:22,381:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['A', 'P', 'C', 'LK', 'WK',
                                             'A_Coef', 'LKG'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('trained_model',
                 LinearDiscriminantAnalysis(covariance_estimator=None,
                                            n_components=None, priors=None,
                                            shrinkage=None, solver='svd',
                                            store_covariance=False,
                                            tol=0.0001))],
         verbose=False)
2024-05-03 11:08:22,382:INFO:save_model() successfully completed......................................
